{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE-W6E3Cy66L"
   },
   "source": [
    "# Distributed Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HtMh17BUy6Pc"
   },
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_recommenders as tfrs\n",
    "# import tensorflow_io as tfio\n",
    "\n",
    "# from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HrKzx8b5zDHV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'spotify-2tower'\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "MODEL_VERSION = 'jwv4-approx-valid-one-epoch'\n",
    "PIPELINE_VERSION = 'v1'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}-{PIPELINE_VERSION}'\n",
    "\n",
    "PROJECT= 'hybrid-vertex'\n",
    "REGION='us-central1'\n",
    "# BUCKET_NAME='spotify-tfrecords-blog'\n",
    "OUTPUT_BUCKET = 'jt-tfrs-test'\n",
    "STAGING_BUCKET =f'gs://{OUTPUT_BUCKET}'\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_NAME = f'{MODEL_ROOT_NAME}-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = 'tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'\n",
    "\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kj-KiRcvzfuk"
   },
   "outputs": [],
   "source": [
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_DISPLAY_NAME = f\"{MODEL_ROOT_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "\n",
    "# tensorboard_resource_name = tensorboard.gca_resource.name\n",
    "# print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD = 'projects/934903580331/locations/us-central1/tensorboards/4842196432167370752'\n",
    "# TENSORBOARD = 'projects/934903580331/locations/us-central1/tensorboards/5764308455871479808'\n",
    "\n",
    "# TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/8299553571104358400\"\n",
    "TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/9194643997044244480\"\n",
    "\n",
    "# tb = aiplatform.Tensorboard('projects/934903580331/locations/us-central1/tensorboards/2710867908514283520')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZbeq5FC0NBf"
   },
   "source": [
    "## Perepare Vertex Training Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create repo for training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/spotify-tfrs\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QLUTCt34zx3e"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "\n",
    "# Make folder for Python training script\n",
    "# ! rm -rf {REPO_DOCKER_PATH_PREFIX}\n",
    "# ! mkdir {REPO_DOCKER_PATH_PREFIX}\n",
    "\n",
    "# Add package information\n",
    "# ! touch {REPO_DOCKER_PATH_PREFIX}/README.md\n",
    "\n",
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "! touch {REPO_DOCKER_PATH_PREFIX}/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interactive training shell in Vertex AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/interactive_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gcloud compute images list \\\n",
    "        --project deeplearning-platform-release \\\n",
    "        --no-standard-images\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "gcloud compute images describe-from-family IMAGE_FAMILY \\\n",
    "        --project deeplearning-platform-release\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.tfrs\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM tensorflow/tensorflow:2.8.2-gpu\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer/* trainer/ \n",
    "\n",
    "RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93dxYpL00XqU"
   },
   "source": [
    "### requirements.txt\n",
    "\n",
    "* TODO: for profiling, install `google-cloud-aiplatform[cloud_profiler]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L3Xvv9Nc0YCw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/requirements.txt\n",
    "\n",
    "google-cloud-aiplatform==1.17.0\n",
    "tensorflow-recommenders==0.6.0\n",
    "tensorboard==2.8.0\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-profile==2.5.0\n",
    "scann\n",
    "cloudml-hypertune\n",
    "google-cloud-aiplatform[cloud_profiler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google-cloud-aiplatform==1.17.0\n",
    "# tensorflow==2.9.2\n",
    "# tensorflow-cloud==0.1.16\n",
    "# tensorflow-datasets==4.4.0\n",
    "# tensorflow-estimator==2.9.0\n",
    "# tensorflow-hub==0.12.0\n",
    "# tensorflow-io==0.23.1\n",
    "# tensorflow-io-gcs-filesystem==0.27.0\n",
    "# tensorflow-metadata==1.10.0\n",
    "# tensorflow-recommenders==0.7.0\n",
    "# tensorflow-serving-api==2.10.0\n",
    "# tensorflow-transform==1.10.1\n",
    "# tensorboard==2.9.1\n",
    "# tensorboard-data-server==0.6.1\n",
    "# tensorboard-plugin-profile==2.5.0\n",
    "# cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir {REPO_DOCKER_PATH_PREFIX}/trainer/trainer_src\n",
    "# !touch {REPO_DOCKER_PATH_PREFIX}/trainer/trainer_src/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/data_src.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/data_src.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import train_config as cfg\n",
    "\n",
    "MAX_PLAYLIST_LENGTH = cfg.MAX_PADDING # 375\n",
    "\n",
    "def pad_up_to(t, max_in_dims=[1 ,MAX_PLAYLIST_LENGTH], constant_value=''):\n",
    "    s = tf.shape(t)\n",
    "    paddings = [[0, m-s[i]] for (i,m) in enumerate(max_in_dims)]\n",
    "    return tf.pad(t, paddings, 'CONSTANT', constant_values=constant_value)\n",
    "\n",
    "def return_padded_tensors(context, data):\n",
    "    \n",
    "    a = data['track_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    b = data['artist_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    c = data['album_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    d = data['track_uri_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    e = data['duration_ms_songs_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    f = data['artist_pop_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    g = data['artists_followers_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    h = data['track_pop_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    i = data['artist_genres_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        \n",
    "    padded_data = context.copy()\n",
    "    padded_data['track_name_pl'] = a\n",
    "    padded_data['artist_name_pl'] = b\n",
    "    padded_data['album_name_pl'] = c\n",
    "    padded_data['track_uri_pl'] = d\n",
    "    padded_data['duration_ms_songs_pl'] = e\n",
    "    padded_data['artist_pop_pl'] = f\n",
    "    padded_data['artists_followers_pl'] = g\n",
    "    padded_data['track_pop_pl'] = h\n",
    "    padded_data['artist_genres_pl'] = i\n",
    "        \n",
    "    return padded_data\n",
    "\n",
    "candidate_features = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "}\n",
    "\n",
    "cont_feats = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'name': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'collaborative': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'n_songs_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_artists_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_albums_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'description_pl': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "}\n",
    "    ###ragged\n",
    "\n",
    "seq_feats = {\n",
    "    'track_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'artist_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'album_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'track_uri_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'duration_ms_songs_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artists_followers_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'track_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_genres_pl': tf.io.RaggedFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse_tfrecord(example):\n",
    "    example = tf.io.parse_single_sequence_example(\n",
    "        example, \n",
    "        context_features=cont_feats,\n",
    "        sequence_features=seq_feats\n",
    "    )\n",
    "    return example\n",
    "\n",
    "def parse_candidate_tfrecord_fn(example):\n",
    "    example = tf.io.parse_single_example(\n",
    "        example, \n",
    "        features=candidate_features\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/trainer/model_src.py\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/model_src.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "\n",
    "import train_config as cfg\n",
    "# ====================================================\n",
    "# Playlist (query) Tower\n",
    "# ====================================================\n",
    "\n",
    "# TODO: parameterize\n",
    "\n",
    "EMBEDDING_DIM = cfg.EMBEDDING_DIM       # 32\n",
    "PROJECTION_DIM = cfg.PROJECTION_DIM     # 5\n",
    "SEED = cfg.SEED                         # 1234\n",
    "USE_CROSS_LAYER = cfg.USE_CROSS_LAYER   # True\n",
    "DROPOUT = cfg.USE_DROPOUT               # 'False'\n",
    "DROPOUT_RATE = cfg.DROPOUT_RATE         # '0.33'\n",
    "MAX_PLAYLIST_LENGTH = cfg.MAX_PADDING   # 375\n",
    "TOKEN_DICT = cfg.TOKEN_DICT             # '20000_tokens'\n",
    "\n",
    "# MAX_PLAYLIST_LENGTH = 375\n",
    "# EMBEDDING_DIM = 32\n",
    "# PROJECTION_DIM = 5\n",
    "# SEED = 1234\n",
    "# USE_CROSS_LAYER=True\n",
    "# DROPOUT='False'\n",
    "# DROPOUT_RATE='0.33'\n",
    "# TOKEN_DICT = '20000_tokens'\n",
    "\n",
    "class Playlist_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # ========================================\n",
    "        # non-sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: playlist name\n",
    "        self.pl_name_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS, # not needed if passing vocab\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT]['name'], \n",
    "                #     name=\"pl_name_txt_vectorizer\", \n",
    "                #     ngrams=2\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=1_000_000), #one MILLION playlists\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=1_000_000 + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_name_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"pl_name_pooling\"),\n",
    "            ], name=\"pl_name_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: collaborative\n",
    "        collaborative_vocab = np.array([b'false', b'true'])\n",
    "        \n",
    "        self.pl_collaborative_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=collaborative_vocab, \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_collaborative_lookup\", \n",
    "                    output_mode='int'\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(collaborative_vocab) + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_collaborative_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_collaborative_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: pid\n",
    "        self.pl_track_uri_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['track_uri_can'], \n",
    "                #     mask_token=None, \n",
    "                #     name=\"pl_track_uri_lookup\", \n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_uri_can\"])),\n",
    "\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can'])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_track_uri_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_track_uri_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: n_songs_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_songs_pl'], \n",
    "            vocab_dict['max_n_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_songs_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_songs_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: num_artists_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_artists_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_artists_pl'], \n",
    "            vocab_dict['max_n_artists_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_artists_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_artists_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_artists_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_artists_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                )\n",
    "            ], name=\"n_artists_pl_emb_model\"\n",
    "        )\n",
    "\n",
    "        # Feature: num_albums_pl\n",
    "        n_albums_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_albums_pl'], \n",
    "            vocab_dict['max_n_albums_pl'],\n",
    "            num=100\n",
    "        )\n",
    "        self.n_albums_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_albums_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_albums_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_albums_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_albums_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_pl\n",
    "        self.artist_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=tf.constant(vocab_dict['artist_name_can']), mask_token=None),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"artist_name_can\"]), mask_value=''),\n",
    "\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_name_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_pl_1d\"),\n",
    "            ], name=\"artist_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_pl\n",
    "        # 2.2M unique\n",
    "        self.track_uri_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['track_uri_can'], mask_token=''),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_uri_can\"]), mask_value=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_uri_1d\"),\n",
    "            ], name=\"track_uri_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_pl\n",
    "        self.track_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['track_name_can'], \n",
    "                #     name=\"track_name_pl_lookup\",\n",
    "                #     output_mode='int',\n",
    "                #     mask_token=''\n",
    "                # ),\n",
    "            tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_name_can\"]), mask_value=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_name_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_pl_1d\"),\n",
    "            ], name=\"track_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        Feature: duration_ms_songs_pl\n",
    "        duration_ms_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_duration_ms_songs_pl'], \n",
    "            vocab_dict['max_duration_ms_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.duration_ms_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(duration_ms_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(duration_ms_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"duration_ms_songs_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"duration_ms_songs_pl_emb_layer_pl_1d\"),\n",
    "            ], name=\"duration_ms_songs_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_pl\n",
    "        self.album_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['album_name_can'], \n",
    "                #     mask_token=None, \n",
    "                #     name=\"album_name_pl_lookup\"\n",
    "                # ),\n",
    "            tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"album_name_can\"]), mask_value=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['album_name_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_pl_emb_layer_1d\"),\n",
    "            ], name=\"album_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_pl\n",
    "        artist_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_pop'], \n",
    "            vocab_dict['max_artist_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artist_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artist_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artist_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_pop_1d\"),\n",
    "            ], name=\"artist_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artists_followers_pl\n",
    "        artists_followers_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_followers'], \n",
    "            vocab_dict['max_artist_followers'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artists_followers_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artists_followers_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artists_followers_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artists_followers_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artists_followers_pl_1d\"),\n",
    "            ], name=\"artists_followers_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_pl\n",
    "        track_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_track_pop'], \n",
    "            vocab_dict['max_track_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.track_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(dtype=tf.float32),\n",
    "                tf.keras.layers.Discretization(track_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(track_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_pop_pl_1d\"),\n",
    "            ], name=\"track_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_pl\n",
    "        self.artist_genres_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"album_uri_can\"]), mask_value=''),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['artist_genres_can'], mask_token=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_genres_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_genres_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_pl_1d\"),\n",
    "            ], name=\"artist_genres_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # dense and cross layers\n",
    "        # ========================================\n",
    "\n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"pl_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "            \n",
    "        # Dense Layers\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"pl_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: tf.nn.l2_normalize(\n",
    "                    x, 1, epsilon=1e-12, name=\"normalize_dense\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # ========================================\n",
    "    # call\n",
    "    # ========================================\n",
    "    def call(self, data):\n",
    "        '''\n",
    "        The call method defines what happens when\n",
    "        the model is called\n",
    "        '''\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.pl_name_text_embedding(data['name']),\n",
    "                self.pl_collaborative_embedding(data['collaborative']),\n",
    "                self.pl_track_uri_embedding(data[\"track_uri_can\"]),\n",
    "                self.n_songs_pl_embedding(data[\"n_songs_pl\"]),\n",
    "                self.n_artists_pl_embedding(data['num_artists_pl']),\n",
    "                self.n_albums_pl_embedding(data[\"num_albums_pl\"]),\n",
    "                \n",
    "                # sequence features\n",
    "                self.artist_name_pl_embedding(tf.reshape(data[\"artist_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))), #reshape to get [BATCH, MAX_SEQ_LEN]\n",
    "                self.track_uri_pl_embedding(tf.reshape(data[\"track_uri_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.track_name_pl_embedding(tf.reshape(data[\"track_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.duration_ms_songs_pl_embedding(tf.reshape(data[\"duration_ms_songs_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.album_name_pl_embedding(tf.reshape(data[\"album_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artist_pop_pl_embedding(tf.reshape(data[\"artist_pop_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artists_followers_pl_embedding(tf.reshape(data[\"artists_followers_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.track_pop_pl_embedding(tf.reshape(data[\"track_pop_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artist_genres_pl_embedding(tf.reshape(data[\"artist_genres_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "            ], axis=1)\n",
    "        \n",
    "        # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)\n",
    "\n",
    "# ====================================================\n",
    "# Track (candidate) Tower\n",
    "# ====================================================\n",
    "class Candidate_Track_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # Candidate features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_can\n",
    "        self.artist_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "            #     tf.keras.layers.TextVectorization(\n",
    "            #         # max_tokens=MAX_TOKENS,\n",
    "            #         vocabulary=vocab_dict[TOKEN_DICT][\"artist_name_can\"],\n",
    "            #         name=\"artist_name_can_txt_vectorizer\",\n",
    "            #         ngrams=2,\n",
    "            #     ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_name_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_can_pooling\"),\n",
    "            ], name=\"artist_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_can\n",
    "        self.track_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS,\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT][\"track_name_can\"],\n",
    "                #     name=\"track_name_can_txt_vectorizer\",\n",
    "                #     ngrams=2,\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"track_name_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_can_pooling\"),\n",
    "            ], name=\"track_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_can\n",
    "        self.album_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS,\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT][\"album_name_can\"],\n",
    "                #     name=\"album_name_can_txt_vectorizer\",\n",
    "                #     ngrams=2,\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"album_name_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_can_pooling\"),\n",
    "            ], name=\"album_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_uri_can\n",
    "        self.artist_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"artist_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_can\n",
    "        self.track_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_uri_can\"])),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"track_uri_can\"])+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"track_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_uri_can\n",
    "        self.album_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"album_uri_can\"])),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"album_uri_can\"])+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"album_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: duration_ms_can\n",
    "        self.duration_ms_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_duration_ms_songs_pl'],\n",
    "            variance=vocab_dict['var_duration_ms_songs_pl'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_can\n",
    "        self.track_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_track_pop'],\n",
    "            variance=vocab_dict['var_track_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_can\n",
    "        self.artist_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_pop'],\n",
    "            variance=vocab_dict['var_artist_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_followers_can\n",
    "        self.artist_followers_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_followers'],\n",
    "            variance=vocab_dict['var_artist_followers'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_can\n",
    "        self.artist_genres_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS,\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT][\"artist_genres_can\"],\n",
    "                #     name=\"artist_genres_can_txt_vectorizer\",\n",
    "                #     ngrams=2,\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_genres_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_can_pooling\"),\n",
    "            ], name=\"artist_genres_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # Dense & Cross Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"can_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "        \n",
    "        # Dense Layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"candidate_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    # ========================================\n",
    "    # Call Function\n",
    "    # ========================================\n",
    "            \n",
    "    def call(self, data):\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.artist_name_can_text_embedding(data['artist_name_can']),  \n",
    "                self.track_name_can_text_embedding(data['track_name_can']),  \n",
    "                self.album_name_can_text_embedding(data['album_name_can']),  \n",
    "                self.artist_uri_can_embedding(data['artist_uri_can']),  \n",
    "                self.track_uri_can_embedding(data['track_uri_can']),  \n",
    "                self.album_uri_can_embedding(data['album_uri_can']),  \n",
    "                tf.reshape(self.duration_ms_can_normalized(data[\"duration_ms_can\"]), (-1, 1)), \n",
    "                tf.reshape(self.track_pop_can_normalized(data[\"track_pop_can\"]), (-1, 1)),  \n",
    "                tf.reshape(self.artist_pop_can_normalized(data[\"artist_pop_can\"]), (-1, 1)),  \n",
    "                tf.reshape(self.artist_followers_can_normalized(data[\"artist_followers_can\"]), (-1, 1)),  \n",
    "                self.artist_genres_can_text_embedding(data['album_uri_can']),  \n",
    "            ], axis=1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # return self.dense_layers(all_embs)\n",
    "                # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)\n",
    "\n",
    "# ====================================================\n",
    "# Combined 2Tower\n",
    "# ====================================================\n",
    "class TheTwoTowers(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes, vocab_dict_load, parsed_candidate_dataset):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_tower = Playlist_Model(layer_sizes, vocab_dict_load)\n",
    "        \n",
    "        self.candidate_tower = Candidate_Track_Model(layer_sizes, vocab_dict_load)\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=parsed_candidate_dataset.batch(128).map(self.candidate_tower)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_tower(data)\n",
    "        candidate_embeddings = self.candidate_tower(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings, \n",
    "            candidate_embeddings, \n",
    "            compute_metrics=not training\n",
    "        ) # turn off metrics to save time on training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/task.py\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras import mixed_precision\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle as pkl\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "\n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "# ====================================================\n",
    "# Main\n",
    "# ====================================================\n",
    "import data_src as trainer_data\n",
    "import model_src as trainer_model\n",
    "import train_config as cfg\n",
    "import time\n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "    TF_GPU_THREAD_MODE='gpu_private'\n",
    "    \n",
    "    logging.info(\"Starting training...\")\n",
    "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    \n",
    "    storage_client = storage.Client(\n",
    "        project=args.project\n",
    "    )\n",
    "    \n",
    "    WORKING_DIR = f'gs://{args.train_output_gcs_bucket}'             # replaced f'gs://{args.model_dir}/{args.version}'\n",
    "    logging.info(f'Train job output directory: {WORKING_DIR}')\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    \n",
    "    # AIP_TB_LOGS = args.aip_tb_logs # os.environ.get('AIP_TENSORBOARD_LOG_DIR', 'NA')\n",
    "    # logging.info(f'AIP TENSORBOARD LOG DIR: {AIP_TB_LOGS}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU/TPU Strategy\n",
    "    # ====================================================\n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.config.list_physical_devices('GPU'): # TODO: replace with - tf.config.list_physical_devices('GPU') | tf.test.is_gpu_available()\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    \n",
    "    # Single Machine, multiple TPU devices\n",
    "    elif args.distribute == 'tpu':\n",
    "        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "        logging.info(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "    \n",
    "    logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    \n",
    "    # ====================================================\n",
    "    # Vocab Files\n",
    "    # ====================================================\n",
    "\n",
    "    # TODO: parameterize & configure for adapts vs vocab files\n",
    "\n",
    "    BUCKET_NAME = 'spotify-v1'                           # args.vocab_gcs_bucket\n",
    "    FILE_PATH = 'vocabs/v2_string_vocabs'                # args.vocab_gcs_file_path\n",
    "    FILE_NAME = 'string_vocabs_v1_20220924-tokens22.pkl'   # args.vocab_filename\n",
    "    DESTINATION_FILE = 'downloaded_vocabs.txt'     \n",
    "\n",
    "    with open(f'{DESTINATION_FILE}', 'wb') as file_obj:\n",
    "        storage_client.download_blob_to_file(\n",
    "            f'gs://{BUCKET_NAME}/{FILE_PATH}/{FILE_NAME}', file_obj)\n",
    "\n",
    "\n",
    "    with open(f'{DESTINATION_FILE}', 'rb') as pickle_file:\n",
    "        vocab_dict_load = pkl.load(pickle_file)\n",
    "\n",
    "    # ====================================================\n",
    "    # TRAIN dataset - Parse & Pad\n",
    "    # ====================================================\n",
    "\n",
    "    # logging.info(f'Getting train data from bucket: {args.train_dir}')\n",
    "    # logging.info(f'args.train_dir_prefix: {args.train_dir_prefix}')\n",
    "    \n",
    "    logging.info(f'Path to TRAIN files: gs://{args.train_dir}/{args.train_dir_prefix}')\n",
    "    \n",
    "    train_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.train_dir}', prefix=f'{args.train_dir_prefix}', delimiter=\"/\"):\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    # Parse train dataset\n",
    "    # raw_train_ds = tf.data.TFRecordDataset(train_files)\n",
    "    # parsed_train_ds = raw_train_ds.map(trainer_data.parse_tfrecord) # _data\n",
    "    # parsed_padded_train_ds = parsed_train_ds.map(trainer_data.return_padded_tensors) # _data\n",
    "    \n",
    "    # OPTIMIZE DATA INPUT PIPELINE\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "    train_dataset = train_dataset.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    ).map(\n",
    "        trainer_data.parse_tfrecord,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).map(\n",
    "        trainer_data.return_padded_tensors,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).batch(\n",
    "        args.batch_size * strategy.num_replicas_in_sync\n",
    "    ).prefetch(\n",
    "        tf.data.AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # VALID dataset - Parse & Pad \n",
    "    # ====================================================\n",
    "    \n",
    "    # logging.info(f'args.valid_dir: {args.valid_dir}')                   # TODO: args.valid_dir\n",
    "    # logging.info(f'args.valid_dir_prefix: {args.valid_dir_prefix}')     # TODO: args.valid_dir_prefix\n",
    "    \n",
    "    logging.info(f'Path to VALID files: gs://{args.valid_dir}/{args.valid_dir_prefix}')\n",
    "    \n",
    "    valid_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.valid_dir}', prefix=f'{args.valid_dir_prefix}', delimiter=\"/\"):\n",
    "        valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    # Parse train dataset\n",
    "    # raw_valid_ds = tf.data.TFRecordDataset(valid_files)\n",
    "    # parsed_valid_ds = raw_valid_ds.map(trainer_data.parse_tfrecord) # _data\n",
    "    # parsed_padded_valid_ds = parsed_valid_ds.map(trainer_data.return_padded_tensors) # _data\n",
    "    \n",
    "    # OPTIMIZE DATA INPUT PIPELINE\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices(valid_files)\n",
    "    valid_dataset = valid_dataset.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    ).map(\n",
    "        trainer_data.parse_tfrecord,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).map(\n",
    "        trainer_data.return_padded_tensors,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).batch(\n",
    "        args.batch_size * strategy.num_replicas_in_sync\n",
    "    ).prefetch(\n",
    "        tf.data.AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Parse candidates dataset\n",
    "    # ====================================================\n",
    "\n",
    "    # logging.info(f'args.candidate_file_dir: {args.candidate_file_dir}')\n",
    "    # logging.info(f'args.candidate_files_prefix: {args.candidate_files_prefix}')\n",
    "    \n",
    "    logging.info(f'Path to CANDIDATE files: gs://{args.candidate_file_dir}/{args.candidate_files_prefix}')\n",
    "\n",
    "    candidate_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.candidate_file_dir}', prefix=f'{args.candidate_files_prefix}', delimiter=\"/\"):\n",
    "        candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    raw_candidate_dataset = tf.data.TFRecordDataset(candidate_files)\n",
    "    # parsed_candidate_dataset = raw_candidate_dataset.map(trainer_data.parse_candidate_tfrecord_fn) # _data\n",
    "    #generate the candidate dataset\n",
    "    candidate_dataset = tf.data.Dataset.from_tensor_slices(candidate_files)\n",
    "    parsed_candidate_dataset = candidate_dataset.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    ).map(\n",
    "        trainer_data.parse_candidate_tfrecord_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    ).prefetch(\n",
    "        tf.data.AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Prepare Train and Valid Data\n",
    "    # ====================================================\n",
    "    # logging.info(f'preparing train and valid splits...')\n",
    "    # tf.random.set_seed(42)\n",
    "    \n",
    "    # TRAIN\n",
    "    # shuffled_parsed_train_ds = parsed_padded_train_ds.shuffle(10_000, seed=42, reshuffle_each_iteration=False)\n",
    "    # cached_train = shuffled_parsed_train_ds.batch(args.batch_size * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # VALID\n",
    "    # shuffled_parsed_train_ds = parsed_padded_valid_ds.shuffle(10_000, seed=42, reshuffle_each_iteration=False)\n",
    "    # cached_valid = parsed_padded_valid_ds.batch(args.batch_size * strategy.num_replicas_in_sync).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # logging.info(f'TRAIN and VALID prepped...')\n",
    "\n",
    "    # train_data = shuffled_parsed_ds.take(80_000).batch(128)\n",
    "    # valid_data = shuffled_parsed_ds.skip(80_000).take(20_000).batch(128)\n",
    "    \n",
    "    # valid_size = 20_000 # cfg.VALID_SIZE # 20_000 # args.valid_size\n",
    "    # valid = shuffled_parsed_ds.take(valid_size)\n",
    "    # train = shuffled_parsed_ds.skip(valid_size)\n",
    "    # cached_train = train.batch(args.batch_size * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "    # cached_valid = valid.batch(args.batch_size * strategy.num_replicas_in_sync).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging metaparams & hyperparams for Vertex Experiments')\n",
    "    \n",
    "    EXPERIMENT_NAME = f\"{args.experiment_name}\"\n",
    "    RUN_NAME = f\"{args.experiment_run}\"\n",
    "    logging.info(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\\n RUN_NAME: {RUN_NAME}\")\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{EXPERIMENT_NAME}'\n",
    "    metaparams[\"experiment_run\"] = f\"{RUN_NAME}\"\n",
    "    metaparams[\"model_version\"] = f\"{args.model_version}\"\n",
    "    metaparams[\"pipe_version\"] = f\"{args.pipeline_version}\"\n",
    "    metaparams[\"data_regime\"] = f\"{args.data_regime}\"\n",
    "    metaparams[\"distribute\"] = f'{args.distribute}'\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"batch_size\"] = int(args.batch_size)\n",
    "    hyperparams[\"embedding_dim\"] = args.embedding_dim\n",
    "    hyperparams[\"projection_dim\"] = args.projection_dim\n",
    "    hyperparams[\"use_cross_layer\"] = cfg.USE_CROSS_LAYER # args.use_cross_layer\n",
    "    hyperparams[\"use_dropout\"] = cfg.USE_DROPOUT # args.use_dropout\n",
    "    hyperparams[\"dropout_rate\"] = args.dropout_rate\n",
    "    hyperparams['layer_sizes'] = args.layer_sizes\n",
    "    \n",
    "    logging.info(f\"Creating run: {RUN_NAME}; for experiment: {EXPERIMENT_NAME}\")\n",
    "    \n",
    "    # Create experiment\n",
    "    vertex_ai.init(experiment=EXPERIMENT_NAME)\n",
    "    # vertex_ai.start_run(RUN_NAME,resume=True) # RUN_NAME\n",
    "    \n",
    "    with vertex_ai.start_run(RUN_NAME) as my_run:\n",
    "        logging.info(f\"logging metaparams\")\n",
    "        my_run.log_params(metaparams)\n",
    "        \n",
    "        logging.info(f\"logging hyperparams\")\n",
    "        my_run.log_params(hyperparams)\n",
    "        \n",
    "    # ====================================================\n",
    "    # Compile, Adapt, and Train model\n",
    "    # ====================================================\n",
    "    logging.info('Setting model adapts and compiling the model')\n",
    "    \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "    \n",
    "    logging.info(f'adapting layers: {cfg.NEW_ADAPTS}') # args.new_adapts | cfg.NEW_ADAPTS\n",
    "    \n",
    "    # Wrap variable creation within strategy scope\n",
    "    with strategy.scope():\n",
    "\n",
    "        model = trainer_model.TheTwoTowers(LAYER_SIZES, vocab_dict_load, parsed_candidate_dataset)\n",
    "        \n",
    "        # model.query_tower.pl_name_text_embedding.layers[0].adapt(shuffled_parsed_train_ds.map(lambda x: x['name']).batch(args.batch_size)) # TODO: use cached_train or shuffled_parsed_train_ds ?\n",
    "        # artist_name_can\n",
    "        # track_name_can\n",
    "        # album_name_can\n",
    "        # artist_genres_can\n",
    "        \n",
    "        # if cfg.NEW_ADAPTS:\n",
    "            # model.query_tower.pl_name_text_embedding.layers[0].adapt(shuffled_parsed_ds.map(lambda x: x['name']).batch(args.batch_size)) # TODO: adapts on full dataset or train onl\n",
    "            \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adagrad(args.learning_rate))\n",
    "        \n",
    "    # if cfg.NEW_ADAPTS:\n",
    "        # vocab_dict_load['name'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "        # bucket = storage_client.bucket(args.train_output_gcs_bucket)                               # TODO: args.train_output_gcs_bucket # replaced args.model_dir\n",
    "        # blob = bucket.blob(f'{EXPERIMENT_NAME}/{RUN_NAME}/vocabs_stats/vocab_dict_{RUN_NAME}.txt') # replaced f'{args.version}/vocabs_stats/vocab_dict_{RUN_NAME}.txt'\n",
    "        # pickle_out = pkl.dumps(vocab_dict_load)\n",
    "        # blob.upload_from_string(pickle_out)\n",
    "    \n",
    "    logging.info('Adapts finish - training next')\n",
    "        \n",
    "    tf.random.set_seed(args.seed)\n",
    "    \n",
    "    logs_dir = f'gs://{args.train_output_gcs_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs'         # replaced f\"{WORKING_DIR}/tb-logs-{RUN_NAME}\" \n",
    "    AIP_LOGS = os.environ.get('AIP_TENSORBOARD_LOG_DIR', f'{logs_dir}')\n",
    "    logging.info(f'TensorBoard logdir: {AIP_LOGS}')\n",
    "    \n",
    "    cloud_profiler.init()\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=AIP_LOGS,\n",
    "        histogram_freq=0, \n",
    "        write_graph=True, \n",
    "        # profile_batch = '500,520'\n",
    "    )\n",
    "    \n",
    "    # if os.environ.get('AIP_TENSORBOARD_LOG_DIR', 'NA') is not 'NA':\n",
    "    #     tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    #         log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'],\n",
    "    #         histogram_freq=0, write_graph=True, profile_batch = '500,520')\n",
    "    # else:\n",
    "    #     os.mkdir('/tb_logs')\n",
    "    #     tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    #         log_dir='/tb_logs',\n",
    "    #         histogram_freq=0)\n",
    "    \n",
    "    logging.info('Training starting')\n",
    "    layer_history = model.fit(\n",
    "        train_dataset,\n",
    "        # validation_data=valid_dataset,\n",
    "        # validation_freq=args.valid_frequency, # no longer used due to long-running brute force see scann validation belo\n",
    "        callbacks=tensorboard_callback,\n",
    "        # steps_per_epoch=10, #for debugging purposes\n",
    "        epochs=args.num_epochs,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Determine type and task of the machine from the strategy cluster resolver\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id = None, None\n",
    "\n",
    "    song_embeddings = parsed_candidate_dataset.batch(2048).map(model.candidate_tower, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    logging.info(\"Creating scann layer for approximate validation metrics\")\n",
    "    # Compute predictions\n",
    "    scann = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves_to_search=30\n",
    "    )\n",
    "    scann.index_from_dataset(song_embeddings)\n",
    "    with strategy.scope():\n",
    "        model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=scann)\n",
    "        model.compile()\n",
    "    logging.info(\"custom scann layer generation for validation complete\")\n",
    "    #TODO - perhaps output the scann layer for indexing use if needed\n",
    "\n",
    "    # ====================================================\n",
    "    # Eval Metrics\n",
    "    # ====================================================\n",
    "    logging.info('Getting evaluation metrics')\n",
    "\n",
    "    val_metrics = model.evaluate(\n",
    "        valid_dataset,\n",
    "        verbose=\"auto\",\n",
    "        return_dict=True,\n",
    "        callbacks=tensorboard_callback,\n",
    "    ) #check performance\n",
    "\n",
    "    logging.info('Validation metrics below:')\n",
    "    logging.info(val_metrics)\n",
    "    \n",
    "    with vertex_ai.start_run(RUN_NAME,resume=True) as my_run:\n",
    "        logging.info(f\"logging metrics to experiment run {RUN_NAME}\")\n",
    "        my_run.log_metrics(val_metrics)\n",
    "    \n",
    "    # logging.info(f\"Ending experiment run: {RUN_NAME}\")\n",
    "    # vertex_ai.end_run()\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save Towers\n",
    "    # ====================================================\n",
    "    \n",
    "    # logging.info(f'Saving models to {args.model_dir}')                                        # TODO: f'gs://args.train_output_gcs_bucket/{EXPERIMENT_NAME}/{RUN_NAME}/model-dir\n",
    "    MODEL_DIR_GCS_URI = f'gs://{args.train_output_gcs_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}/model-dir'\n",
    "    logging.info(f'Saving models to {MODEL_DIR_GCS_URI}')\n",
    "\n",
    "    query_dir_save = f\"{MODEL_DIR_GCS_URI}/query_tower/\"                                      # replaced: f\"gs://{args.model_dir}/{args.version}/{RUN_NAME}/query_tower/\" \n",
    "    candidate_dir_save = f\"{MODEL_DIR_GCS_URI}/candidate_tower/\"                              # replaced: f\"gs://{args.model_dir}/{args.version}/{RUN_NAME}/candidate_tower/\"\n",
    "    logging.info(f'Saving chief query model to {query_dir_save}')\n",
    "    \n",
    "    # save model from primary node in multiworker\n",
    "    if _is_chief(task_type, task_id):\n",
    "        tf.saved_model.save(model.query_tower, query_dir_save)\n",
    "        logging.info(f'Saved chief query model to {query_dir_save}')\n",
    "        tf.saved_model.save(model.candidate_tower, candidate_dir_save)\n",
    "        logging.info(f'Saved chief candidate model to {candidate_dir_save}')\n",
    "    else:\n",
    "        worker_dir_query = query_dir_save + '/workertemp_query_/' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir_query)\n",
    "        tf.saved_model.save(model.query_tower, worker_dir_query)\n",
    "        logging.info(f'Saved worker: {task_id} query model to {worker_dir_query}')\n",
    "\n",
    "        worker_dir_can = candidate_dir_save + '/workertemp_can_/' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir_can)\n",
    "        tf.saved_model.save(model.candidate_tower, worker_dir_can)\n",
    "        logging.info(f'Saved worker: {task_id} candidate model to {worker_dir_can}')\n",
    "\n",
    "    if not _is_chief(task_type, task_id):\n",
    "        tf.io.gfile.rmtree(worker_dir_can)\n",
    "        tf.io.gfile.rmtree(worker_dir_query)\n",
    "\n",
    "    logging.info('All done - model saved') #all done\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command line arguments\n",
    "    \n",
    "    type: int, float, str\n",
    "          bool() converts empty strings to `False` and non-empty strings to `True`\n",
    "          see more details here: https://docs.python.org/3/library/argparse.html#type\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir',\n",
    "                        default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir', required=False) # TODO: sunset this arg\n",
    "    \n",
    "    parser.add_argument('--train_output_gcs_bucket',\n",
    "                        default=os.getenv('AIP_MODEL_DIR'), type=str, help='bucket for train job output', required=False) # TODO: use this\n",
    "    \n",
    "    parser.add_argument('--train_dir', \n",
    "                        type=str, help='bucket holding training files', required=True)\n",
    "    \n",
    "    parser.add_argument('--train_dir_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "    \n",
    "    parser.add_argument('--valid_dir', \n",
    "                        type=str, help='bucket holding valid files', required=True)\n",
    "    \n",
    "    parser.add_argument('--valid_dir_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "    \n",
    "    parser.add_argument('--candidate_file_dir', \n",
    "                        type=str, help='bucket holding candidate files', required=True)\n",
    "\n",
    "    parser.add_argument('--candidate_files_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "\n",
    "    parser.add_argument('--project', \n",
    "                        type=str, help='project', required=True)\n",
    "\n",
    "    parser.add_argument('--max_padding', \n",
    "                        default=375, type=int, help='max_padding', required=False)\n",
    "\n",
    "    parser.add_argument('--experiment_name', \n",
    "                        type=str, help='#TODO', required=True)\n",
    "\n",
    "    parser.add_argument('--experiment_run', \n",
    "                        type=str, help='#TODO', required=True)\n",
    "\n",
    "    parser.add_argument('--num_epochs', \n",
    "                        default=1, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--batch_size', \n",
    "                        default=128, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--embedding_dim', \n",
    "                        default=32, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--projection_dim', \n",
    "                        default=5, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--seed', \n",
    "                        default=1234, type=str, help='#TODO', required=False)\n",
    "\n",
    "#     parser.add_argument('--use_cross_layer', \n",
    "#                         default=True, type=bool, help='#TODO', required=False)\n",
    "\n",
    "#     parser.add_argument('--use_dropout', \n",
    "#                         default=False, type=bool, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--dropout_rate', \n",
    "                        default=0.4, type=float, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--layer_sizes', \n",
    "                        default='[64,32]', type=str, help='#TODO', required=False)\n",
    "\n",
    "    # parser.add_argument('--aip_tb_logs', \n",
    "    #                     default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str, help='#TODO', required=False)\n",
    "\n",
    "    # parser.add_argument('--new_adapts', \n",
    "    #                     default=False, type=bool, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--learning_rate', \n",
    "                        default=0.01, type=float, help='learning rate', required=False)\n",
    "\n",
    "    # parser.add_argument('--valid_size', \n",
    "    #                     default='#TODO', type=str, help='number of records in valid split', required=False)\n",
    "\n",
    "    parser.add_argument('--valid_frequency', \n",
    "                        default=10, type=int, help='number of epochs per metrics val calculation', required=False)\n",
    "\n",
    "    parser.add_argument('--distribute', \n",
    "                        default='single', type=str, help='TF strategy: single, mirrored, multiworker, tpu', required=False)\n",
    "\n",
    "    # parser.add_argument('--version', \n",
    "    #                     type=str, help='version of train code; for tracking', required=True)\n",
    "    \n",
    "    parser.add_argument('--model_version', \n",
    "                        type=str, help='version of model train code', required=True)\n",
    "    \n",
    "    parser.add_argument('--pipeline_version', \n",
    "                        type=str, help='version of pipeline code; v0 for non-pipeline execution', required=True)\n",
    "    \n",
    "    parser.add_argument('--data_regime', \n",
    "                        type=str, help='id for tracking different datasets', required=True)\n",
    "\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    return parser.parse_args()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting jobs main() script')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/spotify-tfrs/src\u001b[00m\n",
      " Dockerfile.tfrs\n",
      " README.md\n",
      " cloudbuild.yaml\n",
      " downloaded_vocabs.txt\n",
      " \u001b[01;34mpipelines\u001b[00m\n",
      "  build_custom_train_image.py\n",
      "  build_vocabs_stats.py\n",
      "  create_tensorboard.py\n",
      "  find_model_endpoint_test.py\n",
      "  generate_candidate_embedding_index.py\n",
      "  train_custom_model.py\n",
      " \u001b[01;34mtrainer\u001b[00m\n",
      "     __init__.py\n",
      "     data_src.py\n",
      "     interactive_train.py\n",
      "     model_src.py\n",
      "     requirements.txt\n",
      "     task.py\n",
      "     train_config.py\n",
      "\n",
      "2 directories, 17 files\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/spotify-tfrs/src\n",
    "#/vertex_train/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Worker Pool Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=b\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# # Single machine, single GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-2g'\n",
    "# REPLICA_COUNT = 1\n",
    "# # ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# # # Single Machine; multiple GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-4g' # a2-ultragpu-4g\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 4\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# # # Multiple Machines, 1 GPU per Machine\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 9\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write `train_config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/trainer/train_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/train_config.py\n",
    "\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "\n",
    "NEW_ADAPTS = 'True'\n",
    "USE_CROSS_LAYER = 'True'\n",
    "USE_DROPOUT = 'True'\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "MAX_PADDING = 5 # this should improve performance vs 375\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "SEED = 1234\n",
    "DROPOUT_RATE = 0.4\n",
    "TOKEN_DICT = '20000_tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previously defined VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: hybrid-vertex\n",
      "APP: sp\n",
      "MODEL_TYPE: 2tower\n",
      "FRAMEWORK: tfrs\n",
      "MODEL_VERSION: jwv4-approx-valid-one-epoch\n",
      "PIPELINE_VERSION: v1\n",
      "\n",
      "MODEL_ROOT_NAME: sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1\n",
      "OUTPUT_BUCKET: jt-tfrs-test\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT: {PROJECT}\")\n",
    "\n",
    "print(f\"APP: {APP}\")\n",
    "print(f\"MODEL_TYPE: {MODEL_TYPE}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_VERSION: {MODEL_VERSION}\")\n",
    "print(f\"PIPELINE_VERSION: {PIPELINE_VERSION}\\n\")\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")\n",
    "print(f\"OUTPUT_BUCKET: {OUTPUT_BUCKET}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--train_output_gcs_bucket=jt-tfrs-test',\n",
      "                              '--train_dir=spotify-beam-v3',\n",
      "                              '--train_dir_prefix=v3/dif_artist/',\n",
      "                              '--valid_dir=spotify-beam-v3',\n",
      "                              '--valid_dir_prefix=v3/dif_artist_valid/',\n",
      "                              '--candidate_file_dir=spotify-beam-v3',\n",
      "                              '--candidate_files_prefix=v3/candidates/',\n",
      "                              '--experiment_name=dev-2tower-tfrs-jwv4-approx-valid-one-epoch',\n",
      "                              '--experiment_run=run-20221003-033240',\n",
      "                              '--num_epochs=1',\n",
      "                              '--batch_size=2048',\n",
      "                              '--embedding_dim=64',\n",
      "                              '--projection_dim=5',\n",
      "                              '--layer_sizes=[128,64]',\n",
      "                              '--learning_rate=0.01',\n",
      "                              '--valid_frequency=1',\n",
      "                              '--distribute=mirrored',\n",
      "                              '--model_version=jwv4-approx-valid-one-epoch',\n",
      "                              '--pipeline_version=v1',\n",
      "                              '--data_regime=dif-artist-beam-tfrecord'],\n",
      "                     'command': ['python', 'trainer/task.py'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training'},\n",
      "  'machine_spec': {'accelerator_count': 4,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-ultragpu-1g'},\n",
      "  'replica_count': 1}]\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# from trainer import train_config as config\n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# # \"gs://spotify-tfrecords-blog/tfrecords_v1/train/output-00000-of-00796.tfrecord\"\n",
    "# # gs://spotify-tfrs-dir/small-dataset/output-00000-of-00796.tfrecord\n",
    "\n",
    "# GCS buckets & paths to source data\n",
    "CANDIDATE_FILE_DIR = 'spotify-beam-v3'      #'spotify-tfrecords-blog'\n",
    "CANDIDATE_PREFIX = 'v3/candidates/'         # 'tfrecords_v1/train/'\n",
    "\n",
    "TRAIN_DIR = 'spotify-beam-v3'               #'spotify-tfrecords-blog'\n",
    "TRAIN_DIR_PREFIX = 'v3/dif_artist/'         # 'tfrecords_v1/train/'\n",
    "\n",
    "VALID_DIR = 'spotify-beam-v3'               #'spotify-tfrecords-blog'\n",
    "VALID_DIR_PREFIX = 'v3/dif_artist_valid/'   # 'tfrecords_v1/train/'\n",
    "\n",
    "# MODEL_DIR='spotify-tfrs-dir'  \n",
    "# OUTPUT_BUCKET = 'jt-tfrs-test' # replaced MODEL_DIR='spotify-tfrs-dir' \n",
    "\n",
    "EXPERIMENT_PREFIX = 'dev'                                   # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}'\n",
    "RUN_NAME=f'run-{TIMESTAMP}'\n",
    "DATA_REGIME = 'dif-artist-beam-tfrecord' # 'full-beam-tfrecord'\n",
    "\n",
    "VALID_FREQUENCY = 1 #change to 10\n",
    "# VALID_SIZE = 20_000\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 2048 # 2048\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "MAX_PADDING = 5\n",
    "EMBEDDING_DIM = 64\n",
    "PROJECTION_DIM = 5\n",
    "\n",
    "DROPOUT_RATE = 0.4\n",
    "LAYER_SIZES = '[128,64]'\n",
    "\n",
    "WORKER_CMD = [\"python\", \"trainer/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT}',\n",
    "    f'--train_output_gcs_bucket={OUTPUT_BUCKET}',\n",
    "    f'--train_dir={TRAIN_DIR}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={VALID_DIR}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    # f'--model_dir={MODEL_DIR}',\n",
    "    f'--candidate_file_dir={CANDIDATE_FILE_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={MODEL_VERSION}',\n",
    "    f'--pipeline_version={PIPELINE_VERSION}',\n",
    "    f'--data_regime={DATA_REGIME}',\n",
    "]\n",
    "\n",
    "# deprecated model args\n",
    "    # f'--valid_size={VALID_SIZE}',\n",
    "    # f'--new_adapts={new_adapts}',\n",
    "    # f'--use_cross_layer={use_cross_layer}',\n",
    "    # f'--use_dropout={use_dropout}',\n",
    "\n",
    "    \n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Locally\n",
    "\n",
    "* TODO: local test handle module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/spotify-tfrs/src\u001b[00m\n",
      " Dockerfile.tfrs\n",
      " README.md\n",
      " cloudbuild.yaml\n",
      " downloaded_vocabs.txt\n",
      " \u001b[01;34mpipelines\u001b[00m\n",
      "  build_custom_train_image.py\n",
      "  build_vocabs_stats.py\n",
      "  create_tensorboard.py\n",
      "  find_model_endpoint_test.py\n",
      "  generate_candidate_embedding_index.py\n",
      "  train_custom_model.py\n",
      " \u001b[01;34mtrainer\u001b[00m\n",
      "     __init__.py\n",
      "     data_src.py\n",
      "     interactive_train.py\n",
      "     model_src.py\n",
      "     requirements.txt\n",
      "     task.py\n",
      "     train_config.py\n",
      "\n",
      "2 directories, 17 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/spotify-tfrs/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-tfrs'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-tfrs')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "_DISTRIBUTE_STRATEGY='single'\n",
    "_EXPERIMENT_NAME=f'local-testing-{MODEL_VERSION}'\n",
    "_RUN_NAME=f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# !cd src/trainer; python3 task.py \\\n",
    "#     --project={PROJECT} --train_output_gcs_bucket={OUTPUT_BUCKET} --train_dir={TRAIN_DIR} --train_dir_prefix={TRAIN_DIR_PREFIX} \\\n",
    "#     --valid_dir={VALID_DIR} --valid_dir_prefix={VALID_DIR_PREFIX} \\\n",
    "#     --candidate_file_dir={CANDIDATE_FILE_DIR} --candidate_files_prefix={CANDIDATE_PREFIX} \\\n",
    "#     --experiment_name={_EXPERIMENT_NAME} --experiment_run={_RUN_NAME} \\\n",
    "#     --max_padding={MAX_PADDING} \\\n",
    "#     --num_epochs={NUM_EPOCHS} --batch_size={BATCH_SIZE} --embedding_dim={EMBEDDING_DIM} --projection_dim={PROJECTION_DIM} \\\n",
    "#     --dropout_rate={DROPOUT_RATE} --layer_sizes={LAYER_SIZES} --learning_rate={LEARNING_RATE} \\\n",
    "#     --valid_frequency={VALID_FREQUENCY} --distribute={_DISTRIBUTE_STRATEGY} \\\n",
    "#     --model_version={MODEL_VERSION} --pipeline_version={PIPELINE_VERSION} \\\n",
    "#     --data_regime={DATA_REGIME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Custom Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME: tfrs\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training\n",
      "FILE_LOCATION: ./src\n",
      "MACHINE_TYPE: e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-tfrs'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-tfrs')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Cloud Build\n",
    "\n",
    "This will build the training container used in Vertex Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 27 file(s) totalling 402.1 MiB before compression.\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1664767972.729897-66b438c3b88941608d032da1f42ae1e3.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/aa92e186-6bdd-4b22-8d48-ba13cce4b104].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/aa92e186-6bdd-4b22-8d48-ba13cce4b104?project=934903580331 ].\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"aa92e186-6bdd-4b22-8d48-ba13cce4b104\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1664767972.729897-66b438c3b88941608d032da1f42ae1e3.tgz#1664768168308182\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1664767972.729897-66b438c3b88941608d032da1f42ae1e3.tgz#1664768168308182...\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "| [1 files][129.8 MiB/129.8 MiB]                                                \n",
      "Operation completed over 1 objects/129.8 MiB.\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Sending build context to Docker daemon  420.5MB\n",
      "Step 1/4 : FROM tensorflow/tensorflow:2.8.2-gpu\n",
      "2.8.2-gpu: Pulling from tensorflow/tensorflow\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "50b37fabf1b7: Pulling fs layer\n",
      "269c6117408b: Pulling fs layer\n",
      "5ec4361a6d52: Pulling fs layer\n",
      "a490261931bd: Pulling fs layer\n",
      "844fbc8fd6b0: Pulling fs layer\n",
      "4fabf614524b: Pulling fs layer\n",
      "740ccd578ab1: Pulling fs layer\n",
      "0109560da7c9: Pulling fs layer\n",
      "0a4aed53d5e8: Pulling fs layer\n",
      "398903116ae5: Pulling fs layer\n",
      "30c6be94d3fd: Pulling fs layer\n",
      "d618b4133209: Pulling fs layer\n",
      "bb2b5c16eb9e: Pulling fs layer\n",
      "a490261931bd: Waiting\n",
      "4fabf614524b: Waiting\n",
      "740ccd578ab1: Waiting\n",
      "30c6be94d3fd: Waiting\n",
      "0109560da7c9: Waiting\n",
      "d618b4133209: Waiting\n",
      "0a4aed53d5e8: Waiting\n",
      "bb2b5c16eb9e: Waiting\n",
      "398903116ae5: Waiting\n",
      "5ec4361a6d52: Waiting\n",
      "269c6117408b: Verifying Checksum\n",
      "269c6117408b: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "5ec4361a6d52: Download complete\n",
      "a490261931bd: Verifying Checksum\n",
      "a490261931bd: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "50b37fabf1b7: Verifying Checksum\n",
      "50b37fabf1b7: Download complete\n",
      "50b37fabf1b7: Pull complete\n",
      "4fabf614524b: Verifying Checksum\n",
      "4fabf614524b: Download complete\n",
      "740ccd578ab1: Verifying Checksum\n",
      "740ccd578ab1: Download complete\n",
      "269c6117408b: Pull complete\n",
      "5ec4361a6d52: Pull complete\n",
      "a490261931bd: Pull complete\n",
      "0a4aed53d5e8: Verifying Checksum\n",
      "0a4aed53d5e8: Download complete\n",
      "0109560da7c9: Verifying Checksum\n",
      "0109560da7c9: Download complete\n",
      "398903116ae5: Verifying Checksum\n",
      "398903116ae5: Download complete\n",
      "d618b4133209: Verifying Checksum\n",
      "d618b4133209: Download complete\n",
      "bb2b5c16eb9e: Verifying Checksum\n",
      "bb2b5c16eb9e: Download complete\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "30c6be94d3fd: Verifying Checksum\n",
      "30c6be94d3fd: Download complete\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "844fbc8fd6b0: Verifying Checksum\n",
      "844fbc8fd6b0: Download complete\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "844fbc8fd6b0: Pull complete\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "4fabf614524b: Pull complete\n",
      "740ccd578ab1: Pull complete\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "0109560da7c9: Pull complete\n",
      "0a4aed53d5e8: Pull complete\n",
      "398903116ae5: Pull complete\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "30c6be94d3fd: Pull complete\n",
      "d618b4133209: Pull complete\n",
      "bb2b5c16eb9e: Pull complete\n",
      "Digest: sha256:fffa0c1556a257afdac73137bb342e323346d75b5ba091b833151fb5d4b4bfb9\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:2.8.2-gpu\n",
      " ---> da05ec95e63b\n",
      "Step 2/4 : WORKDIR /src\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      " ---> Running in 5380b8d8c5bc\n",
      "Removing intermediate container 5380b8d8c5bc\n",
      " ---> 745b72201708\n",
      "Step 3/4 : COPY trainer/* trainer/\n",
      " ---> f645a421d09f\n",
      "Step 4/4 : RUN pip install -r trainer/requirements.txt\n",
      " ---> Running in f2d5d5f2c2ff\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Collecting google-cloud-aiplatform==1.17.0\n",
      "  Downloading google_cloud_aiplatform-1.17.0-py2.py3-none-any.whl (2.2 MB)\n",
      "Collecting tensorflow-recommenders==0.6.0\n",
      "  Downloading tensorflow_recommenders-0.6.0-py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: tensorboard==2.8.0 in /usr/local/lib/python3.8/dist-packages (from -r trainer/requirements.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.8/dist-packages (from -r trainer/requirements.txt (line 5)) (0.6.1)\n",
      "Collecting tensorboard-plugin-profile==2.5.0\n",
      "  Downloading tensorboard_plugin_profile-2.5.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting scann\n",
      "  Downloading scann-1.2.6-cp38-cp38-manylinux2014_x86_64.whl (10.9 MB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.6.2-py2.py3-none-any.whl (233 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.10.1-py3-none-any.whl (115 kB)\n",
      "Requirement already satisfied: protobuf<5.0.0dev,>=3.19.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform==1.17.0->-r trainer/requirements.txt (line 2)) (3.19.4)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (0.34.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (2.6.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (1.22.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (62.3.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (2.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (3.3.7)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (1.46.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorboard-plugin-profile==2.5.0->-r trainer/requirements.txt (line 6)) (1.14.0)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (0.26.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (1.14.1)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (14.0.1)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (4.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.6.0->tensorflow-recommenders==0.6.0->-r trainer/requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (4.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (4.11.4)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard==2.8.0->-r trainer/requirements.txt (line 4)) (3.8.0)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3972 sha256=28fd804e64db309f4eb72053e96bac0efd0b4f13648886ff2f1da69bf75afd1f\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/fb/3b/365271726c73d8bc0b5bf39ef0f5db5a9c75b2babe4fd67794\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: googleapis-common-protos, grpc-google-iam-v1, grpcio-status, google-api-core, proto-plus, google-cloud-resource-manager, google-crc32c, google-resumable-media, google-cloud-core, google-cloud-storage, pyparsing, packaging, python-dateutil, google-cloud-bigquery, google-cloud-aiplatform, tensorflow-recommenders, gviz-api, tensorboard-plugin-profile, scann, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 google-api-core-2.10.1 google-cloud-aiplatform-1.17.0 google-cloud-bigquery-2.34.4 google-cloud-core-2.3.2 google-cloud-resource-manager-1.6.2 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-resumable-media-2.4.0 googleapis-common-protos-1.56.4 grpc-google-iam-v1-0.12.4 grpcio-status-1.49.1 gviz-api-1.10.0 packaging-21.3 proto-plus-1.22.1 pyparsing-3.0.9 python-dateutil-2.8.2 scann-1.2.6 tensorboard-plugin-profile-2.5.0 tensorflow-recommenders-0.6.0\n",
      "\u001b[91mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "grpcio-status 1.49.1 requires grpcio>=1.49.1, but you'll have grpcio 1.46.3 which is incompatible.\n",
      "grpcio-status 1.49.1 requires protobuf>=4.21.3, but you'll have protobuf 3.19.4 which is incompatible.\n",
      "google-api-core 2.10.1 requires protobuf<5.0.0dev,>=3.20.1, but you'll have protobuf 3.19.4 which is incompatible.\n",
      "google-cloud-resource-manager 1.6.2 requires protobuf<5.0.0dev,>=3.20.2, but you'll have protobuf 3.19.4 which is incompatible.\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container f2d5d5f2c2ff\n",
      " ---> a2c83a1b29d4\n",
      "Successfully built a2c83a1b29d4\n",
      "Successfully tagged gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training\n",
      "The push refers to repository [gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training]\n",
      "527e2e06e046: Preparing\n",
      "a47d6ec8b861: Preparing\n",
      "816531110692: Preparing\n",
      "3e4911f324e8: Preparing\n",
      "056f6d8bf473: Preparing\n",
      "8cad1679989f: Preparing\n",
      "1ace9aa322ca: Preparing\n",
      "d89aeca36b7d: Preparing\n",
      "6720a6e02d0f: Preparing\n",
      "c8b79c7a5abf: Preparing\n",
      "e04073e60732: Preparing\n",
      "cc037dbc2179: Preparing\n",
      "b6da86715aa4: Preparing\n",
      "f8cec97f25fc: Preparing\n",
      "f68b7a92fa28: Preparing\n",
      "b56f642b53f1: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "cc037dbc2179: Waiting\n",
      "b6da86715aa4: Waiting\n",
      "8cad1679989f: Waiting\n",
      "f8cec97f25fc: Waiting\n",
      "f68b7a92fa28: Waiting\n",
      "1ace9aa322ca: Waiting\n",
      "b56f642b53f1: Waiting\n",
      "d89aeca36b7d: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "6720a6e02d0f: Waiting\n",
      "c8b79c7a5abf: Waiting\n",
      "e04073e60732: Waiting\n",
      "056f6d8bf473: Layer already exists\n",
      "3e4911f324e8: Layer already exists\n",
      "1ace9aa322ca: Layer already exists\n",
      "8cad1679989f: Layer already exists\n",
      "6720a6e02d0f: Layer already exists\n",
      "d89aeca36b7d: Layer already exists\n",
      "c8b79c7a5abf: Layer already exists\n",
      "e04073e60732: Layer already exists\n",
      "cc037dbc2179: Layer already exists\n",
      "b6da86715aa4: Layer already exists\n",
      "f8cec97f25fc: Layer already exists\n",
      "f68b7a92fa28: Layer already exists\n",
      "a47d6ec8b861: Pushed\n",
      "816531110692: Pushed\n",
      "b56f642b53f1: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "527e2e06e046: Pushed\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "latest: digest: sha256:02078d1424670487122bea3babc4541742fd0790d14721e477b2da173a63986d size: 3890\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                                                 STATUS\n",
      "aa92e186-6bdd-4b22-8d48-ba13cce4b104  2022-10-03T03:36:08+00:00  2M8S      gs://hybrid-vertex_cloudbuild/source/1664767972.729897-66b438c3b88941608d032da1f42ae1e3.tgz  gcr.io/hybrid-vertex/sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1-training (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit train job to Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Could not load library libcudnn_adv_train.so.8. Error: /opt/conda/bin/../lib/libcudnn_ops_train.so.8: undefined symbol: _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv, version libcudnn_ops_infer.so.8\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sp-2tower-tfrs-jwv4-approx-valid-one-epoch-v1'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ROOT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = f'train-{MODEL_ROOT_NAME}-max-pad-5-mirrored-strategy' #-{TIMESTAMP}'\n",
    "\n",
    "# e.g., MODEL_DIR_GCS_URI = f'gs://{args.train_output_gcs_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}/model-dir'\n",
    "BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{MODEL_ROOT_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "\n",
    "print(f'JOB_NAME:{JOB_NAME}')\n",
    "print(f'BASE_OUTPUT_DIR:{BASE_OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/5683424807119486976 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=BASE_OUTPUT_DIR,\n",
    "    # labels={'gpu':f'{ACCELERATOR_TYPE}'}\n",
    ")\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SavedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob created. Resource name: projects/934903580331/locations/us-central1/customJobs/8549403019987386368\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/934903580331/locations/us-central1/customJobs/8549403019987386368')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8549403019987386368?project=934903580331\n",
      "View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+934903580331+locations+us-central1+tensorboards+9194643997044244480+experiments+8549403019987386368\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 23:06:33.237029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.249391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.251187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.253232: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-02 23:06:33.255164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.256929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.258655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.921007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.922917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.924542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-02 23:06:33.926155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 access the interactive shell terminals for the custom job:\n",
      "workerpool0-0:\n",
      "5364d11638fe76b6-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/3073025873104863232 current state:\n",
      "JobState.JOB_STATE_CANCELLED\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/8549403019987386368 current state:\n",
      "JobState.JOB_STATE_CANCELLED\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "query_tower_uri = 'gs://jt-tfrs-test/dev-2tower-tfrs-jwv4-approx-valid-one-epoch/run-20220930-202044/model-dir/query_tower'\n",
    "candidate_tower_uri = 'gs://jt-tfrs-test/dev-2tower-tfrs-jwv4-approx-valid-one-epoch/run-20220930-202044/model-dir/candidate_tower'\n",
    "loaded_query_model = tf.saved_model.load(query_tower_uri)\n",
    "loaded_candidate_model = tf.saved_model.load(candidate_tower_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serving_default']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_candidate_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_1': TensorSpec(shape=(None, 32), dtype=tf.float32, name='output_1')}\n"
     ]
    }
   ],
   "source": [
    "infer = loaded_candidate_model.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_1': TensorShape([None, 32])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 access the interactive shell terminals for the custom job:\n",
      "workerpool0-0:\n",
      "4fb756319cc74dce-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/456170606811938816 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "predict2 = loaded_candidate_model.signatures['serving_default']\n",
    "predict2.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, artist_pop_can, album_name_can, artist_uri_can, track_pop_can, track_uri_can, artist_genres_can, artist_followers_can, duration_ms_can, artist_name_can, album_uri_can, track_name_can) at 0x7FC79C510ED0>})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_candidate_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_dataset_candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18182/142102832.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m embs_iter = parsed_dataset_candidates.batch(1).map(lambda data: predict2(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0martist_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"artist_name_can\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mtrack_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_name_can'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0malbum_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'album_name_can'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mtrack_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_uri_can'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_dataset_candidates' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/292211432877981696 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "embs_iter = parsed_dataset_candidates.batch(1).map(lambda data: predict2(\n",
    "                artist_name = data[\"artist_name_can\"],\n",
    "                track_name = data['track_name_can'],\n",
    "                album_name = data['album_name_can'],\n",
    "                track_uri = data['track_uri_can'],\n",
    "                artist_uri = data['artist_uri_can'],\n",
    "                album_uri = data['album_uri_can'],\n",
    "                duration_ms = data['duration_ms_can'],\n",
    "                track_pop = data['track_pop_can'],\n",
    "                artist_pop = data['artist_pop_can'],\n",
    "                artist_followers = data['artist_followers_can'],\n",
    "                artist_genres = data['artist_genres_can']))\n",
    "\n",
    "    \n",
    "candidate_features = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "> adapts vs vocab_dict\n",
    "\n",
    "```\n",
    "test_playlist_model = Playlist_Model(layer_sizes, vocab_dict_load)\n",
    "test_playlist_model.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(1000))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_duration_ms_seed_pl = 13000151.68\n",
    "var_duration_ms_seed_pl = 133092900971233.58\n",
    "vocab_dict_load['avg_duration_ms_seed_pl']=avg_duration_ms_seed_pl\n",
    "vocab_dict_load['var_duration_ms_seed_pl']=var_duration_ms_seed_pl\n",
    "\n",
    "avg_n_songs_pl = 55.21\n",
    "var_n_songs_pl = 2317.54\n",
    "vocab_dict_load['avg_n_songs_pl']=avg_n_songs_pl\n",
    "vocab_dict_load['var_n_songs_pl']=var_n_songs_pl\n",
    "\n",
    "avg_n_artists_pl = 30.56\n",
    "var_n_artists_pl = 769.26\n",
    "vocab_dict_load['avg_n_artists_pl']=avg_n_artists_pl\n",
    "vocab_dict_load['var_n_artists_pl']=var_n_artists_pl\n",
    "\n",
    "avg_n_albums_pl = 40.25\n",
    "var_n_albums_pl = 1305.54\n",
    "vocab_dict_load['avg_n_albums_pl']=avg_n_albums_pl\n",
    "vocab_dict_load['var_n_albums_pl']=var_n_albums_pl\n",
    "\n",
    "avg_artist_pop = 16.08\n",
    "var_artist_pop = 300.64\n",
    "vocab_dict_load['avg_artist_pop']=avg_artist_pop\n",
    "vocab_dict_load['var_artist_pop']=var_artist_pop\n",
    "\n",
    "avg_duration_ms_songs_pl = 234823.14\n",
    "var_duration_ms_songs_pl = 5558806228.41\n",
    "vocab_dict_load['avg_duration_ms_songs_pl']=avg_duration_ms_songs_pl\n",
    "vocab_dict_load['var_duration_ms_songs_pl']=var_duration_ms_songs_pl\n",
    "\n",
    "avg_artist_followers = 43337.77\n",
    "var_artist_followers = 377777790193.57\n",
    "vocab_dict_load['avg_artist_followers']=avg_artist_followers\n",
    "vocab_dict_load['var_artist_followers']=var_artist_followers\n",
    "\n",
    "avg_track_pop = 10.85\n",
    "var_track_pop = 202.18\n",
    "vocab_dict_load['avg_track_pop']=avg_track_pop\n",
    "vocab_dict_load['var_track_pop']=var_track_pop\n",
    "# vocab_dict_load['unique_pids_string']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archived Dockerfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# # Dockerfile-gpu\n",
    "# FROM gcr.io/deeplearning-platform-release/tf-gpu.2-9\n",
    "\n",
    "# WORKDIR /src\n",
    "\n",
    "# # Copies the trainer code to the docker image.\n",
    "# COPY trainer/* trainer/ \n",
    "\n",
    "# RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m97"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
