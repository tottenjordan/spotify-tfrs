{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE-W6E3Cy66L"
   },
   "source": [
    "# Distributed Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HtMh17BUy6Pc"
   },
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_recommenders as tfrs\n",
    "# import tensorflow_io as tfio\n",
    "\n",
    "# from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HrKzx8b5zDHV"
   },
   "outputs": [],
   "source": [
    "# PREFIX = 'spotify-2tower'\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "MODEL_VERSION = 'jtv2-scann'\n",
    "PIPELINE_VERSION = 'v0'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}-{PIPELINE_VERSION}'\n",
    "\n",
    "PROJECT= 'hybrid-vertex'\n",
    "REGION='us-central1'\n",
    "# BUCKET_NAME='spotify-tfrecords-blog'\n",
    "OUTPUT_BUCKET = 'jt-tfrs-test'\n",
    "STAGING_BUCKET =f'gs://{OUTPUT_BUCKET}'\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_NAME = f'{MODEL_ROOT_NAME}-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = 'tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'\n",
    "\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kj-KiRcvzfuk"
   },
   "outputs": [],
   "source": [
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_DISPLAY_NAME = f\"{MODEL_ROOT_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "\n",
    "tensorboard_resource_name = tensorboard.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD = 'projects/934903580331/locations/us-central1/tensorboards/4842196432167370752'\n",
    "# TENSORBOARD = 'projects/934903580331/locations/us-central1/tensorboards/5764308455871479808'\n",
    "\n",
    "# TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/8299553571104358400\"\n",
    "# TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/9194643997044244480\"\n",
    "TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/5595704944821796864\"\n",
    "\n",
    "# tb = aiplatform.Tensorboard('projects/934903580331/locations/us-central1/tensorboards/2710867908514283520')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZbeq5FC0NBf"
   },
   "source": [
    "## Perepare Vertex Training Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create repo for training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QLUTCt34zx3e"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "\n",
    "# Make folder for Python training script\n",
    "# ! rm -rf {REPO_DOCKER_PATH_PREFIX}\n",
    "# ! mkdir {REPO_DOCKER_PATH_PREFIX}\n",
    "\n",
    "# Add package information\n",
    "# ! touch {REPO_DOCKER_PATH_PREFIX}/README.md\n",
    "\n",
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "! touch {REPO_DOCKER_PATH_PREFIX}/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interactive training shell in Vertex AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gcloud compute images list \\\n",
    "        --project deeplearning-platform-release \\\n",
    "        --no-standard-images\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "gcloud compute images describe-from-family IMAGE_FAMILY \\\n",
    "        --project deeplearning-platform-release\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM tensorflow/tensorflow:2.8.2-gpu\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer/* trainer/ \n",
    "\n",
    "RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93dxYpL00XqU"
   },
   "source": [
    "### requirements.txt\n",
    "\n",
    "* TODO: for profiling, install `google-cloud-aiplatform[cloud_profiler]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "L3Xvv9Nc0YCw"
   },
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/requirements.txt\n",
    "\n",
    "google-cloud-aiplatform==1.17.0\n",
    "tensorflow-recommenders==0.6.0\n",
    "tensorboard==2.8.0\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-profile==2.5.0\n",
    "scann\n",
    "cloudml-hypertune\n",
    "google-cloud-aiplatform[cloud_profiler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google-cloud-aiplatform==1.17.0\n",
    "# tensorflow==2.9.2\n",
    "# tensorflow-cloud==0.1.16\n",
    "# tensorflow-datasets==4.4.0\n",
    "# tensorflow-estimator==2.9.0\n",
    "# tensorflow-hub==0.12.0\n",
    "# tensorflow-io==0.23.1\n",
    "# tensorflow-io-gcs-filesystem==0.27.0\n",
    "# tensorflow-metadata==1.10.0\n",
    "# tensorflow-recommenders==0.7.0\n",
    "# tensorflow-serving-api==2.10.0\n",
    "# tensorflow-transform==1.10.1\n",
    "# tensorboard==2.9.1\n",
    "# tensorboard-data-server==0.6.1\n",
    "# tensorboard-plugin-profile==2.5.0\n",
    "# cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir {REPO_DOCKER_PATH_PREFIX}/trainer/trainer_src\n",
    "# !touch {REPO_DOCKER_PATH_PREFIX}/trainer/trainer_src/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/data_src.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import train_config as cfg\n",
    "\n",
    "MAX_PLAYLIST_LENGTH = cfg.MAX_PADDING # 375\n",
    "\n",
    "def pad_up_to(t, max_in_dims=[1 ,MAX_PLAYLIST_LENGTH], constant_value=''):\n",
    "    s = tf.shape(t)\n",
    "    paddings = [[0, m-s[i]] for (i,m) in enumerate(max_in_dims)]\n",
    "    return tf.pad(t, paddings, 'CONSTANT', constant_values=constant_value)\n",
    "\n",
    "def return_padded_tensors(context, data):\n",
    "    \n",
    "    a = data['track_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    b = data['artist_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    c = data['album_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    d = data['track_uri_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    e = data['duration_ms_songs_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    f = data['artist_pop_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    g = data['artists_followers_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    h = data['track_pop_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "    i = data['artist_genres_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        \n",
    "    padded_data = context.copy()\n",
    "    padded_data['track_name_pl'] = a\n",
    "    padded_data['artist_name_pl'] = b\n",
    "    padded_data['album_name_pl'] = c\n",
    "    padded_data['track_uri_pl'] = d\n",
    "    padded_data['duration_ms_songs_pl'] = e\n",
    "    padded_data['artist_pop_pl'] = f\n",
    "    padded_data['artists_followers_pl'] = g\n",
    "    padded_data['track_pop_pl'] = h\n",
    "    padded_data['artist_genres_pl'] = i\n",
    "        \n",
    "    return padded_data\n",
    "\n",
    "candidate_features = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "}\n",
    "\n",
    "cont_feats = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'name': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'collaborative': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'n_songs_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_artists_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_albums_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'description_pl': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "}\n",
    "    ###ragged\n",
    "\n",
    "seq_feats = {\n",
    "    'track_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'artist_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'album_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'track_uri_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'duration_ms_songs_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artists_followers_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'track_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_genres_pl': tf.io.RaggedFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse_tfrecord(example):\n",
    "    example = tf.io.parse_single_sequence_example(\n",
    "        example, \n",
    "        context_features=cont_feats,\n",
    "        sequence_features=seq_feats\n",
    "    )\n",
    "    return example\n",
    "\n",
    "def parse_candidate_tfrecord_fn(example):\n",
    "    example = tf.io.parse_single_example(\n",
    "        example, \n",
    "        features=candidate_features\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/model_src.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "\n",
    "import train_config as cfg\n",
    "# ====================================================\n",
    "# Playlist (query) Tower\n",
    "# ====================================================\n",
    "\n",
    "# TODO: parameterize\n",
    "\n",
    "EMBEDDING_DIM = cfg.EMBEDDING_DIM       # 32\n",
    "PROJECTION_DIM = cfg.PROJECTION_DIM     # 5\n",
    "SEED = cfg.SEED                         # 1234\n",
    "USE_CROSS_LAYER = cfg.USE_CROSS_LAYER   # True\n",
    "DROPOUT = cfg.USE_DROPOUT               # 'False'\n",
    "DROPOUT_RATE = cfg.DROPOUT_RATE         # '0.33'\n",
    "MAX_PLAYLIST_LENGTH = cfg.MAX_PADDING   # 375\n",
    "TOKEN_DICT = cfg.TOKEN_DICT             # '20000_tokens'\n",
    "\n",
    "# MAX_PLAYLIST_LENGTH = 375\n",
    "# EMBEDDING_DIM = 32\n",
    "# PROJECTION_DIM = 5\n",
    "# SEED = 1234\n",
    "# USE_CROSS_LAYER=True\n",
    "# DROPOUT='False'\n",
    "# DROPOUT_RATE='0.33'\n",
    "# TOKEN_DICT = '20000_tokens'\n",
    "\n",
    "class Playlist_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict): #, max_padding_len):\n",
    "        super().__init__()\n",
    "\n",
    "        # ========================================\n",
    "        # non-sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: playlist name\n",
    "        self.pl_name_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS, # not needed if passing vocab\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT]['name'], \n",
    "                #     name=\"pl_name_txt_vectorizer\", \n",
    "                #     ngrams=2\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=1_000_000), #one MILLION playlists\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=1_000_000 + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_name_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"pl_name_pooling\"),\n",
    "            ], name=\"pl_name_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: collaborative\n",
    "        collaborative_vocab = np.array([b'false', b'true'])\n",
    "        \n",
    "        self.pl_collaborative_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=collaborative_vocab, \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_collaborative_lookup\", \n",
    "                    output_mode='int'\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(collaborative_vocab) + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_collaborative_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_collaborative_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: pid\n",
    "        self.pl_track_uri_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['track_uri_can'], \n",
    "                #     mask_token=None, \n",
    "                #     name=\"pl_track_uri_lookup\", \n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_uri_can\"])),\n",
    "\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can'])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_track_uri_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_track_uri_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: n_songs_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_songs_pl'], \n",
    "            vocab_dict['max_n_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_songs_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_songs_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: num_artists_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_artists_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_artists_pl'], \n",
    "            vocab_dict['max_n_artists_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_artists_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_artists_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_artists_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_artists_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                )\n",
    "            ], name=\"n_artists_pl_emb_model\"\n",
    "        )\n",
    "\n",
    "        # Feature: num_albums_pl\n",
    "        n_albums_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_albums_pl'], \n",
    "            vocab_dict['max_n_albums_pl'],\n",
    "            num=100\n",
    "        )\n",
    "        self.n_albums_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_albums_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_albums_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_albums_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_albums_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_pl\n",
    "        self.artist_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=tf.constant(vocab_dict['artist_name_can']), mask_token=None),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"artist_name_can\"]), mask_value=''),\n",
    "\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_name_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_pl_1d\"),\n",
    "            ], name=\"artist_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_pl\n",
    "        # 2.2M unique\n",
    "        self.track_uri_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['track_uri_can'], mask_token=''),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_uri_can\"]), mask_value=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_uri_1d\"),\n",
    "            ], name=\"track_uri_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_pl\n",
    "        self.track_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['track_name_can'], \n",
    "                #     name=\"track_name_pl_lookup\",\n",
    "                #     output_mode='int',\n",
    "                #     mask_token=''\n",
    "                # ),\n",
    "            tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_name_can\"]), mask_value=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_name_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_pl_1d\"),\n",
    "            ], name=\"track_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        Feature: duration_ms_songs_pl\n",
    "        duration_ms_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_duration_ms_songs_pl'], \n",
    "            vocab_dict['max_duration_ms_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.duration_ms_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(duration_ms_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(duration_ms_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"duration_ms_songs_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"duration_ms_songs_pl_emb_layer_pl_1d\"),\n",
    "            ], name=\"duration_ms_songs_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_pl\n",
    "        self.album_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['album_name_can'], \n",
    "                #     mask_token=None, \n",
    "                #     name=\"album_name_pl_lookup\"\n",
    "                # ),\n",
    "            tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"album_name_can\"]), mask_value=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['album_name_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_pl_emb_layer_1d\"),\n",
    "            ], name=\"album_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_pl\n",
    "        artist_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_pop'], \n",
    "            vocab_dict['max_artist_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artist_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artist_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artist_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_pop_1d\"),\n",
    "            ], name=\"artist_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artists_followers_pl\n",
    "        artists_followers_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_followers'], \n",
    "            vocab_dict['max_artist_followers'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artists_followers_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artists_followers_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artists_followers_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artists_followers_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artists_followers_pl_1d\"),\n",
    "            ], name=\"artists_followers_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_pl\n",
    "        track_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_track_pop'], \n",
    "            vocab_dict['max_track_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.track_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(dtype=tf.float32),\n",
    "                tf.keras.layers.Discretization(track_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(track_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_pop_pl_1d\"),\n",
    "            ], name=\"track_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_pl\n",
    "        self.artist_genres_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"album_uri_can\"]), mask_value=''),\n",
    "                # tf.keras.layers.StringLookup(\n",
    "                #     vocabulary=vocab_dict['artist_genres_can'], mask_token=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_genres_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_genres_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_pl_1d\"),\n",
    "            ], name=\"artist_genres_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # dense and cross layers\n",
    "        # ========================================\n",
    "\n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"pl_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "            \n",
    "        # Dense Layers\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"pl_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: tf.nn.l2_normalize(\n",
    "                    x, 1, epsilon=1e-12, name=\"normalize_dense\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # ========================================\n",
    "    # call\n",
    "    # ========================================\n",
    "    def call(self, data):\n",
    "        '''\n",
    "        The call method defines what happens when\n",
    "        the model is called\n",
    "        '''\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.pl_name_text_embedding(data['name']),\n",
    "                self.pl_collaborative_embedding(data['collaborative']),\n",
    "                self.pl_track_uri_embedding(data[\"track_uri_can\"]),\n",
    "                self.n_songs_pl_embedding(data[\"n_songs_pl\"]),\n",
    "                self.n_artists_pl_embedding(data['num_artists_pl']),\n",
    "                self.n_albums_pl_embedding(data[\"num_albums_pl\"]),\n",
    "                \n",
    "                # sequence features\n",
    "                self.artist_name_pl_embedding(tf.reshape(data[\"artist_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))), #reshape to get [BATCH, MAX_SEQ_LEN]\n",
    "                self.track_uri_pl_embedding(tf.reshape(data[\"track_uri_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.track_name_pl_embedding(tf.reshape(data[\"track_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.duration_ms_songs_pl_embedding(tf.reshape(data[\"duration_ms_songs_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.album_name_pl_embedding(tf.reshape(data[\"album_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artist_pop_pl_embedding(tf.reshape(data[\"artist_pop_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artists_followers_pl_embedding(tf.reshape(data[\"artists_followers_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.track_pop_pl_embedding(tf.reshape(data[\"track_pop_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artist_genres_pl_embedding(tf.reshape(data[\"artist_genres_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "            ], axis=1)\n",
    "        \n",
    "        # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)\n",
    "\n",
    "# ====================================================\n",
    "# Track (candidate) Tower\n",
    "# ====================================================\n",
    "class Candidate_Track_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # Candidate features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_can\n",
    "        self.artist_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "            #     tf.keras.layers.TextVectorization(\n",
    "            #         # max_tokens=MAX_TOKENS,\n",
    "            #         vocabulary=vocab_dict[TOKEN_DICT][\"artist_name_can\"],\n",
    "            #         name=\"artist_name_can_txt_vectorizer\",\n",
    "            #         ngrams=2,\n",
    "            #     ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_name_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_can_pooling\"),\n",
    "            ], name=\"artist_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_can\n",
    "        self.track_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS,\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT][\"track_name_can\"],\n",
    "                #     name=\"track_name_can_txt_vectorizer\",\n",
    "                #     ngrams=2,\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"track_name_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_can_pooling\"),\n",
    "            ], name=\"track_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_can\n",
    "        self.album_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS,\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT][\"album_name_can\"],\n",
    "                #     name=\"album_name_can_txt_vectorizer\",\n",
    "                #     ngrams=2,\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"album_name_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_can_pooling\"),\n",
    "            ], name=\"album_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_uri_can\n",
    "        self.artist_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"artist_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_can\n",
    "        self.track_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"track_uri_can\"])),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"track_uri_can\"])+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"track_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_uri_can\n",
    "        self.album_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=len(vocab_dict[\"album_uri_can\"])),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"album_uri_can\"])+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"album_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: duration_ms_can\n",
    "        self.duration_ms_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_duration_ms_songs_pl'],\n",
    "            variance=vocab_dict['var_duration_ms_songs_pl'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_can\n",
    "        self.track_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_track_pop'],\n",
    "            variance=vocab_dict['var_track_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_can\n",
    "        self.artist_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_pop'],\n",
    "            variance=vocab_dict['var_artist_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_followers_can\n",
    "        self.artist_followers_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_followers'],\n",
    "            variance=vocab_dict['var_artist_followers'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_can\n",
    "        self.artist_genres_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.TextVectorization(\n",
    "                #     # max_tokens=MAX_TOKENS,\n",
    "                #     vocabulary=vocab_dict[TOKEN_DICT][\"artist_genres_can\"],\n",
    "                #     name=\"artist_genres_can_txt_vectorizer\",\n",
    "                #     ngrams=2,\n",
    "                # ),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_genres_can_emb_layer\",\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_can_pooling\"),\n",
    "            ], name=\"artist_genres_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # Dense & Cross Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"can_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "        \n",
    "        # Dense Layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"candidate_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    # ========================================\n",
    "    # Call Function\n",
    "    # ========================================\n",
    "            \n",
    "    def call(self, data):\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.artist_name_can_text_embedding(data['artist_name_can']),  \n",
    "                self.track_name_can_text_embedding(data['track_name_can']),  \n",
    "                self.album_name_can_text_embedding(data['album_name_can']),  \n",
    "                self.artist_uri_can_embedding(data['artist_uri_can']),  \n",
    "                self.track_uri_can_embedding(data['track_uri_can']),  \n",
    "                self.album_uri_can_embedding(data['album_uri_can']),  \n",
    "                tf.reshape(self.duration_ms_can_normalized(data[\"duration_ms_can\"]), (-1, 1)), \n",
    "                tf.reshape(self.track_pop_can_normalized(data[\"track_pop_can\"]), (-1, 1)),  \n",
    "                tf.reshape(self.artist_pop_can_normalized(data[\"artist_pop_can\"]), (-1, 1)),  \n",
    "                tf.reshape(self.artist_followers_can_normalized(data[\"artist_followers_can\"]), (-1, 1)),  \n",
    "                self.artist_genres_can_text_embedding(data['album_uri_can']),  \n",
    "            ], axis=1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # return self.dense_layers(all_embs)\n",
    "                # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)\n",
    "\n",
    "# ====================================================\n",
    "# Combined 2Tower\n",
    "# ====================================================\n",
    "class TheTwoTowers(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes, vocab_dict_load, parsed_candidate_dataset): # , max_padding_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_tower = Playlist_Model(layer_sizes, vocab_dict_load) #, max_padding_len)\n",
    "        \n",
    "        self.candidate_tower = Candidate_Track_Model(layer_sizes, vocab_dict_load)\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=parsed_candidate_dataset.batch(128).map(self.candidate_tower)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_tower(data)\n",
    "        candidate_embeddings = self.candidate_tower(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings, \n",
    "            candidate_embeddings, \n",
    "            compute_metrics=not training\n",
    "        ) # turn off metrics to save time on training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/task.py\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras import mixed_precision\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle as pkl\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "\n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "# ====================================================\n",
    "# Main\n",
    "# ====================================================\n",
    "import data_src as trainer_data\n",
    "import model_src as trainer_model\n",
    "import train_config as cfg\n",
    "import time\n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    # tf.debugging.set_log_device_placement(True) # logs all tf ops and their device placement;\n",
    "    TF_GPU_THREAD_MODE='gpu_private'\n",
    "    \n",
    "    logging.info(\"Starting training...\")\n",
    "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    \n",
    "    storage_client = storage.Client(\n",
    "        project=args.project\n",
    "    )\n",
    "    \n",
    "    WORKING_DIR = f'gs://{args.train_output_gcs_bucket}'             # replaced f'gs://{args.model_dir}/{args.version}'\n",
    "    logging.info(f'Train job output directory: {WORKING_DIR}')\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    \n",
    "    # AIP_TB_LOGS = args.aip_tb_logs # os.environ.get('AIP_TENSORBOARD_LOG_DIR', 'NA')\n",
    "    # logging.info(f'AIP TENSORBOARD LOG DIR: {AIP_TB_LOGS}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU/TPU Strategy\n",
    "    # ====================================================\n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.config.list_physical_devices('GPU'): # TODO: replace with - tf.config.list_physical_devices('GPU') | tf.test.is_gpu_available()\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    \n",
    "    # Single Machine, multiple TPU devices\n",
    "    elif args.distribute == 'tpu':\n",
    "        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "        logging.info(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "    \n",
    "    logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    \n",
    "    # ====================================================\n",
    "    # Vocab Files\n",
    "    # ====================================================\n",
    "\n",
    "    # TODO: parameterize & configure for adapts vs vocab files\n",
    "\n",
    "    BUCKET_NAME = 'spotify-v1'                           # args.vocab_gcs_bucket\n",
    "    FILE_PATH = 'vocabs/v2_string_vocabs'                # args.vocab_gcs_file_path\n",
    "    FILE_NAME = 'string_vocabs_v1_20220924-tokens22.pkl'   # args.vocab_filename\n",
    "    DESTINATION_FILE = 'downloaded_vocabs.txt'     \n",
    "\n",
    "    with open(f'{DESTINATION_FILE}', 'wb') as file_obj:\n",
    "        storage_client.download_blob_to_file(\n",
    "            f'gs://{BUCKET_NAME}/{FILE_PATH}/{FILE_NAME}', file_obj)\n",
    "\n",
    "\n",
    "    with open(f'{DESTINATION_FILE}', 'rb') as pickle_file:\n",
    "        vocab_dict_load = pkl.load(pickle_file)\n",
    "\n",
    "    # ====================================================\n",
    "    # TRAIN dataset - Parse & Pad\n",
    "    # ====================================================\n",
    "    \n",
    "    logging.info(f'Path to TRAIN files: gs://{args.train_dir}/{args.train_dir_prefix}')\n",
    "    \n",
    "    train_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.train_dir}', prefix=f'{args.train_dir_prefix}', delimiter=\"/\"):\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    # Parse train dataset\n",
    "    # raw_train_ds = tf.data.TFRecordDataset(train_files)\n",
    "    # parsed_train_ds = raw_train_ds.map(trainer_data.parse_tfrecord) # _data\n",
    "    # parsed_padded_train_ds = parsed_train_ds.map(trainer_data.return_padded_tensors) # _data\n",
    "    \n",
    "    # OPTIMIZE DATA INPUT PIPELINE\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "    train_dataset = train_dataset.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    ).map(\n",
    "        trainer_data.parse_tfrecord,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).map(\n",
    "        trainer_data.return_padded_tensors, #(max_playlist_len=args.max_padding),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).batch(\n",
    "        args.batch_size * strategy.num_replicas_in_sync\n",
    "    ).prefetch(\n",
    "        tf.data.AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # VALID dataset - Parse & Pad \n",
    "    # ====================================================\n",
    "    \n",
    "    logging.info(f'Path to VALID files: gs://{args.valid_dir}/{args.valid_dir_prefix}')\n",
    "    \n",
    "    valid_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.valid_dir}', prefix=f'{args.valid_dir_prefix}', delimiter=\"/\"):\n",
    "        valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    # Parse train dataset\n",
    "    # raw_valid_ds = tf.data.TFRecordDataset(valid_files)\n",
    "    # parsed_valid_ds = raw_valid_ds.map(trainer_data.parse_tfrecord) # _data\n",
    "    # parsed_padded_valid_ds = parsed_valid_ds.map(trainer_data.return_padded_tensors) # _data\n",
    "    \n",
    "    # OPTIMIZE DATA INPUT PIPELINE\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices(valid_files)\n",
    "    valid_dataset = valid_dataset.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    ).map(\n",
    "        trainer_data.parse_tfrecord,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).map(\n",
    "        trainer_data.return_padded_tensors, #(max_playlist_len=args.max_padding),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          ).batch(\n",
    "        args.batch_size * strategy.num_replicas_in_sync\n",
    "    ).prefetch(\n",
    "        tf.data.AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Parse candidates dataset\n",
    "    # ====================================================\n",
    "    \n",
    "    logging.info(f'Path to CANDIDATE files: gs://{args.candidate_file_dir}/{args.candidate_files_prefix}')\n",
    "\n",
    "    candidate_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.candidate_file_dir}', prefix=f'{args.candidate_files_prefix}', delimiter=\"/\"):\n",
    "        candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    # raw_candidate_dataset = tf.data.TFRecordDataset(candidate_files)\n",
    "    # parsed_candidate_dataset = raw_candidate_dataset.map(trainer_data.parse_candidate_tfrecord_fn) # _data\n",
    "    \n",
    "    #generate the candidate dataset\n",
    "    candidate_dataset = tf.data.Dataset.from_tensor_slices(candidate_files)\n",
    "    parsed_candidate_dataset = candidate_dataset.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    ).map(\n",
    "        trainer_data.parse_candidate_tfrecord_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    ).prefetch(\n",
    "        tf.data.AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging metaparams & hyperparams for Vertex Experiments')\n",
    "    \n",
    "    EXPERIMENT_NAME = f\"{args.experiment_name}\"\n",
    "    RUN_NAME = f\"{args.experiment_run}\"\n",
    "    logging.info(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\\n RUN_NAME: {RUN_NAME}\")\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{EXPERIMENT_NAME}'\n",
    "    metaparams[\"experiment_run\"] = f\"{RUN_NAME}\"\n",
    "    metaparams[\"model_version\"] = f\"{args.model_version}\"\n",
    "    metaparams[\"pipe_version\"] = f\"{args.pipeline_version}\"\n",
    "    metaparams[\"data_regime\"] = f\"{args.data_regime}\"\n",
    "    metaparams[\"distribute\"] = f'{args.distribute}'\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"batch_size\"] = int(args.batch_size)\n",
    "    hyperparams[\"embedding_dim\"] = args.embedding_dim\n",
    "    hyperparams[\"projection_dim\"] = args.projection_dim\n",
    "    hyperparams[\"use_cross_layer\"] = cfg.USE_CROSS_LAYER # args.use_cross_layer\n",
    "    hyperparams[\"use_dropout\"] = cfg.USE_DROPOUT # args.use_dropout\n",
    "    hyperparams[\"dropout_rate\"] = args.dropout_rate\n",
    "    hyperparams['layer_sizes'] = args.layer_sizes\n",
    "    hyperparams['max_padding'] = args.max_padding\n",
    "    \n",
    "    logging.info(f\"Creating run: {RUN_NAME}; for experiment: {EXPERIMENT_NAME}\")\n",
    "    \n",
    "    # Create experiment\n",
    "    vertex_ai.init(experiment=EXPERIMENT_NAME)\n",
    "    # vertex_ai.start_run(RUN_NAME,resume=True) # RUN_NAME\n",
    "    \n",
    "    with vertex_ai.start_run(RUN_NAME) as my_run:\n",
    "        logging.info(f\"logging metaparams\")\n",
    "        my_run.log_params(metaparams)\n",
    "        \n",
    "        logging.info(f\"logging hyperparams\")\n",
    "        my_run.log_params(hyperparams)\n",
    "        \n",
    "    # ====================================================\n",
    "    # Compile, Adapt, and Train model\n",
    "    # ====================================================\n",
    "    logging.info('Setting model adapts and compiling the model')\n",
    "    \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "    \n",
    "    logging.info(f'adapting layers: {cfg.NEW_ADAPTS}') # args.new_adapts | cfg.NEW_ADAPTS\n",
    "    \n",
    "    # Wrap variable creation within strategy scope\n",
    "    with strategy.scope():\n",
    "\n",
    "        model = trainer_model.TheTwoTowers(LAYER_SIZES, vocab_dict_load, parsed_candidate_dataset) #, max_padding_len=args.max_padding)\n",
    "        \n",
    "        # model.query_tower.pl_name_text_embedding.layers[0].adapt(shuffled_parsed_train_ds.map(lambda x: x['name']).batch(args.batch_size)) # TODO: use cached_train or shuffled_parsed_train_ds ?\n",
    "        # artist_name_can\n",
    "        # track_name_can\n",
    "        # album_name_can\n",
    "        # artist_genres_can\n",
    "        \n",
    "        # if cfg.NEW_ADAPTS:\n",
    "            # model.query_tower.pl_name_text_embedding.layers[0].adapt(shuffled_parsed_ds.map(lambda x: x['name']).batch(args.batch_size)) # TODO: adapts on full dataset or train onl\n",
    "            \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adagrad(args.learning_rate))\n",
    "        \n",
    "    # if cfg.NEW_ADAPTS:\n",
    "        # vocab_dict_load['name'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "        # bucket = storage_client.bucket(args.train_output_gcs_bucket)                               # TODO: args.train_output_gcs_bucket # replaced args.model_dir\n",
    "        # blob = bucket.blob(f'{EXPERIMENT_NAME}/{RUN_NAME}/vocabs_stats/vocab_dict_{RUN_NAME}.txt') # replaced f'{args.version}/vocabs_stats/vocab_dict_{RUN_NAME}.txt'\n",
    "        # pickle_out = pkl.dumps(vocab_dict_load)\n",
    "        # blob.upload_from_string(pickle_out)\n",
    "    \n",
    "    logging.info('Adapts finish - training next')\n",
    "        \n",
    "    tf.random.set_seed(args.seed)\n",
    "    \n",
    "    logs_dir = f'gs://{args.train_output_gcs_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs'         # replaced f\"{WORKING_DIR}/tb-logs-{RUN_NAME}\" \n",
    "    AIP_LOGS = os.environ.get('AIP_TENSORBOARD_LOG_DIR', f'{logs_dir}')\n",
    "    logging.info(f'TensorBoard logdir: {AIP_LOGS}')\n",
    "    \n",
    "    cloud_profiler.init()\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=AIP_LOGS,\n",
    "        histogram_freq=0, \n",
    "        write_graph=True, \n",
    "        # profile_batch = '500,520'\n",
    "    )\n",
    "    \n",
    "    # if os.environ.get('AIP_TENSORBOARD_LOG_DIR', 'NA') is not 'NA':\n",
    "    #     tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    #         log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'],\n",
    "    #         histogram_freq=0, write_graph=True, profile_batch = '500,520')\n",
    "    # else:\n",
    "    #     os.mkdir('/tb_logs')\n",
    "    #     tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    #         log_dir='/tb_logs',\n",
    "    #         histogram_freq=0)\n",
    "    \n",
    "    logging.info('Training starting')\n",
    "    start_model_fit = time.time()\n",
    "    \n",
    "    layer_history = model.fit(\n",
    "        train_dataset,\n",
    "        # validation_data=valid_dataset,\n",
    "        # validation_freq=args.valid_frequency, # no longer used due to long-running brute force see scann validation belo\n",
    "        callbacks=tensorboard_callback,\n",
    "        # steps_per_epoch=10, #for debugging purposes\n",
    "        epochs=args.num_epochs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_model_fit = time.time()\n",
    "    elapsed_model_fit = end_model_fit - start_model_fit\n",
    "    elapsed_model_fit = round(elapsed_model_fit, 2)\n",
    "    logging.info(f'Elapsed model_fit: {elapsed_model_fit} seconds')\n",
    "\n",
    "    # Determine type and task of the machine from the strategy cluster resolver\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id = None, None\n",
    "        \n",
    "    # ====================================================\n",
    "    # Aprroximate Validation with ScaNN\n",
    "    # ====================================================\n",
    "    # Get candidate item (songs/tracks) embeddings\n",
    "    song_embeddings = parsed_candidate_dataset.batch(2048).map(\n",
    "        model.candidate_tower, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(\n",
    "        tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Creating ScaNN layer for approximate validation metrics\")\n",
    "    start_scann_layer = time.time()\n",
    "    \n",
    "    # Compute predictions\n",
    "    scann = tfrs.layers.factorized_top_k.ScaNN(\n",
    "        num_reordering_candidates=500,         # TODO: parameterize\n",
    "        num_leaves_to_search=30                # TODO: parameterize\n",
    "    )\n",
    "    scann.index_from_dataset(song_embeddings)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "            candidates=scann\n",
    "        )\n",
    "        model.compile()\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_scann_layer = time.time()\n",
    "    elapsed_scann_layer = end_scann_layer - start_scann_layer\n",
    "    elapsed_scann_layer = round(elapsed_scann_layer, 2)\n",
    "    logging.info(f'Elapsed ScaNN Layer: {elapsed_scann_layer} seconds')\n",
    "    \n",
    "    logging.info(\"custom scann layer generation for validation complete\")\n",
    "    #TODO - perhaps output the scann layer for indexing use if needed\n",
    "\n",
    "    # ====================================================\n",
    "    # Eval Metrics\n",
    "    # ====================================================\n",
    "    logging.info('Getting evaluation metrics')\n",
    "    start_evaluation = time.time()\n",
    "    \n",
    "    val_metrics = model.evaluate(\n",
    "        valid_dataset,\n",
    "        verbose=\"auto\",\n",
    "        return_dict=True,\n",
    "        callbacks=tensorboard_callback,\n",
    "    ) #check performance\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_evaluation = time.time()\n",
    "    elapsed_evaluation = end_evaluation - start_evaluation\n",
    "    elapsed_evaluation = round(elapsed_evaluation, 2)\n",
    "    logging.info(f'Elapsed model Evaluation: {elapsed_evaluation} seconds')\n",
    "\n",
    "    logging.info('Validation metrics below:')\n",
    "    logging.info(val_metrics)\n",
    "    \n",
    "    time_metrics = {}\n",
    "    time_metrics[\"elapsed_model_fit\"] = elapsed_model_fit\n",
    "    time_metrics[\"elapsed_scann_layer\"] = elapsed_scann_layer\n",
    "    time_metrics[\"elapsed_evaluation\"] = elapsed_evaluation\n",
    "    \n",
    "    with vertex_ai.start_run(RUN_NAME,resume=True) as my_run:\n",
    "        logging.info(f\"logging metrics to experiment run {RUN_NAME}\")\n",
    "        my_run.log_metrics(val_metrics)\n",
    "        my_run.log_metrics(time_metrics)\n",
    "    \n",
    "    # logging.info(f\"Ending experiment run: {RUN_NAME}\")\n",
    "    # vertex_ai.end_run()\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save Towers\n",
    "    # ====================================================\n",
    "    \n",
    "    MODEL_DIR_GCS_URI = f'gs://{args.train_output_gcs_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}/model-dir'\n",
    "    logging.info(f'Saving models to {MODEL_DIR_GCS_URI}')\n",
    "\n",
    "    query_dir_save = f\"{MODEL_DIR_GCS_URI}/query_tower/\"                                      # replaced: f\"gs://{args.model_dir}/{args.version}/{RUN_NAME}/query_tower/\" \n",
    "    candidate_dir_save = f\"{MODEL_DIR_GCS_URI}/candidate_tower/\"                              # replaced: f\"gs://{args.model_dir}/{args.version}/{RUN_NAME}/candidate_tower/\"\n",
    "    logging.info(f'Saving chief query model to {query_dir_save}')\n",
    "    \n",
    "    # save model from primary node in multiworker\n",
    "    if _is_chief(task_type, task_id):\n",
    "        tf.saved_model.save(model.query_tower, query_dir_save)\n",
    "        logging.info(f'Saved chief query model to {query_dir_save}')\n",
    "        tf.saved_model.save(model.candidate_tower, candidate_dir_save)\n",
    "        logging.info(f'Saved chief candidate model to {candidate_dir_save}')\n",
    "    else:\n",
    "        worker_dir_query = query_dir_save + '/workertemp_query_/' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir_query)\n",
    "        tf.saved_model.save(model.query_tower, worker_dir_query)\n",
    "        logging.info(f'Saved worker: {task_id} query model to {worker_dir_query}')\n",
    "\n",
    "        worker_dir_can = candidate_dir_save + '/workertemp_can_/' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir_can)\n",
    "        tf.saved_model.save(model.candidate_tower, worker_dir_can)\n",
    "        logging.info(f'Saved worker: {task_id} candidate model to {worker_dir_can}')\n",
    "\n",
    "    if not _is_chief(task_type, task_id):\n",
    "        tf.io.gfile.rmtree(worker_dir_can)\n",
    "        tf.io.gfile.rmtree(worker_dir_query)\n",
    "\n",
    "    logging.info('All done - model saved') #all done\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command line arguments\n",
    "    \n",
    "    type: int, float, str\n",
    "          bool() converts empty strings to `False` and non-empty strings to `True`\n",
    "          see more details here: https://docs.python.org/3/library/argparse.html#type\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir',\n",
    "                        default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir', required=False) # TODO: sunset this arg\n",
    "    \n",
    "    parser.add_argument('--train_output_gcs_bucket',\n",
    "                        default=os.getenv('AIP_MODEL_DIR'), type=str, help='bucket for train job output', required=False) # TODO: use this\n",
    "    \n",
    "    parser.add_argument('--train_dir', \n",
    "                        type=str, help='bucket holding training files', required=True)\n",
    "    \n",
    "    parser.add_argument('--train_dir_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "    \n",
    "    parser.add_argument('--valid_dir', \n",
    "                        type=str, help='bucket holding valid files', required=True)\n",
    "    \n",
    "    parser.add_argument('--valid_dir_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "    \n",
    "    parser.add_argument('--candidate_file_dir', \n",
    "                        type=str, help='bucket holding candidate files', required=True)\n",
    "\n",
    "    parser.add_argument('--candidate_files_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "\n",
    "    parser.add_argument('--project', \n",
    "                        type=str, help='project', required=True)\n",
    "\n",
    "    parser.add_argument('--max_padding', \n",
    "                        default=375, type=int, help='max_padding', required=False)\n",
    "\n",
    "    parser.add_argument('--experiment_name', \n",
    "                        type=str, help='#TODO', required=True)\n",
    "\n",
    "    parser.add_argument('--experiment_run', \n",
    "                        type=str, help='#TODO', required=True)\n",
    "\n",
    "    parser.add_argument('--num_epochs', \n",
    "                        default=1, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--batch_size', \n",
    "                        default=128, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--embedding_dim', \n",
    "                        default=32, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--projection_dim', \n",
    "                        default=5, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--seed', \n",
    "                        default=1234, type=str, help='#TODO', required=False)\n",
    "\n",
    "#     parser.add_argument('--use_cross_layer', \n",
    "#                         default=True, type=bool, help='#TODO', required=False)\n",
    "\n",
    "#     parser.add_argument('--use_dropout', \n",
    "#                         default=False, type=bool, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--dropout_rate', \n",
    "                        default=0.4, type=float, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--layer_sizes', \n",
    "                        default='[64,32]', type=str, help='#TODO', required=False)\n",
    "\n",
    "    # parser.add_argument('--aip_tb_logs', \n",
    "    #                     default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str, help='#TODO', required=False)\n",
    "\n",
    "    # parser.add_argument('--new_adapts', \n",
    "    #                     default=False, type=bool, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--learning_rate', \n",
    "                        default=0.01, type=float, help='learning rate', required=False)\n",
    "\n",
    "    # parser.add_argument('--valid_size', \n",
    "    #                     default='#TODO', type=str, help='number of records in valid split', required=False)\n",
    "\n",
    "    parser.add_argument('--valid_frequency', \n",
    "                        default=10, type=int, help='number of epochs per metrics val calculation', required=False)\n",
    "\n",
    "    parser.add_argument('--distribute', \n",
    "                        default='single', type=str, help='TF strategy: single, mirrored, multiworker, tpu', required=False)\n",
    "\n",
    "    # parser.add_argument('--version', \n",
    "    #                     type=str, help='version of train code; for tracking', required=True)\n",
    "    \n",
    "    parser.add_argument('--model_version', \n",
    "                        type=str, help='version of model train code', required=True)\n",
    "    \n",
    "    parser.add_argument('--pipeline_version', \n",
    "                        type=str, help='version of pipeline code; v0 for non-pipeline execution', required=True)\n",
    "    \n",
    "    parser.add_argument('--data_regime', \n",
    "                        type=str, help='id for tracking different datasets', required=True)\n",
    "\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    return parser.parse_args()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting jobs main() script')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree /home/jupyter/spotify-tfrs/src\n",
    "#/vertex_train/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Worker Pool Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=b\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# # Single machine, single GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# # # Single Machine; multiple GPU\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-2g' # a2-ultragpu-4g\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 2\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# # # Multiple Machines, 1 GPU per Machine\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 9\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write `train_config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/train_config.py\n",
    "\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "\n",
    "NEW_ADAPTS = 'True'\n",
    "USE_CROSS_LAYER = 'True'\n",
    "USE_DROPOUT = 'True'\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "MAX_PADDING = 5 # this should improve performance vs 375\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "SEED = 1234\n",
    "DROPOUT_RATE = 0.4\n",
    "TOKEN_DICT = '20000_tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previously defined VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PROJECT: {PROJECT}\")\n",
    "\n",
    "print(f\"APP: {APP}\")\n",
    "print(f\"MODEL_TYPE: {MODEL_TYPE}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_VERSION: {MODEL_VERSION}\")\n",
    "print(f\"PIPELINE_VERSION: {PIPELINE_VERSION}\\n\")\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")\n",
    "print(f\"OUTPUT_BUCKET: {OUTPUT_BUCKET}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "# from trainer import train_config as config\n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# # \"gs://spotify-tfrecords-blog/tfrecords_v1/train/output-00000-of-00796.tfrecord\"\n",
    "# # gs://spotify-tfrs-dir/small-dataset/output-00000-of-00796.tfrecord\n",
    "\n",
    "# GCS buckets & paths to source data\n",
    "CANDIDATE_FILE_DIR = 'spotify-beam-v3'      #'spotify-tfrecords-blog'\n",
    "CANDIDATE_PREFIX = 'v3/candidates/'         # 'tfrecords_v1/train/'\n",
    "\n",
    "TRAIN_DIR = 'spotify-beam-v3'               #'spotify-tfrecords-blog'\n",
    "TRAIN_DIR_PREFIX = 'v3/dif_artist/'         # 'tfrecords_v1/train/' 'v3/dif_artist/' v3/dif-artist-tmp-jt\n",
    "\n",
    "VALID_DIR = 'spotify-beam-v3'               #'spotify-tfrecords-blog'\n",
    "VALID_DIR_PREFIX = 'v3/dif_artist_valid/'   # 'tfrecords_v1/train/' 'v3/dif_artist_valid/' 'v3/dif-artist-tmp-jt/'\n",
    "\n",
    "# MODEL_DIR='spotify-tfrs-dir'  \n",
    "# OUTPUT_BUCKET = 'jt-tfrs-test' # replaced MODEL_DIR='spotify-tfrs-dir' \n",
    "\n",
    "EXPERIMENT_PREFIX = 'dev'                                   # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}'\n",
    "RUN_NAME=f'run-{TIMESTAMP}'\n",
    "DATA_REGIME = 'dif-artist-beam-tfrecord' # 'full-beam-tfrecord'\n",
    "\n",
    "VALID_FREQUENCY = 1 #change to 10\n",
    "# VALID_SIZE = 20_000\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 2048 # 2048, 4096\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "MAX_PADDING = 5\n",
    "EMBEDDING_DIM = 64\n",
    "PROJECTION_DIM = 5\n",
    "\n",
    "DROPOUT_RATE = 0.4\n",
    "LAYER_SIZES = '[64,32]' # [64,32] [128,64]\n",
    "\n",
    "WORKER_CMD = [\"python\", \"trainer/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT}',\n",
    "    f'--train_output_gcs_bucket={OUTPUT_BUCKET}',\n",
    "    f'--train_dir={TRAIN_DIR}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={VALID_DIR}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    # f'--model_dir={MODEL_DIR}',\n",
    "    f'--candidate_file_dir={CANDIDATE_FILE_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={MODEL_VERSION}',\n",
    "    f'--pipeline_version={PIPELINE_VERSION}',\n",
    "    f'--data_regime={DATA_REGIME}',\n",
    "]\n",
    "\n",
    "# deprecated model args\n",
    "    # f'--valid_size={VALID_SIZE}',\n",
    "    # f'--new_adapts={new_adapts}',\n",
    "    # f'--use_cross_layer={use_cross_layer}',\n",
    "    # f'--use_dropout={use_dropout}',\n",
    "\n",
    "    \n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Locally\n",
    "\n",
    "* TODO: local test handle module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !tree /home/jupyter/spotify-tfrs/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupyter/spotify-tfrs')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "_DISTRIBUTE_STRATEGY='single'\n",
    "_EXPERIMENT_NAME=f'local-testing-{MODEL_VERSION}'\n",
    "_RUN_NAME=f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cd src/trainer; python3 task.py \\\n",
    "#     --project={PROJECT} --train_output_gcs_bucket={OUTPUT_BUCKET} --train_dir={TRAIN_DIR} --train_dir_prefix={TRAIN_DIR_PREFIX} \\\n",
    "#     --valid_dir={VALID_DIR} --valid_dir_prefix={VALID_DIR_PREFIX} \\\n",
    "#     --candidate_file_dir={CANDIDATE_FILE_DIR} --candidate_files_prefix={CANDIDATE_PREFIX} \\\n",
    "#     --experiment_name={_EXPERIMENT_NAME} --experiment_run={_RUN_NAME} \\\n",
    "#     --max_padding={MAX_PADDING} \\\n",
    "#     --num_epochs={NUM_EPOCHS} --batch_size={BATCH_SIZE} --embedding_dim={EMBEDDING_DIM} --projection_dim={PROJECTION_DIM} \\\n",
    "#     --dropout_rate={DROPOUT_RATE} --layer_sizes={LAYER_SIZES} --learning_rate={LEARNING_RATE} \\\n",
    "#     --valid_frequency={VALID_FREQUENCY} --distribute={_DISTRIBUTE_STRATEGY} \\\n",
    "#     --model_version={MODEL_VERSION} --pipeline_version={PIPELINE_VERSION} \\\n",
    "#     --data_regime={DATA_REGIME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Custom Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupyter/spotify-tfrs')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Cloud Build\n",
    "\n",
    "This will build the training container used in Vertex Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit train job to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = f'train-{MODEL_ROOT_NAME}-2-a100-layers' #-{TIMESTAMP}'\n",
    "\n",
    "# e.g., MODEL_DIR_GCS_URI = f'gs://{args.train_output_gcs_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}/model-dir'\n",
    "# BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{MODEL_ROOT_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "\n",
    "print(f'JOB_NAME:{JOB_NAME}')\n",
    "print(f'BASE_OUTPUT_DIR:{BASE_OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=BASE_OUTPUT_DIR,\n",
    "    # labels={'gpu':f'{ACCELERATOR_TYPE}'}\n",
    ")\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SavedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "query_tower_uri = 'gs://jt-tfrs-test/dev-2tower-tfrs-jwv4-approx-valid-one-epoch/run-20220930-202044/model-dir/query_tower'\n",
    "candidate_tower_uri = 'gs://jt-tfrs-test/dev-2tower-tfrs-jwv4-approx-valid-one-epoch/run-20220930-202044/model-dir/candidate_tower'\n",
    "loaded_query_model = tf.saved_model.load(query_tower_uri)\n",
    "loaded_candidate_model = tf.saved_model.load(candidate_tower_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(loaded_candidate_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = loaded_candidate_model.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict2 = loaded_candidate_model.signatures['serving_default']\n",
    "predict2.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_candidate_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_iter = parsed_dataset_candidates.batch(1).map(lambda data: predict2(\n",
    "                artist_name = data[\"artist_name_can\"],\n",
    "                track_name = data['track_name_can'],\n",
    "                album_name = data['album_name_can'],\n",
    "                track_uri = data['track_uri_can'],\n",
    "                artist_uri = data['artist_uri_can'],\n",
    "                album_uri = data['album_uri_can'],\n",
    "                duration_ms = data['duration_ms_can'],\n",
    "                track_pop = data['track_pop_can'],\n",
    "                artist_pop = data['artist_pop_can'],\n",
    "                artist_followers = data['artist_followers_can'],\n",
    "                artist_genres = data['artist_genres_can']))\n",
    "\n",
    "    \n",
    "candidate_features = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "> adapts vs vocab_dict\n",
    "\n",
    "```\n",
    "test_playlist_model = Playlist_Model(layer_sizes, vocab_dict_load)\n",
    "test_playlist_model.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(1000))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
