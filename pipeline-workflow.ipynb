{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38858d8a-935c-4e80-aa32-28bf4f665938",
   "metadata": {},
   "source": [
    "## Orchestrate RecSys workflow with Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accec8f9-be16-4e39-b903-d42ac2f9b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639915f-9dc1-47fe-b896-e14848dcbc59",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec70b5-0bd6-4a87-a498-74db04ee7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform==1.17.0 --upgrade\n",
    "# !pip install google-cloud-pipeline-components==1.0.19 --upgrade\n",
    "# !pip install kfp==1.8.13 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feca7cb-541a-41ee-b1f5-e15867c41fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.13\n",
      "google_cloud_pipeline_components version: 1.0.19\n",
      "aiplatform SDK version: 1.17.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ac58ad-b893-4e25-bbf3-7dcaa9cb34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "import kfp\n",
    "# from kfp.v2 import dsl\n",
    "# import kfp.v2.dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "# GCP\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "# from google.cloud import bigquery\n",
    "# from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8746ea8-a856-47a1-ab58-1007428771c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'pipev1'\n",
    "\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "\n",
    "PIPE_USER = 'jtott' \n",
    "BUCKET = 'jt-tfrs-test'\n",
    "BUCKET_URI = f'gs://{BUCKET}'\n",
    "\n",
    "DOCKERNAME_TRAIN = 'Dockerfile.tfrs'\n",
    "\n",
    "PIPELINE_ROOT = f'gs://{BUCKET}/pipeline_root/{PIPE_USER}'\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location='us-central1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913503a0-2de1-4e76-ac31-de8bef3511dd",
   "metadata": {},
   "source": [
    "## Create Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c77416-6928-45f3-affa-1fc532bfd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/pipelines\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f66a90-d8fd-4c7c-a0f6-b4cff1e00398",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TODO: Build custom train image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699aedd1-a27f-4a82-ae2f-ec6a093e41c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/spotify-tfrs\n",
      "Dockerfile.tfrs\n",
      "gs://jt-tfrs-test\n",
      "pipev1\n"
     ]
    }
   ],
   "source": [
    "!export PWD=pwd\n",
    "!export DOCKERNAME_TRAIN=DOCKERNAME_TRAIN\n",
    "!export BUCKET_URI=BUCKET_URI\n",
    "!export VERSION=VERSION\n",
    "! echo $PWD\n",
    "! echo $DOCKERNAME_TRAIN\n",
    "! echo $BUCKET_URI\n",
    "! echo $VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bcb6d-9276-40b6-a057-d3bb0ec1d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\n"
     ]
    }
   ],
   "source": [
    "! echo $REPO_DOCKER_PATH_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed6f84fe-6d03-4fb4-93b3-1fe7b46d73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # copy training Dockerfile\n",
    "\n",
    "# !gsutil cp $PWD/src/Dockerfile.tfrs $BUCKET_URI/$VERSION/src/\n",
    "\n",
    "# # # copy training application code\n",
    "\n",
    "# !gsutil cp -r $PWD/src/trainer/* $BUCKET_URI/$VERSION/src/trainer/\n",
    "\n",
    "# # # list copied files from GCS location\n",
    "# !gsutil ls -Rl $BUCKET_URI/$VERSION/trainer/\n",
    "\n",
    "# print(f\"Copied training application code and Dockerfile to {BUCKET_URI}/{VERSION}/trainer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5458a79-92c8-4003-b67e-6d171c784a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/build_custom_train_image.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/build_custom_train_image.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    output_component_file=\"./pipelines/build_custom_train_image.yaml\",\n",
    ")\n",
    "def build_custom_train_image(\n",
    "    project: str, \n",
    "    gcs_train_script_path: str,                                                 # TRAIN_APP_CODE_PATH = f\"{BUCKET_URI}/{APP_NAME}/{VERSION}/vertex_train/\"\n",
    "    training_image_uri: str,                                                   # TRAIN_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/multiworker:2tower-pipe-{VERSION}\" \n",
    ") -> NamedTuple(\"Outputs\", [(\"training_image_uri\", str)]):\n",
    "\n",
    "    # TODO: make output Artifact for image_uri\n",
    "    \"\"\"\n",
    "    custom pipeline component to build custom training image using\n",
    "    Cloud Build and the training application code and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "\n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    gs_dockerfile_path = os.path.join(gcs_train_script_path, \"Dockerfile\")   # two-tower-pipes/2tower-recsys/vertex_train\n",
    "    _gcs_train_script_path = os.path.join(gcs_train_script_path, \"trainer/\")  # TRAIN_APP_CODE_PATH = f\"{BUCKET_URI}/{APP_NAME}/{VERSION}/vertex_train/\"\n",
    "\n",
    "    logging.info(f\"training_image_uri: {training_image_uri}\") \n",
    "\n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", _gcs_train_script_path, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", gs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        # {\n",
    "        #     \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "        #     # \"name\": \"gcr.io/kaniko-project/executor:v1.8.0\",        # TODO; downgraded to avoid error in build\n",
    "        #     # \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        #     \"args\": [f\"--destination={training_image_uri}\", \"--cache=false\"],\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['build','-t', f'{training_image_uri}', '.'],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['push', f'{training_image_uri}'], \n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "    \n",
    "    logging.info(f\"training_image_uri: {training_image_uri}\")\n",
    "\n",
    "    # return step outputs\n",
    "    return (\n",
    "        training_image_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bdef3-eae6-45bd-b2be-2fdda7361843",
   "metadata": {},
   "source": [
    "### Create Managed TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1baca5af-40db-480c-bfd4-aa9436535e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/create_tensorboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/create_tensorboard.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=[\n",
    "                       'google-cloud-aiplatform==1.17.0',\n",
    "  ],\n",
    "  output_component_file=\"./pipelines/create_tensorboard.yaml\",\n",
    ")\n",
    "def create_tensorboard(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    gcs_bucket_name: str,\n",
    "    model_display_name: str,\n",
    "    create_tb_resource: bool,\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('tensorboard', Artifact),\n",
    "                            ('tensorboard_resource_name', str),\n",
    "]):\n",
    "\n",
    "    import google.cloud.aiplatform as aiplatform\n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "\n",
    "    # TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    TENSORBOARD_DISPLAY_NAME = f\"tb-{model_display_name}-{version}\"\n",
    "\n",
    "    if create_tb_resource:\n",
    "        logging.info(f\"TENSORBOARD_DISPLAY_NAME: {TENSORBOARD_DISPLAY_NAME}\")\n",
    "\n",
    "        tensorboard = aiplatform.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "\n",
    "        tensorboard_resource_name = tensorboard.resource_name # projects/934903580331/locations/us-central1/tensorboards/6275818857298919424\n",
    "\n",
    "        logging.info(f\"Created tensorboard_resource_name: {tensorboard_resource_name}\")\n",
    "\n",
    "    else:\n",
    "        logging.info(f\"Searching for Existing TB: {TENSORBOARD_DISPLAY_NAME}\")\n",
    "\n",
    "        _tb_resource = aiplatform.TensorboardExperiment.list(\n",
    "            filter=f'display_name=\"{TENSORBOARD_DISPLAY_NAME}\"'\n",
    "        )[0]\n",
    "\n",
    "        # retrieve endpoint uri\n",
    "        tensorboard_resource_name = _tb_resource.resource_name\n",
    "        logging.info(f\"Found existing TB resource: {tensorboard_resource_name}\")\n",
    "\n",
    "        tensorboard = aiplatform.Tensorboard(f'{tensorboard_resource_name}')\n",
    "\n",
    "    return (\n",
    "        tensorboard,\n",
    "        f'{tensorboard_resource_name}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefb60e-21cf-487f-9b89-d7f3ec0f5342",
   "metadata": {},
   "source": [
    "### Train Custom Two-Tower model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ccc41-990a-4e2c-8fe0-401fc1b6bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@component(\n",
    "base_image='python:3.9',\n",
    "packages_to_install=[\n",
    "                       'google-cloud-aiplatform==1.17.0',\n",
    "                       'tensorflow==2.9.2',\n",
    "                       'tensorflow-recommenders==0.7.0',\n",
    "                       'numpy',\n",
    "                       'google-cloud-storage',\n",
    "  ])\n",
    "def train_custom_model(\n",
    "    project: str,\n",
    "    version: str,\n",
    "    model_name: str, \n",
    "    worker_pool_specs: dict,\n",
    "    # vocab_dict_uri: str, \n",
    "    model_dir: str,\n",
    "    training_image_uri: str,\n",
    "    tensorboard_resource_name: str,\n",
    "    service_account: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('query_tower_dir_uri', str),\n",
    "    ('candidate_tower_dir_uri', str),\n",
    "    ('candidate_index_dir_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import logging\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location='us-central1',\n",
    "    )\n",
    "    \n",
    "    JOB_NAME = f'train-{model_name}'\n",
    "    logging.info(f'JOB_NAME: {JOB_NAME}')\n",
    "    logging.info(f'tensorboard_resource_name: {tensorboard_resource_name}')\n",
    "    logging.info(f'service_account: {service_account}')\n",
    "    logging.info(f'worker_pool_specs: {worker_pool_specs}')\n",
    "  \n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=job_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        staging_bucket=base_output_dir,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Submitting train job to Vertex AI...')\n",
    "\n",
    "    job.run(\n",
    "        tensorboard=tensorboard_resource_name,\n",
    "        service_account=f'{service_account}',\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    "    )\n",
    "    \n",
    "    query_tower_dir_uri = f\"gs://{model_dir}/{version}/{experiment_run}/query_tower/\" \n",
    "    candidate_tower_dir_uri = f\"gs://{model_dir}/{version}/{experiment_run}/candidate_tower/\"\n",
    "    candidate_index_dir_uri = f\"gs://{model_dir}/{version}/{experiment_run}/candidate-index/\"\n",
    "    \n",
    "    return (\n",
    "        f'{query_tower_dir_uri}',\n",
    "        f'{candidate_tower_dir_uri}',\n",
    "        f'{candidate_index_dir_uri}'.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf946db3-1c3e-46fc-9e52-493b2a9761da",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32208b6-e678-4043-9bc5-d7bd6ad7c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pipelines import ....\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'{VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version:str,\n",
    "    pipeline_tag: str,\n",
    "    train_image_uri: str,\n",
    "    model_dir: str,\n",
    "    output_dir_gcs_bucket_name: str,\n",
    "    create_tb_resource: bool,\n",
    "    model_name: str,\n",
    "):\n",
    "\n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build Custom TRain Image\n",
    "    # ========================================================================\n",
    "    \n",
    "    build_custom_train_image_op = (\n",
    "        build_custom_train_image.build_custom_train_image(\n",
    "            project=project,\n",
    "            gcs_train_script_path=gcs_train_script_path,\n",
    "            training_image_uri=train_image_uri,\n",
    "        )\n",
    "        .set_display_name(\"Build custom train image\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    create_tensorboard_op = (\n",
    "        create_tensorboard.create_tensorboard(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=version,\n",
    "            gcs_bucket_name=output_dir_gcs_bucket_name,\n",
    "            create_tb_resource=create_tb_resource\n",
    "        )\n",
    "        .set_display_name(\"Tensorboard Instance\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    run_train_task_op = (\n",
    "        train_custom_model.train_custom_model(\n",
    "            project=project,\n",
    "            version=version,\n",
    "            model_name = model_name,\n",
    "            worker_pool_specs=WORKER_POOL_SPECS, \n",
    "            base_output_dir=BASE_OUTPUT_DIR,\n",
    "            vocab_dict_uri = 'todo', \n",
    "            training_image_uri=build_custom_train_image_op.outputs['ccc'],     # TRAIN_IMAGE,\n",
    "            tensorboard_resource_name=create_tensorboard_op.outputs['tensorboard_resource_name'],\n",
    "            service_account=service_account,\n",
    "        )\n",
    "        .set_display_name(\"Multiworker Training\")\n",
    "        .set_caching_options(True)  # TODO\n",
    "        .after(build_custom_train_image_op)\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
