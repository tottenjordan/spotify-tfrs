{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38858d8a-935c-4e80-aa32-28bf4f665938",
   "metadata": {},
   "source": [
    "## Orchestrate RecSys workflow with Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accec8f9-be16-4e39-b903-d42ac2f9b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639915f-9dc1-47fe-b896-e14848dcbc59",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ec70b5-0bd6-4a87-a498-74db04ee7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform==1.17.0 --upgrade\n",
    "# !pip install google-cloud-pipeline-components==1.0.19 --upgrade\n",
    "# !pip install kfp==1.8.13 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3feca7cb-541a-41ee-b1f5-e15867c41fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.13\n",
      "google_cloud_pipeline_components version: 1.0.19\n",
      "aiplatform SDK version: 1.17.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ac58ad-b893-4e25-bbf3-7dcaa9cb34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "# from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "# GCP\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "# from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983cc6c-9ecc-41c6-a308-ab21060d5497",
   "metadata": {},
   "source": [
    "### Setup clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5531f13f-4062-4b2e-a3f8-c66f11a7fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Setup Clients\n",
    "# ====================================================\n",
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID\n",
    "\n",
    "# colab_auth.authenticate_user()\n",
    "\n",
    "storage_client = storage.Client(\n",
    "    project=PROJECT_ID\n",
    ")\n",
    "\n",
    "pipeline_client = AIPlatformClient(\n",
    "  project_id=PROJECT_ID,\n",
    "  region=LOCATION,\n",
    ")\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID, location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c89c04-abb9-4ff3-8a86-7f1d2b7ffb54",
   "metadata": {},
   "source": [
    "### Pipeline job vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3953ae3-89c8-42a7-bb7d-6a52b21fbafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT_PATH: gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipeline_root\n",
      "PIPELINES_FILEPATH: gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipeline_root/pipelines.json\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# ====================================================\n",
    "# Job vars \n",
    "# (consistent with manual train job execution)\n",
    "# ====================================================\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "MODEL_VERSION = 'v15'\n",
    "PIPELINE_VERSION = 'v3'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}-{PIPELINE_VERSION}'\n",
    "\n",
    "EXPERIMENT_PREFIX = 'pipe-dev'                                                  # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}'\n",
    "RUN_NAME=f'run-{TIMESTAMP}'\n",
    "\n",
    "# TODO:\n",
    "DATA_REGIME = 'small-jt-tfrecord'\n",
    "IMAGE_URI = 'gcr.io/hybrid-vertex/sp-2tower-tfrs-v15-v0-training'\n",
    "\n",
    "# ====================================================\n",
    "# Pipeline output repo \n",
    "# ====================================================\n",
    "OUTPUT_BUCKET = 'jt-tfrs-test'\n",
    "STAGING_BUCKET =f'gs://{OUTPUT_BUCKET}'\n",
    "\n",
    "DOCKERNAME_TRAIN = 'Dockerfile.tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'gs://{OUTPUT_BUCKET}/{MODEL_ROOT_NAME}/pipeline_root'\n",
    "# PIPELINE_ROOT_PATH = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}/pipeline_root' # TODO: in future version\n",
    "print('PIPELINE_ROOT_PATH: {}'.format(PIPELINE_ROOT_PATH))\n",
    "\n",
    "# Optional: save and load pipeline definition\n",
    "PIPELINES = {}\n",
    "\n",
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipelines.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "if os.path.isfile(PIPELINES_FILEPATH):\n",
    "    with open(PIPELINES_FILEPATH) as f:\n",
    "        PIPELINES = json.load(f)\n",
    "else:\n",
    "    PIPELINES = {}\n",
    "\n",
    "def save_pipelines():\n",
    "    with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "        json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913503a0-2de1-4e76-ac31-de8bef3511dd",
   "metadata": {},
   "source": [
    "## Create Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c77416-6928-45f3-affa-1fc532bfd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/pipelines\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/pipelines\n",
    "# ! touch {REPO_DOCKER_PATH_PREFIX}/pipelines/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f66a90-d8fd-4c7c-a0f6-b4cff1e00398",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TODO: Build custom train image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699aedd1-a27f-4a82-ae2f-ec6a093e41c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/spotify-tfrs\n",
      "Dockerfile.tfrs\n",
      "gs://jt-tfrs-test\n",
      "v15\n",
      "v3\n",
      "small-jt-tfrecord\n"
     ]
    }
   ],
   "source": [
    "!export PWD=pwd\n",
    "!export DOCKERNAME_TRAIN=DOCKERNAME_TRAIN\n",
    "!export STAGING_BUCKET=STAGING_BUCKET\n",
    "!export MODEL_VERSION=MODEL_VERSION\n",
    "!export PIPELINE_VERSION=PIPELINE_VERSION\n",
    "!export DATA_REGIME=DATA_REGIME\n",
    "! echo $PWD\n",
    "! echo $DOCKERNAME_TRAIN\n",
    "! echo $STAGING_BUCKET\n",
    "! echo $MODEL_VERSION\n",
    "! echo $PIPELINE_VERSION\n",
    "! echo $DATA_REGIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e330af-0f5f-4bde-8ce9-21af37b9e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP: sp\n",
      "MODEL_TYPE: 2tower\n",
      "FRAMEWORK: tfrs\n",
      "MODEL_VERSION: v15\n",
      "PIPELINE_VERSION: v3\n",
      "\n",
      "MODEL_ROOT_NAME: sp-2tower-tfrs-v15-v3\n",
      "OUTPUT_BUCKET: jt-tfrs-test\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-v15-v0-training\n",
      "EXPERIMENT_NAME: pipe-dev-2tower-tfrs-v15\n",
      "\n",
      "BASE_OUTPUT_DIR   : gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15\n",
      "PIPELINE_ROOT_PATH: gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "print(f\"APP: {APP}\")\n",
    "print(f\"MODEL_TYPE: {MODEL_TYPE}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_VERSION: {MODEL_VERSION}\")\n",
    "print(f\"PIPELINE_VERSION: {PIPELINE_VERSION}\\n\")\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")\n",
    "print(f\"OUTPUT_BUCKET: {OUTPUT_BUCKET}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\\n\")\n",
    "\n",
    "# print(f\"RUN_NAME: {RUN_NAME}\")\n",
    "\n",
    "BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{MODEL_ROOT_NAME}/{EXPERIMENT_NAME}'\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"PIPELINE_ROOT_PATH: {PIPELINE_ROOT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b60bd-b7b3-479b-b368-7a3162933a60",
   "metadata": {},
   "source": [
    "#### TODO: parameterize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed6f84fe-6d03-4fb4-93b3-1fe7b46d73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/spotify-tfrs/src/Dockerfile.tfrs [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  270.0 B/  270.0 B]                                                \n",
      "Operation completed over 1 objects/270.0 B.                                      \n",
      "Copying file:///home/jupyter/spotify-tfrs/src/cloudbuild.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  178.0 B/  178.0 B]                                                \n",
      "Operation completed over 1 objects/178.0 B.                                      \n",
      "Copying file:///home/jupyter/spotify-tfrs/src/trainer/__init__.py [Content-Type=text/x-python]...\n",
      "Copying file:///home/jupyter/spotify-tfrs/src/trainer/_data.py [Content-Type=text/x-python]...\n",
      "Copying file:///home/jupyter/spotify-tfrs/src/trainer/_model.py [Content-Type=text/x-python]...\n",
      "Copying file:///home/jupyter/spotify-tfrs/src/trainer/interactive_train.py [Content-Type=text/x-python]...\n",
      "Copying file:///home/jupyter/spotify-tfrs/src/trainer/requirements.txt [Content-Type=text/plain]...\n",
      "Copying file:///home/jupyter/spotify-tfrs/src/trainer/task.py [Content-Type=text/x-python]...\n",
      "Copying file:///home/jupyter/spotify-tfrs/src/trainer/train_config.py [Content-Type=text/x-python]...\n",
      "/ [7/7 files][ 58.4 KiB/ 58.4 KiB] 100% Done                                    \n",
      "Operation completed over 7 objects/58.4 KiB.                                     \n",
      "Copied training application code and Dockerfile to gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src\n"
     ]
    }
   ],
   "source": [
    "# # # copy training Dockerfile\n",
    "# ! gsutil cp $PWD/src/Dockerfile.tfrs $BUCKET_URI/$VERSION/src\n",
    "!gsutil cp $PWD/src/Dockerfile.tfrs gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/\n",
    "!gsutil cp $PWD/src/cloudbuild.yaml gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/\n",
    "\n",
    "# # # copy training application code\n",
    "! gsutil -m cp -r $PWD/src/trainer/* gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer\n",
    "\n",
    "print(f\"Copied training application code and Dockerfile to {BASE_OUTPUT_DIR}/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ec8c20-0bd7-44c5-9ff1-558d333f8dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       178  2022-09-23T16:36:46Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src\n",
      "gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/:\n",
      "       270  2022-09-23T16:38:51Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/Dockerfile.tfrs\n",
      "       178  2022-09-23T16:38:53Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/cloudbuild.yaml\n",
      "\n",
      "gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/:\n",
      "         0  2022-09-23T16:38:56Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/__init__.py\n",
      "      5651  2022-09-23T16:38:56Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/_data.py\n",
      "     30991  2022-09-23T16:38:56Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/_model.py\n",
      "        46  2022-09-23T16:38:56Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/interactive_train.py\n",
      "       166  2022-09-23T16:38:56Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/requirements.txt\n",
      "     22767  2022-09-23T16:38:56Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/task.py\n",
      "       193  2022-09-23T16:38:56Z  gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/trainer/train_config.py\n",
      "TOTAL: 10 objects, 60440 bytes (59.02 KiB)\n"
     ]
    }
   ],
   "source": [
    "# # # list copied files from GCS location\n",
    "! gsutil ls -Rl gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipe-dev-2tower-tfrs-v15/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5458a79-92c8-4003-b67e-6d171c784a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/build_custom_train_image.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/build_custom_train_image.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    # output_component_file=\"./pipelines/build_custom_train_image.yaml\",\n",
    ")\n",
    "def build_custom_train_image(\n",
    "    project: str, \n",
    "    gcs_train_script_path: str,   # TRAIN_APP_CODE_PATH = f\"{BUCKET_URI}/{VERSION}/src/\" # jt-tfrs-test/pipev1/src\n",
    "    training_image_uri: str,      # TRAIN_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/multiworker:2tower-pipe-{VERSION}\"\n",
    "    train_dockerfile_name: str,   # Dockerfile.tfrs\n",
    ") -> NamedTuple(\"Outputs\", [(\"training_image_uri\", str)]):\n",
    "\n",
    "    # TODO: make output Artifact for image_uri\n",
    "    \"\"\"\n",
    "    custom pipeline component to build custom training image using\n",
    "    Cloud Build and the training application code and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "\n",
    "    import logging\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "    \n",
    "    logging.info(f\"train_dockerfile_name: {train_dockerfile_name}\")\n",
    "\n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    gcs_dockerfile_path = os.path.join(gcs_train_script_path, f\"{train_dockerfile_name}\")   # two-tower-pipes/2tower-recsys/vertex_train\n",
    "    # gcs_cloudbuild_path = os.path.join(gcs_train_script_path, f\"cloudbuild.yaml\")\n",
    "    gcs_train_script_dir = os.path.join(gcs_train_script_path, \"trainer/\")  # TRAIN_APP_CODE_PATH = f\"{BUCKET_URI}/{APP_NAME}/{VERSION}/vertex_train/\"\n",
    "    \n",
    "    logging.info(f\"gcs_dockerfile_path: {gcs_dockerfile_path}\")\n",
    "    # logging.info(f\"gcs_cloudbuild_path: {gcs_cloudbuild_path}\")\n",
    "    logging.info(f\"gcs_train_script_dir: {gcs_train_script_dir}\")\n",
    "    \n",
    "    logging.info(f\"training_image_uri: {training_image_uri}\") \n",
    "     \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", gcs_train_script_dir, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", gcs_dockerfile_path, f\"{train_dockerfile_name}\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        # {\n",
    "        #     \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "        #     # \"name\": \"gcr.io/kaniko-project/executor:v1.8.0\",        # TODO; downgraded to avoid error in build\n",
    "        #     # \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        #     \"args\": [f\"--destination={training_image_uri}\", \"--cache=false\"],\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['build','-t', f'{training_image_uri}', '.'],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['push', f'{training_image_uri}'], \n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    logging.info(f\"Elapsed timefor build: {elapsed_time}\")\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "    \n",
    "    logging.info(f\"training_image_uri: {training_image_uri}\")\n",
    "\n",
    "    # return step outputs\n",
    "    return (\n",
    "        training_image_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bdef3-eae6-45bd-b2be-2fdda7361843",
   "metadata": {},
   "source": [
    "### Create Managed TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1baca5af-40db-480c-bfd4-aa9436535e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/create_tensorboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/create_tensorboard.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.17.0',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/create_tensorboard.yaml\",\n",
    ")\n",
    "def create_tensorboard(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    gcs_bucket_name: str,\n",
    "    model_display_name: str,\n",
    "    create_tb_resource: bool,\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('tensorboard', Artifact),\n",
    "                            ('tensorboard_resource_name', str),\n",
    "]):\n",
    "\n",
    "    import google.cloud.aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "\n",
    "    # TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    TENSORBOARD_DISPLAY_NAME = f\"tb-{model_display_name}-{model_version}\"\n",
    "\n",
    "    if create_tb_resource==True:\n",
    "        logging.info(f\"TENSORBOARD_DISPLAY_NAME: {TENSORBOARD_DISPLAY_NAME}\")\n",
    "\n",
    "        tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "\n",
    "        tensorboard_resource_name = tensorboard.resource_name # projects/934903580331/locations/us-central1/tensorboards/6275818857298919424\n",
    "\n",
    "        logging.info(f\"Created tensorboard_resource_name: {tensorboard_resource_name}\")\n",
    "\n",
    "    else:\n",
    "        logging.info(f\"Searching for Existing TB: {TENSORBOARD_DISPLAY_NAME}\")\n",
    "\n",
    "        _tb_resource = vertex_ai.TensorboardExperiment.list(\n",
    "            filter=f'display_name=\"{TENSORBOARD_DISPLAY_NAME}\"'\n",
    "        )[0]\n",
    "\n",
    "        # retrieve endpoint uri\n",
    "        tensorboard_resource_name = _tb_resource.resource_name\n",
    "        logging.info(f\"Found existing TB resource: {tensorboard_resource_name}\")\n",
    "\n",
    "        tensorboard = vertex_ai.Tensorboard(f'{tensorboard_resource_name}')\n",
    "\n",
    "    return (\n",
    "        tensorboard,\n",
    "        f'{tensorboard_resource_name}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7f764-5263-4e98-bfc6-3b1f4cf7d492",
   "metadata": {},
   "source": [
    "### Build vocabs and stats\n",
    "\n",
    "** TODO:**\n",
    "* parallel tasks for `candidates` and `playlist` data\n",
    "* seperate calcs for `train` and `valid` splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e28e080-ccab-4801-9c96-75715f1a5102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/build_vocabs_stats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/build_vocabs_stats.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.17.0',\n",
    "        'tensorflow==2.9.2',\n",
    "        'tensorflow-recommenders==0.7.0',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/build_vocabs_stats.yaml\",\n",
    ")\n",
    "def build_vocabs_string_lookups(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    # version: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    train_dir: str,\n",
    "    train_dir_prefix: str,\n",
    "    valid_dir: str,\n",
    "    valid_dir_prefix: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str, \n",
    "    # gcs_bucket_name: str,\n",
    "    # path_to_train_dir: str,\n",
    "    # path_to_valid_dir: str,\n",
    "    # path_to_candidate_dir: str,\n",
    "    path_to_vocab_file: str,\n",
    "    vocab_file_name: str,\n",
    "    generate_vocabs_stats: bool,\n",
    "    max_padding: int = 375,\n",
    "    split_names: list = ['train', 'valid'],\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('vocab_dict', Artifact),\n",
    "                            ('vocab_gcs_filename', str),\n",
    "                            ('vocab_gcs_sub_dir', str),\n",
    "                            ('vocab_gcs_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import json\n",
    "    import logging\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_recommenders as tfrs\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import string\n",
    "\n",
    "    # TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # logging.info(f'TIMESTAMP: {TIMESTAMP}')\n",
    "    \n",
    "    MAX_PLAYLIST_LENGTH = max_padding       # 375\n",
    "    logging.info(f'MAX_PLAYLIST_LENGTH: {MAX_PLAYLIST_LENGTH}')\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    ################################################################################\n",
    "    # Helper Functions for feature parsing\n",
    "    ################################################################################\n",
    "    \n",
    "    candidate_features = {\n",
    "        'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    }\n",
    "\n",
    "    all_features = {\n",
    "        'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'pos_seed_track': tf.io.FixedLenFeature(dtype=tf.int64, shape=()),\n",
    "        'track_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'track_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'duration_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'track_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_genres_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_followers_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'pid': tf.io.FixedLenFeature(dtype=tf.int64, shape=()),\n",
    "        'name': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'collaborative': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'duration_ms_seed_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'n_songs_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'num_artists_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'num_albums_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'description_pl': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        ###ragged\n",
    "        'track_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "        'artist_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "        'album_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "        'track_uri_pl': tf.io.RaggedFeature(tf.string),\n",
    "        'duration_ms_songs_pl': tf.io.RaggedFeature(tf.float32),\n",
    "        'artist_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "        'artists_followers_pl': tf.io.RaggedFeature(tf.float32),\n",
    "        'track_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "        'artist_genres_pl': tf.io.RaggedFeature(tf.string),\n",
    "    }\n",
    "    \n",
    "    def parse_candidate_tfrecord_fn(example, feature_dict=candidate_features):\n",
    "        example = tf.io.parse_single_example(\n",
    "            example, \n",
    "            features=feature_dict\n",
    "        )\n",
    "        return example\n",
    "    \n",
    "    def parse_tfrecord_fn(example, feature_dict=all_features): # =all_features\n",
    "        example = tf.io.parse_single_example(\n",
    "            example, \n",
    "            features=feature_dict\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    def pad_up_to(t, max_in_dims=[1 ,MAX_PLAYLIST_LENGTH], constant_value=''):\n",
    "        s = tf.shape(t)\n",
    "        paddings = [[0, m-s[i]] for (i,m) in enumerate(max_in_dims)]\n",
    "        return tf.pad(t, paddings, 'CONSTANT', constant_values=constant_value)\n",
    "\n",
    "    def return_padded_tensors(data):\n",
    "\n",
    "        a = pad_up_to(tf.reshape(data['track_name_pl'], shape=(1,-1)) , constant_value='')\n",
    "        b = pad_up_to(tf.reshape(data['artist_name_pl'], shape=(1,-1)) , constant_value='')\n",
    "        c = pad_up_to(tf.reshape(data['album_name_pl'], shape=(1,-1)) , constant_value='')\n",
    "        d = pad_up_to(tf.reshape(data['track_uri_pl'], shape=(1, -1,)) , constant_value='')\n",
    "        e = pad_up_to(tf.reshape(data['duration_ms_songs_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "        f = pad_up_to(tf.reshape(data['artist_pop_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "        g = pad_up_to(tf.reshape(data['artists_followers_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "        h = pad_up_to(tf.reshape(data['track_pop_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "        i = pad_up_to(tf.reshape(data['artist_genres_pl'], shape=(1,-1)) , constant_value='')\n",
    "\n",
    "        padded_data = data.copy()\n",
    "        padded_data['track_name_pl'] = a\n",
    "        padded_data['artist_name_pl'] = b\n",
    "        padded_data['album_name_pl'] = c\n",
    "        padded_data['track_uri_pl'] = d\n",
    "        padded_data['duration_ms_songs_pl'] = e\n",
    "        padded_data['artist_pop_pl'] = f\n",
    "        padded_data['artists_followers_pl'] = g\n",
    "        padded_data['track_pop_pl'] = h\n",
    "        padded_data['artist_genres_pl'] = i\n",
    "\n",
    "        return padded_data\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    if generate_vocabs_stats == False:\n",
    "        VOCAB_FILENAME = f'{vocab_file_name}'\n",
    "        VOCAB_GCS_SUB_DIR = f'{path_to_vocab_file}'\n",
    "        VOCAB_GCS_URI = f'gs://{train_output_gcs_bucket}/{VOCAB_GCS_SUB_DIR}/{VOCAB_FILENAME}'\n",
    "        \n",
    "        logging.info(f\"Loading Vocab File: {VOCAB_FILENAME} from {VOCAB_GCS_URI}\")\n",
    "        \n",
    "        with open(f'{VOCAB_FILENAME}', 'wb') as file_obj:\n",
    "            storage_client.download_blob_to_file(\n",
    "                f'gs://{VOCAB_GCS_URI}', file_obj)\n",
    "\n",
    "\n",
    "        with open(f'{VOCAB_FILENAME}', 'rb') as pickle_file:\n",
    "            vocab_dict = pkl.load(pickle_file)\n",
    "        \n",
    "    else:\n",
    "        # ========================================================================\n",
    "        # Candidate dataset\n",
    "        # ========================================================================\n",
    "\n",
    "        candidate_files = []\n",
    "        for blob in storage_client.list_blobs(f'{candidate_file_dir}', prefix=f'{candidate_files_prefix}', delimiter=\"/\"):\n",
    "            candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "        raw_candidate_dataset = tf.data.TFRecordDataset(candidate_files)\n",
    "        parsed_candidate_dataset = raw_candidate_dataset.map(parse_candidate_tfrecord_fn)\n",
    "\n",
    "        '''\n",
    "        Get vocabularies of unique values for strings\n",
    "        '''\n",
    "        logging.info(f\"Getting unqiue values for `artist_name_can`...\")\n",
    "        v_artist_name_can = np.unique(\n",
    "          np.concatenate(\n",
    "              list(\n",
    "                  parsed_candidate_dataset.map(\n",
    "                      lambda x: x['artist_name_can']\n",
    "                  )\n",
    "                  .batch(1000)\n",
    "              )\n",
    "          )\n",
    "        )\n",
    "        v_artist_name_can_cleaned = [str(z).replace(\"b'\",\"\").translate(str.maketrans('', '', string.punctuation)) for z in v_artist_name_can]\n",
    "    \n",
    "        # v_track_uri_can = np.unique(\n",
    "        #   np.concatenate(\n",
    "        #       list(\n",
    "        #           parsed_candidate_dataset.map(\n",
    "        #               lambda x: x['track_uri_can']\n",
    "        #           )\n",
    "        #           .batch(1000)\n",
    "        #       )\n",
    "        #   )\n",
    "        # )\n",
    "        # v_track_uri_can_cleaned = [str(z).replace(\"b'\",\"\").translate(str.maketrans('', '', string.punctuation)) for z in v_track_uri_can]\n",
    "        \n",
    "        logging.info(f\"Getting unqiue values for `track_name_can`...\") \n",
    "        v_track_name_can = np.unique(\n",
    "          np.concatenate(\n",
    "              list(\n",
    "                  parsed_candidate_dataset.map(\n",
    "                      lambda x: x['track_name_can']\n",
    "                  )\n",
    "                  .batch(1000)\n",
    "              )\n",
    "          )\n",
    "        )\n",
    "        v_track_name_can_cleaned = [str(z).replace(\"b'\",\"\").translate(str.maketrans('', '', string.punctuation)) for z in v_track_name_can]\n",
    "    \n",
    "        logging.info(f\"Getting unqiue values for `album_name_can`...\")\n",
    "        v_album_name_can = np.unique(\n",
    "          np.concatenate(\n",
    "              list(\n",
    "                  parsed_candidate_dataset.map(\n",
    "                      lambda x: x['album_name_can']\n",
    "                  )\n",
    "                  .batch(1000)\n",
    "              )\n",
    "          )\n",
    "        )\n",
    "        v_album_name_can_cleaned = [str(z).replace(\"b'\",\"\").translate(str.maketrans('', '', string.punctuation)) for z in v_album_name_can]\n",
    "\n",
    "        logging.info(f\"Getting unqiue values for `artist_genres_can`...\")\n",
    "        v_artist_genres_can = np.unique(\n",
    "          np.concatenate(\n",
    "              list(\n",
    "                  parsed_candidate_dataset.map(\n",
    "                      lambda x: x['artist_genres_can']\n",
    "                  )\n",
    "                  .batch(1000)\n",
    "              )\n",
    "          )\n",
    "        )\n",
    "        v_artist_genres_can_cleaned = [str(z).replace(\"b'\",\"\").translate(str.maketrans('', '', string.punctuation)) for z in v_artist_genres_can]\n",
    "\n",
    "        # ========================================================================\n",
    "        # TODO: parameterize / automate\n",
    "        # ========================================================================\n",
    "        avg_duration_ms_seed_pl = 13000151.68\n",
    "        var_duration_ms_seed_pl = 133092900971233.58\n",
    "\n",
    "        avg_n_songs_pl = 55.21\n",
    "        var_n_songs_pl = 2317.54\n",
    "\n",
    "        avg_n_artists_pl = 30.56\n",
    "        var_n_artists_pl = 769.26\n",
    "\n",
    "        avg_n_albums_pl = 40.25\n",
    "        var_n_albums_pl = 1305.54\n",
    "\n",
    "        avg_artist_pop = 16.08\n",
    "        var_artist_pop = 300.64\n",
    "\n",
    "        avg_duration_ms_songs_pl = 234823.14\n",
    "        var_duration_ms_songs_pl = 5558806228.41\n",
    "\n",
    "        avg_artist_followers = 43337.77\n",
    "        var_artist_followers = 377777790193.57\n",
    "\n",
    "        avg_track_pop = 10.85\n",
    "        var_track_pop = 202.18\n",
    "\n",
    "        # ========================================================================\n",
    "        # Create Vocab Dict\n",
    "        # ========================================================================\n",
    "    \n",
    "        vocab_dict = {\n",
    "            'artist_name_can': v_artist_name_can_cleaned,\n",
    "            'track_name_can': v_track_name_can_cleaned,\n",
    "            'album_name_can': v_album_name_can_cleaned,\n",
    "            'artist_genres_can': v_artist_genres_can_cleaned,\n",
    "            'avg_duration_ms_seed_pl': avg_duration_ms_seed_pl,\n",
    "            'var_duration_ms_seed_pl': var_duration_ms_seed_pl,\n",
    "            'avg_n_songs_pl': avg_n_songs_pl,\n",
    "            'var_n_songs_pl': var_n_songs_pl,\n",
    "            'avg_n_artists_pl': avg_n_artists_pl,\n",
    "            'var_n_artists_pl': var_n_artists_pl,\n",
    "            'avg_n_albums_pl': avg_n_albums_pl,\n",
    "            'var_n_albums_pl': var_n_albums_pl,\n",
    "            'avg_artist_pop': avg_artist_pop,\n",
    "            'var_artist_pop': var_artist_pop,\n",
    "            'avg_duration_ms_songs_pl': avg_duration_ms_songs_pl,\n",
    "            'var_duration_ms_songs_pl': var_duration_ms_songs_pl,\n",
    "            'avg_artist_followers': avg_artist_followers,\n",
    "            'var_artist_followers': var_artist_followers,\n",
    "            'avg_track_pop': avg_track_pop,\n",
    "            'var_track_pop': var_track_pop,\n",
    "        }\n",
    "        VOCAB_FILENAME = f'vocab_dict_{experiment_run}.pkl'\n",
    "        VOCAB_GCS_SUB_DIR = f'{experiment_name}/{experiment_run}/vocabs_stats'\n",
    "        VOCAB_GCS_URI = f'gs://{train_output_gcs_bucket}/{VOCAB_GCS_SUB_DIR}/{VOCAB_FILENAME}'\n",
    "        \n",
    "        logging.info(f\"Saving Vocab File: {VOCAB_FILENAME} to {VOCAB_GCS_URI}\")\n",
    "        \n",
    "        # Upload vocab_dict to GCS\n",
    "        bucket = storage_client.bucket(train_output_gcs_bucket)\n",
    "        blob = bucket.blob(f'{VOCAB_GCS_SUB_DIR}/{VOCAB_FILENAME}')\n",
    "        pickle_out = pkl.dumps(vocab_dict)\n",
    "        blob.upload_from_string(pickle_out)\n",
    "        \n",
    "        return (\n",
    "            vocab_dict,\n",
    "            f'{VOCAB_FILENAME}',\n",
    "            f'{VOCAB_GCS_SUB_DIR}',\n",
    "            f'{VOCAB_GCS_URI}',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefb60e-21cf-487f-9b89-d7f3ec0f5342",
   "metadata": {},
   "source": [
    "### Train Custom Two-Tower model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849ccc41-990a-4e2c-8fe0-401fc1b6bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/train_custom_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/train_custom_model.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.17.0',\n",
    "        'tensorflow==2.9.2',\n",
    "        'tensorflow-recommenders==0.7.0',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def train_custom_model(\n",
    "    project: str,\n",
    "    # version: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    model_name: str, \n",
    "    worker_pool_specs: dict,\n",
    "    vocab_dict_uri: str, \n",
    "    train_output_gcs_bucket: str,                         # change to workdir?\n",
    "    training_image_uri: str,\n",
    "    tensorboard_resource_name: str,\n",
    "    service_account: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('query_tower_dir_uri', str),\n",
    "    ('candidate_tower_dir_uri', str),\n",
    "    # ('candidate_index_dir_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location='us-central1',\n",
    "    )\n",
    "    \n",
    "    JOB_NAME = f'train-{model_name}'\n",
    "    logging.info(f'JOB_NAME: {JOB_NAME}')\n",
    "    \n",
    "    BASE_OUTPUT_DIR = f'gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}'\n",
    "    logging.info(f'BASE_OUTPUT_DIR: {BASE_OUTPUT_DIR}')\n",
    "    \n",
    "    logging.info(f'vocab_dict_uri: {vocab_dict_uri}')\n",
    "    \n",
    "    logging.info(f'tensorboard_resource_name: {tensorboard_resource_name}')\n",
    "    logging.info(f'service_account: {service_account}')\n",
    "    logging.info(f'worker_pool_specs: {worker_pool_specs}')\n",
    "  \n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=JOB_NAME,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        staging_bucket=BASE_OUTPUT_DIR,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Submitting train job to Vertex AI...')\n",
    "\n",
    "    job.run(\n",
    "        tensorboard=tensorboard_resource_name,\n",
    "        service_account=f'{service_account}',\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    "        sync=False,\n",
    "    )\n",
    "    \n",
    "    # TODO: this is hardcoded, but created in train job\n",
    "    query_tower_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/model-dir/query_tower\" \n",
    "    candidate_tower_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/model-dir/candidate_tower\"\n",
    "    # candidate_index_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/candidate-index\"\n",
    "    \n",
    "    return (\n",
    "        f'{query_tower_dir_uri}',\n",
    "        f'{candidate_tower_dir_uri}',\n",
    "        # f'{candidate_index_dir_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79545a-5311-4db4-986c-20a4f8867969",
   "metadata": {},
   "source": [
    "### Find Model Endpoint\n",
    "\n",
    "> TODO: inspect component behavior for different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e348d026-ce80-4f07-874e-cfbe6919ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/find_model_endpoint_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/find_model_endpoint_test.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=['google-cloud-aiplatform==1.17.0'],\n",
    "    # output_component_file=\"./pipelines/find_model_endpoint.yaml\",\n",
    ")\n",
    "def find_model_endpoint_test(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    endpoint_name: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('create_new_endpoint', str),\n",
    "                            ('existing_endpoint_uri', str),\n",
    "                            ('deployed_models_count', int),\n",
    "                            ('undeploy_model_needed', str),\n",
    "                            ('deployed_model_list', list),\n",
    "                            ('endpoint_traffic_split', str),\n",
    "]):\n",
    "\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    deployed_model_list = []\n",
    "\n",
    "    logging.info(f\"Searching for model endpoint: {endpoint_name}\")\n",
    "\n",
    "    if vertex_ai.Endpoint.list(\n",
    "        filter=f'display_name=\"{endpoint_name}\"'):\n",
    "        '''\n",
    "        Because existing Endpoint found: \n",
    "            (1) will not create new\n",
    "            (2) Need the endpoint uri\n",
    "            (3) Need list of deployed models on this endpoint;\n",
    "            (4) If more than 1 deployed model exists, trigger subsequent conditional step\n",
    "                to undeploy all but 1 (latest) model \n",
    "\n",
    "        '''\n",
    "        logging.info(f\"Model endpoint, {endpoint_name}, already exists\")\n",
    "        create_new_endpoint=\"False\"\n",
    "    \n",
    "        # create endpoint list resource in memory\n",
    "        _endpoint = vertex_ai.Endpoint.list(\n",
    "            filter=f'display_name=\"{endpoint_name}\"'\n",
    "        )[0]\n",
    "        \n",
    "        logging.info(f\"Parsing details for _endpoint: {_endpoint}\")\n",
    "\n",
    "        # retrieve endpoint uri\n",
    "        existing_endpoint_uri = _endpoint.resource_name\n",
    "        logging.info(f\"existing_endpoint_uri: {existing_endpoint_uri}\")\n",
    "        _traffic_split = _endpoint.traffic_split\n",
    "\n",
    "        # retrieve deployed model IDs\n",
    "        deployed_models = _endpoint.gca_resource.deployed_models\n",
    "        deployed_models_count = len(deployed_models)\n",
    "        logging.info(f\"deployed_models_count: {deployed_models_count}\")\n",
    "\n",
    "        if deployed_models_count > 1:\n",
    "            # deployed_model_id_0 = _endpoint.gca_resource.deployed_models[0].id\n",
    "            # deployed_model_id_1 = _endpoint.gca_resource.deployed_models[1].id\n",
    "            undeploy_model_needed = \"True\"                               # arbitrary assumption: no more than 2 (3) models per model_endpoint\n",
    "            for model in deployed_models:\n",
    "                deployed_model_list.append(model.id)\n",
    "        elif deployed_models_count == 0:\n",
    "            undeploy_model_needed = \"False\"\n",
    "        else:\n",
    "            undeploy_model_needed = \"False\"\n",
    "            deployed_model_list.append(_endpoint.gca_resource.deployed_models[0].id)\n",
    "\n",
    "        # deployed_model_id = _endpoint.gca_resource.deployed_models[0].id\n",
    "        logging.info(f\"Currently deployed_model_list {deployed_model_list}\")\n",
    "\n",
    "    else:\n",
    "        logging.info(f\"Model endpoint, {endpoint_name}, does not exist\")\n",
    "\n",
    "        create_new_endpoint=\"True\"\n",
    "        deployed_models_count=0\n",
    "        existing_endpoint_uri=\"N/A\"\n",
    "        undeploy_model_needed = \"N/A\"\n",
    "        _traffic_split = \"N/A\"\n",
    "        # deployed_model_list = []\n",
    "\n",
    "        logging.info(f\"create_new_endpoint {create_new_endpoint}\")\n",
    "        logging.info(f\"existing_endpoint_uri {existing_endpoint_uri}\")\n",
    "        logging.info(f\"deployed_models_count {deployed_models_count}\")\n",
    "        logging.info(f\"undeploy_model_needed {undeploy_model_needed}\")\n",
    "        logging.info(f\"deployed_model_list {deployed_model_list}\")\n",
    "        logging.info(f\"_traffic_split {_traffic_split}\")\n",
    "\n",
    "\n",
    "    return (\n",
    "        f'{create_new_endpoint}',\n",
    "        f'{existing_endpoint_uri}',\n",
    "        deployed_models_count,\n",
    "        f'{undeploy_model_needed}',\n",
    "        deployed_model_list,\n",
    "        f'{_traffic_split}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2aa34-f512-4f5f-a9fe-d4519b8b2f4f",
   "metadata": {},
   "source": [
    "### Generate Candidate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "294f2cc6-257c-43a3-b7cb-ec32993f98c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipelines/generate_candidate_embedding_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/pipelines/generate_candidate_embedding_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.17.0',\n",
    "        'tensorflow==2.9.2',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/generate_candidate_embedding_index.yaml\",\n",
    ")\n",
    "def generate_candidate_embedding_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    # version: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    # data_bucket_name: str,\n",
    "    model_dir: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    # candidate_items_prefix: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    embedding_index_destination_gcs_uri: str,\n",
    "    uploaded_candidate_model_resources: str,\n",
    ") -> NamedTuple('Outputs', [('candidate_embedding_index_file_uri', str),\n",
    "                            ('embedding_index_gcs_bucket', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import tensorflow as tf\n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    # TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    # initialize clients\n",
    "    storage_client = storage.Client(project=project)\n",
    "    vertex_ai.init(project=project,location=location)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Helper Functions\n",
    "    # ========================================================================\n",
    "    \n",
    "    candidate_features = {\n",
    "        'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    }\n",
    "\n",
    "    def parse_candidate_tfrecord_fn(example):\n",
    "        example = tf.io.parse_single_example(\n",
    "            example, \n",
    "            features=candidate_features\n",
    "        )\n",
    "        return example\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Load Saved Model\n",
    "    # ========================================================================\n",
    "    # TODO: use model uploaded to Vertex Registry\n",
    "    \n",
    "    logging.info(f\"loaded_candidate_tower from model_dir: {model_dir}\")\n",
    "    loaded_candidate_tower = tf.saved_model.load(model_dir)\n",
    "    \n",
    "    candidate_predictor = loaded_candidate_tower.signatures[\"serving_default\"]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Candidate Dataset\n",
    "    # ========================================================================\n",
    "    \n",
    "    # data_bucket_name = 'spotify-beam-v3'\n",
    "    # candidate_items_prefix = 'v3/candidates-jt-tmp/'\n",
    "\n",
    "    candidate_files = []\n",
    "    for blob in storage_client.list_blobs(f'{candidate_file_dir}', prefix=f'{candidate_items_prefix}', delimiter=\"/\"):\n",
    "        if blob.name[-9:] == 'tfrecords':\n",
    "            candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    raw_dataset = tf.data.TFRecordDataset(candidate_files)\n",
    "    parsed_candidate_dataset = raw_dataset.map(parse_candidate_tfrecord_fn)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Convert candidates to embeddings\n",
    "    # ========================================================================\n",
    "    \n",
    "    embs_iter = parsed_candidate_dataset.batch(1).map(\n",
    "        lambda data: candidate_predictor(\n",
    "            artist_name_can = data[\"artist_name_can\"],\n",
    "            track_name_can = data['track_name_can'],\n",
    "            album_name_can = data['album_name_can'],\n",
    "            track_uri_can = data['track_uri_can'],\n",
    "            artist_uri_can = data['artist_uri_can'],\n",
    "            album_uri_can = data['album_uri_can'],\n",
    "            duration_ms_can = data['duration_ms_can'],\n",
    "            track_pop_can = data['track_pop_can'],\n",
    "            artist_pop_can = data['artist_pop_can'],\n",
    "            artist_followers_can = data['artist_followers_can'],\n",
    "            artist_genres_can = data['artist_genres_can']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    embs = []\n",
    "    for emb in embs_iter:\n",
    "        embs.append(emb)\n",
    "\n",
    "    print(f\"Length of embs: {len(embs)}\")\n",
    "    \n",
    "    cleaned_embs = [x['output_1'].numpy()[0] for x in embs] #clean up the output\n",
    "\n",
    "    print(f\"Length of cleaned_embs: {len(cleaned_embs)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # candidate track uri (IDs)\n",
    "    # ========================================================================\n",
    "    \n",
    "    # clean product IDs\n",
    "    track_uris = [x['track_uri_can'].numpy() for x in parsed_candidate_dataset]\n",
    "    track_uris_cleaned = [str(z).replace(\"b'\",\"\").replace(\"'\",\"\") for z in track_uris]\n",
    "    \n",
    "    logging.info(f\"Length of track_uris: {len(track_uris)}\")\n",
    "    logging.info(f\"Length of track_uris_cleaned: {len(track_uris_cleaned)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Remove bad records (e.g., nans)\n",
    "    # ========================================================================\n",
    "    \n",
    "    bad_records = []\n",
    "\n",
    "    for i, emb in enumerate(cleaned_embs):\n",
    "        bool_emb = np.isnan(emb)\n",
    "        for val in bool_emb:\n",
    "            if val:\n",
    "                bad_records.append(i)\n",
    "\n",
    "    bad_record_filter = np.unique(bad_records)\n",
    "\n",
    "    logging.info(f\"bad_records: {len(bad_records)}\")\n",
    "    logging.info(f\"bad_record_filter: {len(bad_record_filter)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # zip good and bad records seperately\n",
    "    # ========================================================================\n",
    "    \n",
    "    track_uris_valid = []\n",
    "    emb_valid = []\n",
    "\n",
    "    bad_record_ids = []\n",
    "    bad_record_embs = []\n",
    "\n",
    "    for i, pair in enumerate(zip(track_uris_cleaned, cleaned_embs)):\n",
    "        if i in bad_record_filter:\n",
    "            t_uri, embed = pair\n",
    "            bad_record_ids.append(t_uri)\n",
    "            bad_record_embs.append(embed)\n",
    "            # pass\n",
    "        else:\n",
    "            t_uri, embed = pair\n",
    "            track_uris_valid.append(t_uri)\n",
    "            emb_valid.append(embed)\n",
    "            \n",
    "    logging.info(f\"Length of emb_valid             : {len(emb_valid)}\")\n",
    "    logging.info(f\"Length of track_uris_valid      : {len(track_uris_valid)}\")\n",
    "    \n",
    "    logging.info(f\"Length of bad_record_ids        : {len(bad_record_ids)}\")\n",
    "    logging.info(f\"Length of bad_record_embs       : {len(bad_record_embs)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Write json file\n",
    "    # ========================================================================\n",
    "\n",
    "    embeddings_index_filename = f'candidate_embeddings_{model_version}_{experiment_run}.json'\n",
    "    \n",
    "    logging.info(f\"Writting embedding vectors to file: {embeddings_index_filename}\")\n",
    "\n",
    "    with open(f'{embeddings_index_filename}', 'w') as f:\n",
    "        for prod, emb in zip(track_uris_valid, emb_valid):\n",
    "            f.write('{\"id\":\"' + str(prod) + '\",')\n",
    "            f.write('\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + \"]}\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "\n",
    "    # DESTINATION_BLOB_NAME = embeddings_index_filename\n",
    "    # SOURCE_FILE_NAME = embeddings_index_filename\n",
    "\n",
    "    # ========================================================================\n",
    "    # Upload vocab_dict to GCS\n",
    "    # ========================================================================\n",
    "    # spotify-tfrs/candidate_embeddings_local_v2_092122.json\n",
    "    # bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    # candidate_tower_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/model-dir/candidate_tower/\"\n",
    "    \n",
    "    EMBEDDING_INDEX_DIR_GCS_URI = f'gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}/candidate-embeddings/corpus-index-dir'\n",
    "    \n",
    "    logging.info(f\"Uploading emebdding vectors to : {EMBEDDING_INDEX_DIR_GCS_URI}/{embeddings_index_filename}\")\n",
    "    blob = Blob.from_string(os.path.join(EMBEDDING_INDEX_DIR_GCS_URI, embeddings_index_filename))\n",
    "    blob.bucket._client = storage_client\n",
    "    blob.upload_from_filename(embeddings_index_filename)\n",
    "    \n",
    "    embedding_index_file_uri = f'{embedding_index_destination_gcs_uri}/{embeddings_index_filename}'\n",
    "    logging.info(f\"Saved embedding vectors for candidate corpus: {embedding_index_file_uri}\")\n",
    "    \n",
    "    # Note: TODO: considerations for storing files used to create index and index updates\n",
    "    \n",
    "    return (\n",
    "        f'{embedding_index_file_uri}',\n",
    "        f'{EMBEDDING_INDEX_DIR_GCS_URI}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c6e9e-94e7-4599-bdef-90c16527b9ad",
   "metadata": {},
   "source": [
    "## Prepare Job Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c3f0d-6057-4fc0-8f7b-3ef53ddf9051",
   "metadata": {},
   "source": [
    "### Vertex Train: workerpool specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeec6383-91f7-4430-a1a4-7aaaf680298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735cb80-e8f6-4587-80e4-369b016438f2",
   "metadata": {},
   "source": [
    "#### Accelerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "307e8f8c-9baa-4c63-b5cd-59aafd8c8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# # Single machine, single GPU\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# # # Single Machine; multiple GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-4g' # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 4\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# # # Multiple Machines, 1 GPU per Machine\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 9\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "691faaa9-0d34-42c2-b3ea-c7fa4e42d7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP: sp\n",
      "MODEL_TYPE: 2tower\n",
      "FRAMEWORK: tfrs\n",
      "MODEL_VERSION: v15\n",
      "PIPELINE_VERSION: v3\n",
      "\n",
      "MODEL_ROOT_NAME: sp-2tower-tfrs-v15-v3\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-v15-v0-training\n",
      "\n",
      "EXPERIMENT_PREFIX: pipe-dev\n",
      "EXPERIMENT_NAME: pipe-dev-2tower-tfrs-v15\n",
      "RUN_NAME: run-20220923-193149\n",
      "\n",
      "DATA_REGIME: small-jt-tfrecord\n",
      "OUTPUT_BUCKET: jt-tfrs-test\n",
      "DOCKERNAME_TRAIN: Dockerfile.tfrs\n",
      "PIPELINE_ROOT_PATH: gs://jt-tfrs-test/sp-2tower-tfrs-v15-v3/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "print(f\"APP: {APP}\")\n",
    "print(f\"MODEL_TYPE: {MODEL_TYPE}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_VERSION: {MODEL_VERSION}\")\n",
    "print(f\"PIPELINE_VERSION: {PIPELINE_VERSION}\\n\")\n",
    "\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\\n\")\n",
    "\n",
    "print(f\"EXPERIMENT_PREFIX: {EXPERIMENT_PREFIX}\")\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\\n\")\n",
    "\n",
    "print(f\"DATA_REGIME: {DATA_REGIME}\")\n",
    "print(f\"OUTPUT_BUCKET: {OUTPUT_BUCKET}\")\n",
    "print(f\"DOCKERNAME_TRAIN: {DOCKERNAME_TRAIN}\")\n",
    "print(f\"PIPELINE_ROOT_PATH: {PIPELINE_ROOT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f00958-21eb-4d33-b750-cf237a860f19",
   "metadata": {},
   "source": [
    "#### Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64e912d6-8e86-4d20-9f8b-e1d7ffa45488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--train_output_gcs_bucket=jt-tfrs-test',\n",
      "                              '--train_dir=spotify-tfrs-dir',\n",
      "                              '--train_dir_prefix=small-dataset/',\n",
      "                              '--valid_dir=spotify-tfrs-dir',\n",
      "                              '--valid_dir_prefix=small-dataset/',\n",
      "                              '--candidate_file_dir=spotify-tfrs-dir',\n",
      "                              '--candidate_files_prefix=small-dataset/',\n",
      "                              '--experiment_name=pipe-dev-2tower-tfrs-v15',\n",
      "                              '--experiment_run=run-20220923-193149',\n",
      "                              '--num_epochs=1',\n",
      "                              '--batch_size=256',\n",
      "                              '--embedding_dim=32',\n",
      "                              '--projection_dim=5',\n",
      "                              '--layer_sizes=[64,32]',\n",
      "                              '--learning_rate=0.01',\n",
      "                              '--valid_frequency=10',\n",
      "                              '--distribute=single',\n",
      "                              '--model_version=v15',\n",
      "                              '--pipeline_version=v3',\n",
      "                              '--data_regime=small-jt-tfrecord'],\n",
      "                     'command': ['python', 'trainer/task.py'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/sp-2tower-tfrs-v15-v0-training'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "# TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# PREFIX = '2tower-pipe'\n",
    "# FRAMEWORK = 'tfrs'\n",
    "# model_VERSION = 'v14'\n",
    "# MODEL_NAME = f'{PREFIX}-{FRAMEWORK}-{model_VERSION}'\n",
    "\n",
    "# =================================================\n",
    "# GCP project\n",
    "# =================================================\n",
    "PROJECT= 'hybrid-vertex'\n",
    "REGION='us-central1'\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "# =================================================\n",
    "# recsys jobs\n",
    "# =================================================\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "MODEL_VERSION = 'v15'\n",
    "PIPELINE_VERSION = 'v3'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}-{PIPELINE_VERSION}'\n",
    "\n",
    "# =================================================\n",
    "# Vertex Experiment tracking\n",
    "# =================================================\n",
    "# timestamp previously calculated; keep everything together\n",
    "EXPERIMENT_PREFIX = 'pipe-dev'                                                 \n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}'\n",
    "RUN_NAME=f'run-{TIMESTAMP}'  \n",
    "DATA_REGIME = 'small-jt-tfrecord'\n",
    "\n",
    "# deprecated\n",
    "# EXPERIMENT_NAME=f'data1-small-{model_VERSION}'\n",
    "# RUN_NAME=f'run-{TIMESTAMP}'\n",
    "\n",
    "# =================================================\n",
    "# Data sources\n",
    "# =================================================\n",
    "# # \"gs://spotify-tfrecords-blog/tfrecords_v1/train/output-00000-of-00796.tfrecord\"\n",
    "# # gs://spotify-tfrs-dir/small-dataset/output-00000-of-00796.tfrecord\n",
    "\n",
    "CANDIDATE_FILE_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "CANDIDATE_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "TRAIN_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "TRAIN_DIR_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "VALID_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "VALID_DIR_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "# MODEL_DIR='jt-tfrs-test'\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# train image\n",
    "# =================================================\n",
    "# Existing image URI or name for image to create\n",
    "IMAGE_URI = 'gcr.io/hybrid-vertex/sp-2tower-tfrs-v15-v0-training'\n",
    "# IMAGE_NAME = f'{MODEL_NAME}-training'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT}/{IMAGE_NAME}'\n",
    "\n",
    "# =================================================\n",
    "# train job config\n",
    "# =================================================\n",
    "VALID_FREQUENCY = 10\n",
    "# VALID_SIZE = 20_000\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# MAX_PADDING = 375\n",
    "EMBEDDING_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "\n",
    "DROPOUT_RATE = 0.4\n",
    "LAYER_SIZES = '[64,32]'\n",
    "\n",
    "WORKER_CMD = [\"python\", \"trainer/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT}',\n",
    "    f'--train_output_gcs_bucket={OUTPUT_BUCKET}',\n",
    "    f'--train_dir={TRAIN_DIR}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={VALID_DIR}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    f'--candidate_file_dir={CANDIDATE_FILE_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={MODEL_VERSION}',\n",
    "    f'--pipeline_version={PIPELINE_VERSION}',\n",
    "    f'--data_regime={DATA_REGIME}',\n",
    "]\n",
    "\n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0075fb81-a2f7-4ca3-a782-ee7e7e5f81ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/spotify-tfrs\n"
     ]
    }
   ],
   "source": [
    "!export PWD=pwd\n",
    "! echo $PWD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf946db3-1c3e-46fc-9e52-493b2a9761da",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e5554d4-2f27-4238-b9fb-819c0e2f8207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: 2tower-recsys-v3\n",
      "PIPELINE_NAME: modeling-v15-2tower-recsys-v3\n"
     ]
    }
   ],
   "source": [
    "# PROJECT_ID = 'hybrid-vertex'\n",
    "# LOCATION = 'us-central1'\n",
    "\n",
    "# model_VERSION = model_VERSION                            # Matching Engine & TwoTwoer Code\n",
    "# PIPELINE_VERSION = VERSION                                  # pipeline code\n",
    "\n",
    "PIPELINE_TAG = f'2tower-recsys-{PIPELINE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "PIPELINE_NAME = f'modeling-{MODEL_VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a32208b6-e678-4043-9bc5-d7bd6ad7c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.pipelines import build_custom_train_image, build_vocabs_stats, create_tensorboard, find_model_endpoint_test, generate_candidate_embedding_index\n",
    "\n",
    "from src.pipelines import build_custom_train_image, build_vocabs_stats, \\\n",
    "                          create_tensorboard, find_model_endpoint_test, \\\n",
    "                          generate_candidate_embedding_index, train_custom_model\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    service_account: str,\n",
    "    # version:str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    pipeline_tag: str,\n",
    "    train_image_uri: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    embedding_index_destination_gcs_uri: str,\n",
    "    gcs_train_script_path: str,\n",
    "    create_tb_resource: bool,\n",
    "    model_display_name: str,\n",
    "    train_dockerfile_name: str,\n",
    "    # data_bucket_name: str,\n",
    "    # path_to_train_dir: str,\n",
    "    # path_to_valid_dir: str,\n",
    "    # path_to_candidate_dir: str,\n",
    "    # candidate_items_prefix: str,\n",
    "    train_dir: str,\n",
    "    train_dir_prefix: str,\n",
    "    valid_dir: str,\n",
    "    valid_dir_prefix: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    path_to_vocab_file: str,\n",
    "    vocab_file_name: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    vpc_network_name: str,\n",
    "    model_endpoint_name: str,\n",
    "    generate_vocabs_stats: bool,\n",
    "    serving_machine_type: str,\n",
    "    serving_replica_count_min: int = 1,\n",
    "    serving_replica_count_max: int = 3,\n",
    "    max_padding: int = 375,\n",
    "    split_names: list = ['train', 'valid'],\n",
    "):\n",
    "    \n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TODO: data preocessing coponents\n",
    "    # ========================================================================\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build Custom Train Image\n",
    "    # ========================================================================\n",
    "    \n",
    "    # build_custom_train_image_op = (\n",
    "    #     build_custom_train_image.build_custom_train_image(\n",
    "    #         project=project,\n",
    "    #         gcs_train_script_path=gcs_train_script_path,\n",
    "    #         training_image_uri=train_image_uri,\n",
    "    #         train_dockerfile_name=train_dockerfile_name,\n",
    "    #     )\n",
    "    #     .set_display_name(\"Build custom train image\")\n",
    "    #     .set_caching_options(False)\n",
    "    # )\n",
    "    \n",
    "    # create_tensorboard_op = (\n",
    "    #     create_tensorboard.create_tensorboard(\n",
    "    #         project=project,\n",
    "    #         location=location,\n",
    "    #         version=version,\n",
    "    #         model_display_name=model_display_name,\n",
    "    #         gcs_bucket_name=train_output_gcs_bucket,\n",
    "    #         create_tb_resource=create_tb_resource\n",
    "    #     )\n",
    "    #     .set_display_name(\"Tensorboard Instance\")\n",
    "    #     .set_caching_options(True)\n",
    "    # )\n",
    "    \n",
    "    build_vocabs_string_lookups_op = (\n",
    "        build_vocabs_stats.build_vocabs_string_lookups(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            # version=version,\n",
    "            # gcs_bucket_name=data_bucket_name,\n",
    "            # path_to_train_dir=path_to_train_dir,\n",
    "            # path_to_valid_dir=path_to_valid_dir,\n",
    "            # path_to_candidate_dir=path_to_candidate_dir,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            train_dir=train_dir,\n",
    "            train_dir_prefix=train_dir_prefix,\n",
    "            valid_dir=valid_dir,\n",
    "            valid_dir_prefix=valid_dir_prefix,\n",
    "            candidate_file_dir=candidate_file_dir,\n",
    "            candidate_files_prefix=candidate_files_prefix,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run, \n",
    "            path_to_vocab_file=path_to_vocab_file,\n",
    "            vocab_file_name=vocab_file_name,\n",
    "            max_padding=max_padding,\n",
    "            split_names=split_names,\n",
    "            generate_vocabs_stats=generate_vocabs_stats,\n",
    "        )\n",
    "        .set_display_name(\"Build Vocab File\")\n",
    "        .set_caching_options(True)\n",
    "        # .after(build_custom_train_image_op)\n",
    "    )\n",
    "    \n",
    "    run_train_task_op = (\n",
    "        train_custom_model.train_custom_model(\n",
    "            project=project,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            # version=version,\n",
    "            model_name=model_display_name,\n",
    "            worker_pool_specs=WORKER_POOL_SPECS, \n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            vocab_dict_uri=build_vocabs_string_lookups_op.outputs['vocab_gcs_uri'],\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            training_image_uri=train_image_uri,     # build_custom_train_image_op.outputs['training_image_uri'],\n",
    "            tensorboard_resource_name=\"projects/934903580331/locations/us-central1/tensorboards/5925030667573264384\", # create_tensorboard_op.outputs['tensorboard_resource_name'],\n",
    "            service_account=service_account,\n",
    "        )\n",
    "        .set_display_name(\"2Tower Training\")\n",
    "        .set_caching_options(True)\n",
    "        # .after(build_custom_train_image_op)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Upload Query and Candidate Towers\n",
    "    # ========================================================================\n",
    "    \n",
    "    import_unmanaged_query_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['query_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-9:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Query Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    query_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'query-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_query_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"query\"},\n",
    "        )\n",
    "        .after(import_unmanaged_query_model_task)\n",
    "        .set_display_name(\"Upload Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    import_unmanaged_candidate_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-9:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Candidate Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    candidate_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'candidate-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_candidate_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"candidate\"},\n",
    "        )\n",
    "        # .after(import_unmanaged_query_model_task)\n",
    "        .set_display_name(\"Upload Query Tower to Vertex\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    find_model_endpoint_op = (\n",
    "      find_model_endpoint_test.find_model_endpoint_test(\n",
    "          project=project,\n",
    "          location=location,\n",
    "          endpoint_name=model_endpoint_name,\n",
    "      )\n",
    "      .set_display_name(\"Find Model Endpoint\")\n",
    "      .set_caching_options(False)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # Scalable ANN Index with Vertex Matching Engine \n",
    "    # ========================================================================\n",
    "\n",
    "    # Generate Embeddings\n",
    "    generate_candidate_embedding_index_op = (\n",
    "        generate_candidate_embedding_index.generate_candidate_embedding_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            # version=version,\n",
    "            # data_bucket_name=data_bucket_name,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            # model_dir=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            model_dir=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            # candidate_items_prefix=candidate_items_prefix,\n",
    "            candidate_file_dir=candidate_file_dir,\n",
    "            candidate_files_prefix=candidate_files_prefix,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            embedding_index_destination_gcs_uri=embedding_index_destination_gcs_uri,\n",
    "            uploaded_candidate_model_resources=candidate_model_upload_op.outputs['gcp_resources'],\n",
    "        )\n",
    "        .after(run_train_task_op)\n",
    "        .set_display_name(\"Generate Candidate embeddings\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Conditional: create New Model Endpoint - True \n",
    "    # ========================================================================\n",
    "    with kfp.v2.dsl.Condition(\n",
    "      find_model_endpoint_op.outputs[\"create_new_endpoint\"] == \"True\",\n",
    "      name=\"Create New Endpoint\",\n",
    "    ):\n",
    "        endpoint_create_op = (\n",
    "            gcc_aip.EndpointCreateOp(\n",
    "                project=project,\n",
    "                display_name=model_endpoint_name, #f'{pipeline_tag}-model-endpoint',\n",
    "            )\n",
    "            .after(run_train_task_op)\n",
    "            .set_display_name(\"Create New Endpoint | Query Tower\")\n",
    "            .set_caching_options(True)\n",
    "        )\n",
    "        \n",
    "        model_deploy_op = (\n",
    "            gcc_aip.ModelDeployOp(\n",
    "                endpoint=endpoint_create_op.outputs['endpoint'],\n",
    "                model=query_model_upload_op.outputs['model'],\n",
    "                deployed_model_display_name=f'deployed-query-tower-{model_display_name}',\n",
    "                dedicated_resources_machine_type=serving_machine_type,\n",
    "                dedicated_resources_min_replica_count=serving_replica_count_min,\n",
    "                dedicated_resources_max_replica_count=serving_replica_count_max,\n",
    "                traffic_split={\"0\": 100},\n",
    "            )\n",
    "            .set_display_name(\"Deploy Query Tower | New Endpoint\")\n",
    "            .set_caching_options(True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "884b8815-dd67-4f29-8149-02056b92ee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/spotify-tfrs\n"
     ]
    }
   ],
   "source": [
    "!export PWD=pwd\n",
    "! echo $PWD\n",
    "\n",
    "pwd='/home/jupyter/spotify-tfrs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74e21a-f4ab-4d13-853a-a8285e167652",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f22de021-3e5e-4d2b-bd8e-3992c53efb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1295: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/spotify-tfrs/src/pipelines/custom_container_pipeline_spec.json...\n",
      "/ [1 files][ 87.9 KiB/ 87.9 KiB]                                                \n",
      "Operation completed over 1 objects/87.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! rm -f pipelines/custom_container_pipeline_spec.json\n",
    "\n",
    "PIPELINE_JSON_SPEC_PATH = \"./src/pipelines/custom_container_pipeline_spec.json\"\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_PATH,\n",
    ")\n",
    "\n",
    "# copy pipeline spec to version location\n",
    "# !gsutil cp /home/jupyter/vertex-examples/tower_pipes/custom_container_pipeline_spec.json {BUCKET_URI}/{APP_NAME}/{VERSION}/\n",
    "!gsutil cp $PWD/src/pipelines/custom_container_pipeline_spec.json {BUCKET_URI}/{MODEL_ROOT_NAME}/src/custom_container_pipeline_spec.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c276a62e-0a5d-478a-a578-c9852992d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run-20220923-193149'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d05070-4fbb-46c7-bfc9-12880faca636",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fda83968-f581-451f-a75c-760d7951f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/modeling-v15-2tower-recsys-v3-20220923193409?project=hybrid-vertex\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# model_VERSION = model_VERSION                         \n",
    "# PIPELINE_VERSION = 'v1'\n",
    "\n",
    "# EXPERIMENT_NAME = f'pipe-test-{PIPELINE_VERSION}'\n",
    "# EXPERIMENT_RUN = f'run-{TIMESTAMP}'\n",
    "# MODEL_DISPLAY_NAME='2Tower-spot'\n",
    "\n",
    "PROJECT_NUMBER='934903580331'\n",
    "\n",
    "# BUCKET_NAME='jt-tfrs-test'\n",
    "OUTPUT_BUCKET = 'jt-tfrs-test'\n",
    "STAGING_BUCKET =f'gs://{OUTPUT_BUCKET}'\n",
    "\n",
    "\n",
    "DATA_BUCKET_NAME= 'spotify-tfrs-dir'      # 'spotify-beam-v3'\n",
    "SPLIT_NAMES = ['train','valid']           # ['dif_artist','dif_artist_valid']\n",
    "\n",
    "# {BASE_OUTPUT_DIR}/src\"\n",
    "TRAIN_APP_CODE_PATH = f'{BASE_OUTPUT_DIR}/src/trainer'\n",
    "\n",
    "TRAIN_IMAGE_URI = 'gcr.io/hybrid-vertex/sp-2tower-tfrs-v15-v0-training'\n",
    "# DOCKERNAME_TRAIN = 'Dockerfile.tfrs'\n",
    "\n",
    "CANDIDATE_FILE_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "CANDIDATE_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "TRAIN_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "TRAIN_DIR_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "VALID_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "VALID_DIR_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "\n",
    "\n",
    "vpc_network_name = 'ucaip-haystack-vpc-network'\n",
    "embedding_index_destination_gcs_uri = f'{BASE_OUTPUT_DIR}/{RUN_NAME}/candidate-index' \n",
    "# index = f\"gs://{output_dir_gcs_bucket_name}/{version}/{experiment_run}/candidate-index/\"\n",
    "\n",
    "# f'{experiment_name}/{experiment_run}/vocabs_stats'\n",
    "PATH_TO_VOCAB_FILE = f'{EXPERIMENT_NAME}/{RUN_NAME}/vocabs_stats'\n",
    "\n",
    "VOCAB_FILENAME = f'vocab_and_stats_{RUN_NAME}.pkl'\n",
    "GENERATE_VOCABS_AND_STATS=True\n",
    "\n",
    "MODEL_ENDPOINT_NAME = f'model-endpoint-{MODEL_VERSION}'\n",
    "CREATE_TENSORBOARD_RESOURCE = ''\n",
    "SERVICE_ACCOUNT = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "\n",
    "# Serving\n",
    "SERVING_MACHINE_TYPE = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-9:latest'\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# Submit Pipeline Job\n",
    "# ========================================================================\n",
    "# # \"gs://spotify-tfrecords-blog/tfrecords_v1/train/output-00000-of-00796.tfrecord\"\n",
    "# # gs://spotify-tfrs-dir/small-dataset/output-00000-of-00796.tfrecord\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "if not PIPELINES.get('train') or overwrite:\n",
    "    response = pipeline_client.create_run_from_job_spec(\n",
    "        job_spec_path=PIPELINE_JSON_SPEC_PATH,\n",
    "        enable_caching=False,\n",
    "        network=f'projects/{PROJECT_NUMBER}/global/networks/{vpc_network_name}',            #\n",
    "        # service_account=SERVICE_ACCOUNT,                                                 \n",
    "        parameter_values={\n",
    "            'project': PROJECT_ID,\n",
    "            'project_number': PROJECT_NUMBER,\n",
    "            'location': LOCATION,\n",
    "            # 'version': VERSION,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'pipeline_version': PIPELINE_VERSION,\n",
    "            'model_display_name': MODEL_ROOT_NAME,\n",
    "            'pipeline_tag': PIPELINE_TAG,\n",
    "            # 'gcs_bucket_name': BUCKET_NAME,\n",
    "            'gcs_train_script_path': TRAIN_APP_CODE_PATH,\n",
    "            'train_image_uri': IMAGE_URI,\n",
    "            'train_output_gcs_bucket': OUTPUT_BUCKET,\n",
    "            # 'data_bucket_name': DATA_BUCKET_NAME,\n",
    "            # 'path_to_train_dir': TRAIN_DIR_PREFIX,\n",
    "            # 'path_to_valid_dir': TRAIN_DIR_PREFIX,\n",
    "            # 'path_to_candidate_dir': CANDIDATE_PREFIX,\n",
    "            # 'candidate_items_prefix': CANDIDATE_PREFIX,\n",
    "            'train_dir': TRAIN_DIR,\n",
    "            'train_dir_prefix': TRAIN_DIR_PREFIX,\n",
    "            'valid_dir': VALID_DIR,\n",
    "            'valid_dir_prefix': VALID_DIR_PREFIX,\n",
    "            'candidate_file_dir': CANDIDATE_FILE_DIR,\n",
    "            'candidate_files_prefix': CANDIDATE_PREFIX,\n",
    "            'vpc_network_name': vpc_network_name,\n",
    "            'train_dockerfile_name': DOCKERNAME_TRAIN,\n",
    "            'path_to_vocab_file': PATH_TO_VOCAB_FILE,\n",
    "            'vocab_file_name':VOCAB_FILENAME,\n",
    "            'embedding_index_destination_gcs_uri': embedding_index_destination_gcs_uri,\n",
    "            'generate_vocabs_stats': GENERATE_VOCABS_AND_STATS,\n",
    "            'experiment_name': EXPERIMENT_NAME,\n",
    "            'experiment_run': RUN_NAME,\n",
    "            'serving_machine_type': SERVING_MACHINE_TYPE,\n",
    "            'model_endpoint_name': MODEL_ENDPOINT_NAME,\n",
    "            'create_tb_resource': CREATE_TENSORBOARD_RESOURCE,\n",
    "            'service_account': SERVICE_ACCOUNT,\n",
    "            # 'max_padding': 375,\n",
    "            'split_names': SPLIT_NAMES,\n",
    "            'generate_vocabs_stats': GENERATE_VOCABS_AND_STATS,\n",
    "        },\n",
    "        pipeline_root=f'{PIPELINE_ROOT_PATH}/{PIPELINE_VERSION}',\n",
    "    )\n",
    "    PIPELINES['train'] = response['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888c6e5-8d07-42d4-aae9-2e79c19ad416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eab951-93a5-46a0-a2b4-7ab0b8be48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be using these pipeline params \n",
    "\n",
    "            # 'dimensions': dimensions,\n",
    "            # 'ann_index_display_name': ann_index_display_name,\n",
    "            # 'approximate_neighbors_count': approximate_neighbors_count,\n",
    "            # 'distance_measure_type': distance_measure_type,\n",
    "            # 'leaf_node_embedding_count': leaf_node_embedding_count,\n",
    "            # 'leaf_nodes_to_search_percent': leaf_nodes_to_search_percent, \n",
    "            # 'ann_index_description': ann_index_description,\n",
    "            # 'ann_index_labels': ann_index_labels,\n",
    "            # 'brute_force_index_display_name': brute_force_index_display_name,\n",
    "            # 'brute_force_index_description': brute_force_index_description,\n",
    "            # 'brute_force_index_labels': brute_force_index_labels,\n",
    "            # 'deployed_ann_index_name': deployed_ann_index_name,\n",
    "            # 'deployed_brute_force_index_name': deployed_brute_force_index_name,\n",
    "            # 'deployed_test_destination_gcs_uri': deployed_test_destination_gcs_uri,\n",
    "            # \"endpoint_resource_uri\": \"https://us-central1-aiplatform.googleapis.com/v1/\",\n",
    "            # \"serving_image\": \"N/A TODO\",\n",
    "            # \"artifact_uri\": \"N/A TODO\",\n",
    "            # \"resource_name\":\"TODO NONE\",\n",
    "            # 'ann_index_endpoint_display_name': ann_index_endpoint_display_name,\n",
    "            # 'ann_index_endpoint_description': ann_index_endpoint_description,\n",
    "            # 'brute_index_endpoint_display_name': brute_index_endpoint_display_name,\n",
    "            # 'brute_index_endpoint_description': brute_index_endpoint_description,\n",
    "            # 'app_name': APP_NAME,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0130c-6e6b-4d6e-b7a5-6c2265e76371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
