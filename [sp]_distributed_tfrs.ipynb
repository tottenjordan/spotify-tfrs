{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE-W6E3Cy66L"
   },
   "source": [
    "# Distributed Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HtMh17BUy6Pc"
   },
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_recommenders as tfrs\n",
    "# import tensorflow_io as tfio\n",
    "\n",
    "# from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "HrKzx8b5zDHV"
   },
   "outputs": [],
   "source": [
    "PREFIX = 'spotify-2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "VERSION = 'v11'\n",
    "MODEL_NAME = f'{PREFIX}-{FRAMEWORK}-{VERSION}'\n",
    "\n",
    "PROJECT= 'hybrid-vertex'\n",
    "REGION='us-central1'\n",
    "BUCKET_NAME='spotify-tfrecords-blog'\n",
    "STAGING_BUCKET =f'gs://{BUCKET_NAME}'\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_NAME = f'{MODEL_NAME}-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = 'tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "kj-KiRcvzfuk"
   },
   "outputs": [],
   "source": [
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_DISPLAY_NAME = f\"{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/934903580331/locations/us-central1/tensorboards/1347121646351155200/operations/3953758395172913152\n",
      "Tensorboard created. Resource name: projects/934903580331/locations/us-central1/tensorboards/1347121646351155200\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/934903580331/locations/us-central1/tensorboards/1347121646351155200')\n",
      "TensorBoard resource name: projects/934903580331/locations/us-central1/tensorboards/1347121646351155200\n"
     ]
    }
   ],
   "source": [
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "\n",
    "tensorboard_resource_name = tensorboard.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD = 'projects/934903580331/locations/us-central1/tensorboards/4842196432167370752'\n",
    "# TENSORBOARD = 'projects/934903580331/locations/us-central1/tensorboards/5764308455871479808'\n",
    "\n",
    "TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/1347121646351155200\"\n",
    "\n",
    "# tb = aiplatform.Tensorboard('projects/934903580331/locations/us-central1/tensorboards/2710867908514283520')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZbeq5FC0NBf"
   },
   "source": [
    "## Perepare Vertex Training Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create repo for training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/spotify-tfrs\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "QLUTCt34zx3e"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "\n",
    "# Make folder for Python training script\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}\n",
    "\n",
    "# Add package information\n",
    "! touch {REPO_DOCKER_PATH_PREFIX}/README.md\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "! touch {REPO_DOCKER_PATH_PREFIX}/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interactive training shell in Vertex AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/interactive_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gcloud compute images list \\\n",
    "        --project deeplearning-platform-release \\\n",
    "        --no-standard-images\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "gcloud compute images describe-from-family IMAGE_FAMILY \\\n",
    "        --project deeplearning-platform-release\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/Dockerfile.tfrs\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM tensorflow/tensorflow:2.9.2-gpu\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer/* trainer/ \n",
    "\n",
    "RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93dxYpL00XqU"
   },
   "source": [
    "### requirements.txt\n",
    "\n",
    "* TODO: for profiling, install `google-cloud-aiplatform[cloud_profiler]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "L3Xvv9Nc0YCw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/requirements.txt\n",
    "\n",
    "google-cloud-aiplatform==1.17.0\n",
    "tensorflow-recommenders==0.7.0\n",
    "tensorboard==2.9.1\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-profile==2.5.0\n",
    "cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google-cloud-aiplatform==1.17.0\n",
    "# tensorflow==2.9.2\n",
    "# tensorflow-cloud==0.1.16\n",
    "# tensorflow-datasets==4.4.0\n",
    "# tensorflow-estimator==2.9.0\n",
    "# tensorflow-hub==0.12.0\n",
    "# tensorflow-io==0.23.1\n",
    "# tensorflow-io-gcs-filesystem==0.27.0\n",
    "# tensorflow-metadata==1.10.0\n",
    "# tensorflow-recommenders==0.7.0\n",
    "# tensorflow-serving-api==2.10.0\n",
    "# tensorflow-transform==1.10.1\n",
    "# tensorboard==2.9.1\n",
    "# tensorboard-data-server==0.6.1\n",
    "# tensorboard-plugin-profile==2.5.0\n",
    "# cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/_data.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import train_config as config\n",
    "\n",
    "MAX_PLAYLIST_LENGTH = config.MAX_PADDING # 375\n",
    "\n",
    "def pad_up_to(t, max_in_dims=[1 ,MAX_PLAYLIST_LENGTH], constant_value=''):\n",
    "    s = tf.shape(t)\n",
    "    paddings = [[0, m-s[i]] for (i,m) in enumerate(max_in_dims)]\n",
    "    return tf.pad(t, paddings, 'CONSTANT', constant_values=constant_value)\n",
    "\n",
    "def return_padded_tensors(data):\n",
    "    \n",
    "    a = pad_up_to(tf.reshape(data['track_name_pl'], shape=(1,-1)) , constant_value='')\n",
    "    b = pad_up_to(tf.reshape(data['artist_name_pl'], shape=(1,-1)) , constant_value='')\n",
    "    c = pad_up_to(tf.reshape(data['album_name_pl'], shape=(1,-1)) , constant_value='')\n",
    "    d = pad_up_to(tf.reshape(data['track_uri_pl'], shape=(1, -1,)) , constant_value='')\n",
    "    e = pad_up_to(tf.reshape(data['duration_ms_songs_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "    f = pad_up_to(tf.reshape(data['artist_pop_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "    g = pad_up_to(tf.reshape(data['artists_followers_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "    h = pad_up_to(tf.reshape(data['track_pop_pl'], shape=(1,-1)) , constant_value=-1.)\n",
    "    i = pad_up_to(tf.reshape(data['artist_genres_pl'], shape=(1,-1)) , constant_value='')\n",
    "        \n",
    "    padded_data = data.copy()\n",
    "    padded_data['track_name_pl'] = a\n",
    "    padded_data['artist_name_pl'] = b\n",
    "    padded_data['album_name_pl'] = c\n",
    "    padded_data['track_uri_pl'] = d\n",
    "    padded_data['duration_ms_songs_pl'] = e\n",
    "    padded_data['artist_pop_pl'] = f\n",
    "    padded_data['artists_followers_pl'] = g\n",
    "    padded_data['track_pop_pl'] = h\n",
    "    padded_data['artist_genres_pl'] = i\n",
    "        \n",
    "    return padded_data\n",
    "\n",
    "all_features = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'pos_seed_track': tf.io.FixedLenFeature(dtype=tf.int64, shape=()),\n",
    "    'track_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'pid': tf.io.FixedLenFeature(dtype=tf.int64, shape=()),\n",
    "    'name': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'collaborative': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_seed_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'n_songs_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_artists_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_albums_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'description_pl': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    ###ragged\n",
    "    'track_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'artist_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'album_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'track_uri_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'duration_ms_songs_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artists_followers_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'track_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_genres_pl': tf.io.RaggedFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse_tfrecord_fn(example, feature_dict=all_features): # =all_features\n",
    "    example = tf.io.parse_single_example(\n",
    "        example, \n",
    "        features=feature_dict\n",
    "    )\n",
    "    return example\n",
    "\n",
    "candidate_features = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "}\n",
    "\n",
    "def parse_candidate_tfrecord_fn(example, feature_dict=candidate_features):\n",
    "    example = tf.io.parse_single_example(\n",
    "        example, \n",
    "        features=feature_dict\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/_model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "\n",
    "import train_config as config\n",
    "# ====================================================\n",
    "# Playlist (query) Tower\n",
    "# ====================================================\n",
    "\n",
    "# TODO: parameterize\n",
    "\n",
    "EMBEDDING_DIM = config.EMBEDDING_DIM       # 32\n",
    "PROJECTION_DIM = config.PROJECTION_DIM     # 5\n",
    "SEED = config.SEED                         # 1234\n",
    "USE_CROSS_LAYER = config.USE_CROSS_LAYER   # True\n",
    "DROPOUT = config.USE_DROPOUT               # 'False'\n",
    "DROPOUT_RATE = config.DROPOUT_RATE         # '0.33'\n",
    "\n",
    "class Playlist_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # ========================================\n",
    "        # non-sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: playlist name\n",
    "        self.pl_name_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"name\"]), # not needed if passing vocab\n",
    "                    # vocabulary=vocab_dict['name'], \n",
    "                    name=\"pl_name_txt_vectorizer\", \n",
    "                    ngrams=2\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"name\"]) + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_name_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"pl_name_pooling\"),\n",
    "            ], name=\"pl_name_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: collaborative\n",
    "        collaborative_vocab = np.array([b'false', b'true'])\n",
    "        \n",
    "        self.pl_collaborative_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=collaborative_vocab, \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_collaborative_lookup\", \n",
    "                    output_mode='int'\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(collaborative_vocab) + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_collaborative_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_collaborative_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: pid\n",
    "        self.pl_pid_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.IntegerLookup(\n",
    "                    vocabulary=tf.constant(vocab_dict['unique_pids']), \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_pid_lookup\", \n",
    "                    output_mode='int'\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['unique_pids']),\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_pid_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_pid_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: description_pl\n",
    "        # self.pl_description_text_embedding = tf.keras.Sequential(\n",
    "        #     [\n",
    "        #         tf.keras.layers.TextVectorization(\n",
    "        #             max_tokens=len(vocab_dict[\"description_pl\"]), # not needed if passing vocab\n",
    "        #             # vocabulary=tf.constant(vocab_dict['description_pl']), \n",
    "        #             name=\"description_pl_vectorizer\", \n",
    "        #             ngrams=2,\n",
    "        #         ),\n",
    "        #         tf.keras.layers.Embedding(\n",
    "        #             input_dim=len(vocab_dict[\"description_pl\"]) + 1,\n",
    "        #             output_dim=EMBEDDING_DIM,\n",
    "        #             mask_zero=False,\n",
    "        #             name=\"description_pl_emb_layer\",\n",
    "        #         ),\n",
    "        #         tf.keras.layers.GlobalAveragePooling1D(name=\"description_pl_pooling\"),\n",
    "        #     ], name=\"pl_description_emb_model\"\n",
    "        # )\n",
    "        \n",
    "        # Feature: duration_ms_seed_pl                      \n",
    "        # TODO: Noramlize or Descritize?\n",
    "        duration_ms_seed_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_duration_ms_seed_pl'], \n",
    "            vocab_dict['max_duration_ms_seed_pl'], \n",
    "            num=1000\n",
    "        )\n",
    "        self.duration_ms_seed_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(duration_ms_seed_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(duration_ms_seed_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"duration_ms_seed_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"duration_ms_seed_pl_emb_model\"\n",
    "        )\n",
    "        # self.duration_ms_seed_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_duration_ms_seed_pl'],\n",
    "        #     variance=vocab_dict['var_duration_ms_seed_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: n_songs_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_songs_pl'], \n",
    "            vocab_dict['max_n_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_songs_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_songs_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_songs_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_n_songs_pl'],\n",
    "        #     variance=vocab_dict['var_n_songs_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: num_artists_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_artists_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_artists_pl'], \n",
    "            vocab_dict['max_n_artists_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_artists_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_artists_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_artists_pl_buckets) + 2, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_artists_pl_emb_layer\",\n",
    "                    mask_zero=True\n",
    "                )\n",
    "            ], name=\"n_artists_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_artists_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_n_artists_pl'],\n",
    "        #     variance=vocab_dict['var_n_artists_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: num_albums_pl\n",
    "        n_albums_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_albums_pl'], \n",
    "            vocab_dict['max_n_albums_pl'],\n",
    "            num=100\n",
    "        )\n",
    "        self.n_albums_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_albums_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_albums_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_albums_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_albums_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_albums_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_n_albums_pl'],\n",
    "        #     variance=vocab_dict['var_n_albums_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # ========================================\n",
    "        # sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_pl\n",
    "        self.artist_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['artist_name_pl'], mask_token=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_name_pl']) + 2, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"artist_name_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"artist_name_gru\"),\n",
    "            ], name=\"artist_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_pl\n",
    "        # 2.2M unique\n",
    "        self.track_uri_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_pl']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "             # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"track_uri_conv1d\"),\n",
    "             tf.keras.layers.GRU(EMBEDDING_DIM, name=\"track_uri_gru\"),\n",
    "            ], name=\"track_uri_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_pl\n",
    "        self.track_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['track_name_pl'], \n",
    "                    name=\"track_name_pl_lookup\",\n",
    "                    output_mode='int',\n",
    "                    mask_token=''\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_name_pl']) + 2, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"track_name_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"track_name_gru\"),\n",
    "            ], name=\"track_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        Feature: duration_ms_songs_pl\n",
    "        duration_ms_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_duration_ms_songs_pl'], \n",
    "            vocab_dict['max_duration_ms_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.duration_ms_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(duration_ms_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(duration_ms_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"duration_ms_songs_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"duration_ms_songs_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"duration_ms_songs_gru\"),\n",
    "            ], name=\"duration_ms_songs_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_pl\n",
    "        self.album_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=tf.constant(vocab_dict['album_name_pl']), mask_token='', name=\"album_name_pl_lookup\"),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['album_name_pl']) + 2, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"album_name_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"album_name_gru\"),\n",
    "            ], name=\"album_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_pl\n",
    "        artist_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_pop'], \n",
    "            vocab_dict['max_artist_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artist_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artist_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artist_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"artist_pop_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"artist_pop_gru\"),\n",
    "            ], name=\"artist_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artists_followers_pl\n",
    "        artists_followers_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_followers'], \n",
    "            vocab_dict['max_artist_followers'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artists_followers_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artists_followers_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artists_followers_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artists_followers_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"artist_followers_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"artist_followers_gru\"),\n",
    "            ], name=\"artists_followers_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_pl\n",
    "        track_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_track_pop'], \n",
    "            vocab_dict['max_track_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.track_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(dtype=tf.float32),\n",
    "                tf.keras.layers.Discretization(track_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(track_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"track_pop_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"track_pop_gru\"),\n",
    "            ], name=\"track_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_pl\n",
    "        self.artist_genres_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['artist_genres_pl'], mask_token=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_genres_pl'])+2, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_genres_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"artist_genres_conv1d\"),\n",
    "                tf.keras.layers.GRU(EMBEDDING_DIM, name=\"artist_genres_gru\"),\n",
    "            ], name=\"artist_genres_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # dense and cross layers\n",
    "        # ========================================\n",
    "\n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"pl_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "            \n",
    "        # Dense Layers\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"pl_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: tf.nn.l2_normalize(\n",
    "                    x, 1, epsilon=1e-12, name=\"normalize_dense\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # ========================================\n",
    "    # call\n",
    "    # ========================================\n",
    "    def call(self, data):\n",
    "        '''\n",
    "        The call method defines what happens when\n",
    "        the model is called\n",
    "        '''\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.pl_name_text_embedding(data['name']),\n",
    "                self.pl_collaborative_embedding(data['collaborative']),\n",
    "                self.pl_pid_embedding(data[\"pid\"]),\n",
    "                # self.pl_description_text_embedding(data['description_pl']),\n",
    "                self.duration_ms_seed_pl_embedding(data[\"duration_ms_seed_pl\"]),\n",
    "                # tf.reshape(self.duration_ms_seed_pl_normalization(data[\"duration_ms_seed_pl\"]), (-1, 1))      # Normalize or Discretize?\n",
    "                self.n_songs_pl_embedding(data[\"n_songs_pl\"]),\n",
    "                # tf.reshape(self.n_songs_pl_normalization(data[\"n_songs_pl\"]), (-1, 1))                        # Normalize or Discretize?\n",
    "                self.n_artists_pl_embedding(data['num_artists_pl']),\n",
    "                # tf.reshape(self.n_artists_pl_normalization(data[\"num_artists_pl\"]), (-1, 1))                  # Normalize or Discretize?\n",
    "                self.n_albums_pl_embedding(data[\"num_albums_pl\"]),\n",
    "                # tf.reshape(self.n_albums_pl_normalization(data[\"num_albums_pl\"]), (-1, 1))                    # Normalize or Discretize?\n",
    "                \n",
    "                # sequence features\n",
    "                # data[\"pos_pl\"],\n",
    "                self.artist_name_pl_embedding(data[\"artist_name_pl\"]),\n",
    "                self.track_uri_pl_embedding(data[\"track_uri_pl\"]),\n",
    "                self.track_name_pl_embedding(data[\"track_name_pl\"]),\n",
    "                self.duration_ms_songs_pl_embedding(data[\"duration_ms_songs_pl\"]),\n",
    "                self.album_name_pl_embedding(data[\"album_name_pl\"]),\n",
    "                self.artist_pop_pl_embedding(data[\"artist_pop_pl\"]),\n",
    "                self.artists_followers_pl_embedding(data[\"artists_followers_pl\"]),\n",
    "                self.track_pop_pl_embedding(data[\"track_pop_pl\"]),\n",
    "                self.artist_genres_pl_embedding(data[\"artist_genres_pl\"]),\n",
    "            ], axis=1)\n",
    "        \n",
    "        # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)\n",
    "\n",
    "# ====================================================\n",
    "# Track (candidate) Tower\n",
    "# ====================================================\n",
    "class Candidate_Track_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # Candidate features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_can\n",
    "        self.artist_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"artist_name_can\"],\n",
    "                    name=\"artist_name_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"artist_name_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_name_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_can_pooling\"),\n",
    "            ], name=\"artist_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_can\n",
    "        self.track_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"track_name_can\"],\n",
    "                    name=\"track_name_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"track_name_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"track_name_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_can_pooling\"),\n",
    "            ], name=\"track_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_can\n",
    "        self.album_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"album_name_can\"],\n",
    "                    name=\"album_name_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"album_name_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"album_name_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_can_pooling\"),\n",
    "            ], name=\"album_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_uri_can\n",
    "        self.artist_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_uri_can'])+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"artist_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_can\n",
    "        self.track_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can'])+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"track_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_uri_can\n",
    "        self.album_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['album_uri_can'])+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"album_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: duration_ms_can\n",
    "        self.duration_ms_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_duration_ms_songs_pl'],\n",
    "            variance=vocab_dict['var_duration_ms_songs_pl'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_can\n",
    "        self.track_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_track_pop'],\n",
    "            variance=vocab_dict['var_track_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_can\n",
    "        self.artist_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_pop'],\n",
    "            variance=vocab_dict['var_artist_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_followers_can\n",
    "        self.artist_followers_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_followers'],\n",
    "            variance=vocab_dict['var_artist_followers'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_can\n",
    "        self.artist_genres_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"artist_genres_can\"],\n",
    "                    name=\"artist_genres_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"artist_genres_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_genres_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_can_pooling\"),\n",
    "            ], name=\"artist_genres_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # Dense & Cross Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"can_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "        \n",
    "        # Dense Layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"candidate_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    # ========================================\n",
    "    # Call Function\n",
    "    # ========================================\n",
    "            \n",
    "    def call(self, data):\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.artist_name_can_text_embedding(data['artist_name_can']),\n",
    "                self.track_name_can_text_embedding(data['track_name_can']),\n",
    "                self.album_name_can_text_embedding(data['album_name_can']),\n",
    "                self.artist_uri_can_embedding(data['artist_uri_can']),\n",
    "                self.track_uri_can_embedding(data['track_uri_can']),\n",
    "                self.album_uri_can_embedding(data['album_uri_can']),\n",
    "                tf.reshape(self.duration_ms_can_normalized(data[\"duration_ms_can\"]), (-1, 1)),\n",
    "                tf.reshape(self.track_pop_can_normalized(data[\"track_pop_can\"]), (-1, 1)),\n",
    "                tf.reshape(self.artist_pop_can_normalized(data[\"artist_pop_can\"]), (-1, 1)),\n",
    "                tf.reshape(self.artist_followers_can_normalized(data[\"artist_followers_can\"]), (-1, 1)),\n",
    "                self.artist_genres_can_text_embedding(data['album_uri_can']),\n",
    "            ], axis=1\n",
    "        )\n",
    "        \n",
    "        # return self.dense_layers(all_embs)\n",
    "                # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)\n",
    "\n",
    "# ====================================================\n",
    "# Combined 2Tower\n",
    "# ====================================================\n",
    "class TheTwoTowers(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes, vocab_dict_load, parsed_candidate_dataset):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_tower = Playlist_Model(layer_sizes, vocab_dict_load)\n",
    "        \n",
    "        self.candidate_tower = Candidate_Track_Model(layer_sizes, vocab_dict_load)\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=parsed_candidate_dataset.batch(128).cache().map(self.candidate_tower) # TODO: parameterize\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_tower(data)\n",
    "        candidate_embeddings = self.candidate_tower(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings, \n",
    "            candidate_embeddings, \n",
    "            compute_metrics=not training\n",
    "        ) # turn off metrics to save time on training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/task.py\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle as pkl\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "\n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "# ====================================================\n",
    "# Main\n",
    "# ====================================================\n",
    "import _data\n",
    "import _model\n",
    "import train_config as config\n",
    "import time \n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    logging.info(\"Starting training...\")\n",
    "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    \n",
    "    storage_client = storage.Client(\n",
    "        project=args.project\n",
    "    )\n",
    "    \n",
    "    WORKING_DIR = \n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    \n",
    "    # AIP_TB_LOGS = args.aip_tb_logs # os.environ.get('AIP_TENSORBOARD_LOG_DIR', 'NA')\n",
    "    # logging.info(f'AIP TENSORBOARD LOG DIR: {AIP_TB_LOGS}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU/TPU Strategy\n",
    "    # ====================================================\n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.config.list_physical_devices('GPU'): # TODO: replace with - tf.config.list_physical_devices('GPU') | tf.test.is_gpu_available()\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    \n",
    "    # Single Machine, multiple TPU devices\n",
    "    elif args.distribute == 'tpu':\n",
    "        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "        logging.info(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "    \n",
    "    logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    \n",
    "    # ====================================================\n",
    "    # Vocab Files\n",
    "    # ====================================================\n",
    "\n",
    "    # TODO: parameterize & configure for adapts vs vocab files\n",
    "\n",
    "    BUCKET_NAME = 'spotify-v1'\n",
    "    FILE_PATH = 'vocabs/v1_string_vocabs'\n",
    "    FILE_NAME = 'string_vocabs_v1_20220705-202905.txt'\n",
    "    DESTINATION_FILE = 'downloaded_vocabs.txt'\n",
    "\n",
    "    with open(f'{DESTINATION_FILE}', 'wb') as file_obj:\n",
    "        storage_client.download_blob_to_file(\n",
    "            f'gs://{BUCKET_NAME}/{FILE_PATH}/{FILE_NAME}', file_obj)\n",
    "\n",
    "\n",
    "    with open(f'{DESTINATION_FILE}', 'rb') as pickle_file:\n",
    "        vocab_dict_load = pkl.load(pickle_file)\n",
    "\n",
    "\n",
    "    # TODO: include as a preprocessing step \n",
    "    avg_duration_ms_seed_pl = 13000151.68\n",
    "    var_duration_ms_seed_pl = 133092900971233.58\n",
    "    vocab_dict_load['avg_duration_ms_seed_pl']=avg_duration_ms_seed_pl\n",
    "    vocab_dict_load['var_duration_ms_seed_pl']=var_duration_ms_seed_pl\n",
    "\n",
    "    avg_n_songs_pl = 55.21\n",
    "    var_n_songs_pl = 2317.54\n",
    "    vocab_dict_load['avg_n_songs_pl']=avg_n_songs_pl\n",
    "    vocab_dict_load['var_n_songs_pl']=var_n_songs_pl\n",
    "\n",
    "    avg_n_artists_pl = 30.56\n",
    "    var_n_artists_pl = 769.26\n",
    "    vocab_dict_load['avg_n_artists_pl']=avg_n_artists_pl\n",
    "    vocab_dict_load['var_n_artists_pl']=var_n_artists_pl\n",
    "\n",
    "    avg_n_albums_pl = 40.25\n",
    "    var_n_albums_pl = 1305.54\n",
    "    vocab_dict_load['avg_n_albums_pl']=avg_n_albums_pl\n",
    "    vocab_dict_load['var_n_albums_pl']=var_n_albums_pl\n",
    "\n",
    "    avg_artist_pop = 16.08\n",
    "    var_artist_pop = 300.64\n",
    "    vocab_dict_load['avg_artist_pop']=avg_artist_pop\n",
    "    vocab_dict_load['var_artist_pop']=var_artist_pop\n",
    "\n",
    "    avg_duration_ms_songs_pl = 234823.14\n",
    "    var_duration_ms_songs_pl = 5558806228.41\n",
    "    vocab_dict_load['avg_duration_ms_songs_pl']=avg_duration_ms_songs_pl\n",
    "    vocab_dict_load['var_duration_ms_songs_pl']=var_duration_ms_songs_pl\n",
    "\n",
    "    avg_artist_followers = 43337.77\n",
    "    var_artist_followers = 377777790193.57\n",
    "    vocab_dict_load['avg_artist_followers']=avg_artist_followers\n",
    "    vocab_dict_load['var_artist_followers']=var_artist_followers\n",
    "\n",
    "    avg_track_pop = 10.85\n",
    "    var_track_pop = 202.18\n",
    "    vocab_dict_load['avg_track_pop']=avg_track_pop\n",
    "    vocab_dict_load['var_track_pop']=var_track_pop\n",
    "\n",
    "    # ====================================================\n",
    "    # Parse & Pad train dataset\n",
    "    # ====================================================\n",
    "\n",
    "    logging.info(f'args.train_dir: {args.train_dir}')\n",
    "    logging.info(f'args.train_dir_prefix: {args.train_dir_prefix}')\n",
    "    \n",
    "    train_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.train_dir}', prefix=f'{args.train_dir_prefix}', delimiter=\"/\"):\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    # Parse train dataset\n",
    "    raw_train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "    parsed_dataset = raw_train_dataset.map(_data.parse_tfrecord_fn)\n",
    "    parsed_dataset_padded = parsed_dataset.map(_data.return_padded_tensors)  \n",
    "    \n",
    "    # ====================================================\n",
    "    # Parse candidates dataset\n",
    "    # ====================================================\n",
    "\n",
    "    logging.info(f'args.candidate_file_dir: {args.candidate_file_dir}')\n",
    "    logging.info(f'args.candidate_files_prefix: {args.candidate_files_prefix}')\n",
    "\n",
    "    candidate_files = []\n",
    "    for blob in storage_client.list_blobs(f'{args.candidate_file_dir}', prefix=f'{args.candidate_files_prefix}', delimiter=\"/\"):\n",
    "        candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "    raw_candidate_dataset = tf.data.TFRecordDataset(candidate_files)\n",
    "    parsed_candidate_dataset = raw_candidate_dataset.map(_data.parse_candidate_tfrecord_fn)\n",
    "    \n",
    "    # ====================================================\n",
    "    # Prepare Train and Valid Data\n",
    "    # ====================================================\n",
    "    logging.info(f'preparing train and valid splits...')\n",
    "    tf.random.set_seed(42)\n",
    "    shuffled_parsed_ds = parsed_dataset_padded.shuffle(10_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "    # train_data = shuffled_parsed_ds.take(80_000).batch(128)\n",
    "    # valid_data = shuffled_parsed_ds.skip(80_000).take(20_000).batch(128)\n",
    "    \n",
    "    valid_size = 20_000 # config.VALID_SIZE # 20_000 # args.valid_size\n",
    "    valid = shuffled_parsed_ds.take(valid_size)\n",
    "    train = shuffled_parsed_ds.skip(valid_size)\n",
    "    cached_train = train.batch(args.batch_size * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "    cached_valid = valid.batch(args.batch_size * strategy.num_replicas_in_sync).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging metaparams & hyperparams for Vertex Experiments')\n",
    "    \n",
    "    EXPERIMENT_NAME = f\"{args.experiment_name}\"\n",
    "    RUN_NAME = f\"{args.experiment_run}\"\n",
    "    logging.info(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\\n RUN_NAME: {RUN_NAME}\")\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{EXPERIMENT_NAME}'\n",
    "    metaparams[\"experiment_run\"] = f\"{RUN_NAME}\"\n",
    "    metaparams[\"distribute\"] = f'{args.distribute}'\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"batch_size\"] = int(args.batch_size)\n",
    "    hyperparams[\"embedding_dim\"] = args.embedding_dim\n",
    "    hyperparams[\"projection_dim\"] = args.projection_dim\n",
    "    hyperparams[\"use_cross_layer\"] = config.USE_CROSS_LAYER\n",
    "    hyperparams[\"use_dropout\"] = config.USE_DROPOUT\n",
    "    hyperparams[\"dropout_rate\"] = args.dropout_rate\n",
    "    hyperparams['layer_sizes'] = args.layer_sizes\n",
    "    \n",
    "    logging.info(f\"Creating run: {RUN_NAME}; for experiment: {EXPERIMENT_NAME}\")\n",
    "    \n",
    "    # Create experiment\n",
    "    vertex_ai.init(experiment=EXPERIMENT_NAME)\n",
    "    # vertex_ai.start_run(RUN_NAME,resume=True) # RUN_NAME\n",
    "    \n",
    "    with vertex_ai.start_run(RUN_NAME) as my_run:\n",
    "        logging.info(f\"logging metaparams\")\n",
    "        my_run.log_params(metaparams)\n",
    "        \n",
    "        logging.info(f\"logging hyperparams\")\n",
    "        my_run.log_params(hyperparams)\n",
    "        \n",
    "    # ====================================================\n",
    "    # Compile, Adapt, and Train model\n",
    "    # ====================================================\n",
    "    logging.info('Setting model adapts and compiling the model')\n",
    "    \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "    \n",
    "    logging.info(f'adapting layers: {config.NEW_ADAPTS}')\n",
    "    # Wrap variable creation within strategy scope\n",
    "    with strategy.scope():\n",
    "\n",
    "        model = _model.TheTwoTowers(LAYER_SIZES, vocab_dict_load, parsed_candidate_dataset)\n",
    "        \n",
    "        if config.NEW_ADAPTS:\n",
    "            model.query_tower.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(args.batch_size)) # TODO: adapts on full dataset or train only?\n",
    "            # vocab_dict_load['name'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "            \n",
    "#             model.candidate_tower.artist_name_can_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['artist_name_can']).batch(args.batch_size))\n",
    "#             # vocab_dict_load['artist_name_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "            \n",
    "#             model.candidate_tower.track_name_can_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['track_name_can']).batch(args.batch_size))\n",
    "#             # vocab_dict_load['track_name_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "            \n",
    "#             model.candidate_tower.album_name_can_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['album_name_can']).batch(args.batch_size))\n",
    "#             # vocab_dict_load['album_name_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "            \n",
    "#             model.candidate_tower.artist_genres_can_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['artist_genres_can']).batch(args.batch_size))\n",
    "#             # vocab_dict_load['artist_genres_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "            \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adagrad(args.learning_rate))\n",
    "        \n",
    "    if config.NEW_ADAPTS:\n",
    "        vocab_dict_load['name'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "    #     vocab_dict_load['artist_name_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "    #     vocab_dict_load['track_name_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "    #     vocab_dict_load['album_name_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "    #     vocab_dict_load['artist_genres_can'] = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "        bucket = storage_client.bucket(args.model_dir)\n",
    "        blob = bucket.blob(f'vocabs_stats/vocab_dict_{args.version}.txt')\n",
    "        pickle_out = pkl.dumps(vocab_dict_load)\n",
    "        blob.upload_from_string(pickle_out)\n",
    "    \n",
    "    logging.info('Adapts finish - training next')\n",
    "        \n",
    "    tf.random.set_seed(args.seed)\n",
    "    \n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"gs://{args.model_dir}/logs-{RUN_NAME}\",\n",
    "        histogram_freq=0, \n",
    "        write_graph=True, \n",
    "        profile_batch = '500,520'\n",
    "    )\n",
    "    # if os.environ.get('AIP_TENSORBOARD_LOG_DIR', 'NA') is not 'NA':\n",
    "    #     tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    #         log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'],\n",
    "    #         histogram_freq=0, write_graph=True, profile_batch = '500,520')\n",
    "    # else:\n",
    "    #     os.mkdir('/tb_logs')\n",
    "    #     tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    #         log_dir='/tb_logs',\n",
    "    #         histogram_freq=0)\n",
    "        \n",
    "    logging.info('Training starting')\n",
    "    layer_history = model.fit(\n",
    "        cached_train,\n",
    "        validation_data=cached_valid,\n",
    "        validation_freq=args.valid_frequency,\n",
    "        callbacks=tensorboard_callback,\n",
    "        epochs=args.num_epochs,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Determine type and task of the machine from the strategy cluster resolver\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id = None, None\n",
    "    \n",
    "    # ====================================================\n",
    "    # Eval Metrics\n",
    "    # ====================================================\n",
    "    logging.info('Getting evaluation metrics')\n",
    "\n",
    "    val_metrics = model.evaluate(cached_valid, return_dict=True) #check performance\n",
    "    \n",
    "    logging.info('Validation metrics below:')\n",
    "    logging.info(val_metrics)\n",
    "    \n",
    "    with vertex_ai.start_run(RUN_NAME,resume=True) as my_run:\n",
    "        logging.info(f\"logging metrics to experiment run {RUN_NAME}\")\n",
    "        my_run.log_metrics(val_metrics)\n",
    "    \n",
    "    # logging.info(f\"Ending experiment run: {RUN_NAME}\")\n",
    "    # vertex_ai.end_run()\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save Towers\n",
    "    # ====================================================\n",
    "    \n",
    "    logging.info(f'Saving models to {args.model_dir}')\n",
    "\n",
    "    query_dir_save = f\"gs://{args.model_dir}/{args.version}/{RUN_NAME}/query_tower/\" #+ FLAGS.TS \n",
    "    candidate_dir_save = f\"gs://{args.model_dir}/{args.version}/{RUN_NAME}/candidate_tower/\" #+ FLAGS.TS \n",
    "    logging.info(f'Saving chief query model to {query_dir_save}')\n",
    "    \n",
    "    # save model from primary node in multiworker\n",
    "    if _is_chief(task_type, task_id):\n",
    "        tf.saved_model.save(model.query_tower, query_dir_save)\n",
    "        logging.info(f'Saved chief query model to {query_dir_save}')\n",
    "        tf.saved_model.save(model.candidate_tower, candidate_dir_save)\n",
    "        logging.info(f'Saved chief candidate model to {candidate_dir_save}')\n",
    "    else:\n",
    "        worker_dir_query = query_dir_save + '/workertemp_query_/' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir_query)\n",
    "        tf.saved_model.save(model.query_tower, worker_dir_query)\n",
    "        logging.info(f'Saved worker: {task_id} query model to {worker_dir_query}')\n",
    "\n",
    "        worker_dir_can = candidate_dir_save + '/workertemp_can_/' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir_can)\n",
    "        tf.saved_model.save(model.candidate_tower, worker_dir_can)\n",
    "        logging.info(f'Saved worker: {task_id} candidate model to {worker_dir_can}')\n",
    "\n",
    "    if not _is_chief(task_type, task_id):\n",
    "        tf.io.gfile.rmtree(worker_dir_can)\n",
    "        tf.io.gfile.rmtree(worker_dir_query)\n",
    "\n",
    "    logging.info('All done - model saved') #all done\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command line arguments\n",
    "    \n",
    "    type: int, float, str\n",
    "          bool() converts empty strings to `False` and non-empty strings to `True`\n",
    "          see more details here: https://docs.python.org/3/library/argparse.html#type\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir',\n",
    "                        default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir', required=False)\n",
    "    \n",
    "    parser.add_argument('--train_dir', \n",
    "                        type=str, help='dir of training files', required=True)\n",
    "\n",
    "    parser.add_argument('--candidate_file_dir', \n",
    "                        type=str, help='dir of candidate files', required=True)\n",
    "\n",
    "    parser.add_argument('--project', \n",
    "                        type=str, help='project', required=True)\n",
    "\n",
    "    parser.add_argument('--max_padding', \n",
    "                        default=375, type=int, help='max_padding', required=False)\n",
    "\n",
    "    parser.add_argument('--experiment_name', \n",
    "                        type=str, help='#TODO', required=True)\n",
    "\n",
    "    parser.add_argument('--experiment_run', \n",
    "                        type=str, help='#TODO', required=True)\n",
    "\n",
    "    parser.add_argument('--num_epochs', \n",
    "                        default=1, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--batch_size', \n",
    "                        default=128, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--embedding_dim', \n",
    "                        default=32, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--projection_dim', \n",
    "                        default=5, type=int, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--seed', \n",
    "                        default=1234, type=str, help='#TODO', required=False)\n",
    "\n",
    "#     parser.add_argument('--use_cross_layer', \n",
    "#                         default=True, type=bool, help='#TODO', required=False)\n",
    "\n",
    "#     parser.add_argument('--use_dropout', \n",
    "#                         default=False, type=bool, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--dropout_rate', \n",
    "                        default=0.4, type=float, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--layer_sizes', \n",
    "                        default='[64,32]', type=str, help='#TODO', required=False)\n",
    "\n",
    "    # parser.add_argument('--aip_tb_logs', \n",
    "    #                     default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str, help='#TODO', required=False)\n",
    "\n",
    "    # parser.add_argument('--new_adapts', \n",
    "    #                     default=False, type=bool, help='#TODO', required=False)\n",
    "\n",
    "    parser.add_argument('--learning_rate', \n",
    "                        default=0.01, type=float, help='learning rate', required=False)\n",
    "\n",
    "    parser.add_argument('--valid_size', \n",
    "                        default='#TODO', type=str, help='number of records in valid split', required=False)\n",
    "\n",
    "    parser.add_argument('--valid_frequency', \n",
    "                        default=10, type=int, help='number of epochs per metrics val calculation', required=False)\n",
    "\n",
    "    parser.add_argument('--distribute', \n",
    "                        default='single', type=str, help='TF strategy: single, mirrored, multiworker, tpu', required=False)\n",
    "\n",
    "    parser.add_argument('--version', \n",
    "                        type=str, help='version of train code; for tracking', required=True)\n",
    "\n",
    "    parser.add_argument('--train_dir_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "\n",
    "    parser.add_argument('--candidate_files_prefix', \n",
    "                        type=str, help='file path under GCS bucket', required=True)\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    return parser.parse_args()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting jobs main() script')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/spotify-tfrs/src\u001b[00m\n",
      " Dockerfile.tfrs\n",
      " README.md\n",
      " cloudbuild.yaml\n",
      " \u001b[01;34mtrainer\u001b[00m\n",
      "     __init__.py\n",
      "     _data.py\n",
      "     _model.py\n",
      "     interactive_train.py\n",
      "     requirements.txt\n",
      "     task.py\n",
      "\n",
      "1 directory, 9 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/spotify-tfrs/src\n",
    "#/vertex_train/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Worker Pool Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=b\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# # Single machine, single GPU\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# # # Single Machine; multiple GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-4g' # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 4\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# # # Multiple Machines, 1 GPU per Machine\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 9\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v11'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write `train_config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/trainer/train_config.py\n",
    "\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "\n",
    "NEW_ADAPTS = True\n",
    "USE_CROSS_LAYER = True\n",
    "USE_DROPOUT = True\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "MAX_PADDING = 375\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "SEED = 1234\n",
    "DROPOUT_RATE = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--candidate_file_dir=spotify-tfrs-dir',\n",
      "                              '--train_dir=spotify-tfrs-dir',\n",
      "                              '--model_dir=spotify-tfrs-dir',\n",
      "                              '--candidate_files_prefix=small-dataset/',\n",
      "                              '--train_dir_prefix=small-dataset/',\n",
      "                              '--experiment_name=nb-submit-v11',\n",
      "                              '--experiment_run=run-20220921-163503',\n",
      "                              '--num_epochs=1',\n",
      "                              '--batch_size=256',\n",
      "                              '--embedding_dim=32',\n",
      "                              '--projection_dim=5',\n",
      "                              '--layer_sizes=[64,32]',\n",
      "                              '--learning_rate=0.01',\n",
      "                              '--valid_frequency=10',\n",
      "                              '--distribute=single',\n",
      "                              '--version=v11'],\n",
      "                     'command': ['python', 'trainer/task.py'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/spotify-2tower-tfrs-v11-training'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# from trainer import train_config as config\n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# # \"gs://spotify-tfrecords-blog/tfrecords_v1/train/output-00000-of-00796.tfrecord\"\n",
    "# # gs://spotify-tfrs-dir/small-dataset/output-00000-of-00796.tfrecord\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "\n",
    "CANDIDATE_FILE_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "CANDIDATE_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "TRAIN_DIR = 'spotify-tfrs-dir' #'spotify-tfrecords-blog'\n",
    "TRAIN_DIR_PREFIX = 'small-dataset/' # 'tfrecords_v1/train/'\n",
    "\n",
    "MODEL_DIR='spotify-tfrs-dir'\n",
    "\n",
    "EXPERIMENT_NAME=f'nb-submit-{VERSION}'\n",
    "RUN_NAME=f'run-{TIMESTAMP}'\n",
    "\n",
    "VALID_FREQUENCY = 10\n",
    "# VALID_SIZE = 20_000\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# MAX_PADDING = 375\n",
    "EMBEDDING_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "\n",
    "DROPOUT_RATE = 0.4\n",
    "LAYER_SIZES = '[64,32]'\n",
    "\n",
    "WORKER_CMD = [\"python\", \"trainer/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--candidate_file_dir={CANDIDATE_FILE_DIR}',\n",
    "    f'--train_dir={TRAIN_DIR}',\n",
    "    f'--model_dir={MODEL_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    # f'--valid_size={VALID_SIZE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--version={VERSION}',\n",
    "]\n",
    "    \n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/spotify-tfrs/src\u001b[00m\n",
      " Dockerfile.tfrs\n",
      " README.md\n",
      " cloudbuild.yaml\n",
      " \u001b[01;34mtrainer\u001b[00m\n",
      "     __init__.py\n",
      "     _data.py\n",
      "     _model.py\n",
      "     interactive_train.py\n",
      "     requirements.txt\n",
      "     task.py\n",
      "     train_config.py\n",
      "\n",
      "1 directory, 10 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/spotify-tfrs/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-tfrs'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-tfrs')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "_DISTRIBUTE_STRATEGY='single'\n",
    "_EXPERIMENT_NAME=f'local-testing-{VERSION}'\n",
    "_RUN_NAME=f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cd src/vertex_train; python3 -m trainer.task --project={PROJECT_ID} --max_padding={MAX_PADDING} --candidate_file_dir={CANDIDATE_FILE_DIR} --train_dir={TRAIN_DIR} --model_dir={MODEL_DIR} --experiment_name={_EXPERIMENT_NAME} --experiment_run={_RUN_NAME} --num_epochs={NUM_EPOCHS} --batch_size={BATCH_SIZE} --embedding_dim={EMBEDDING_DIM} --projection_dim={PROJECTION_DIM} --valid_size={VALID_SIZE} --use_dropout={USE_DROPOUT} --dropout_rate={DROPOUT_RATE} --layer_sizes={LAYER_SIZES} --learning_rate={LEARNING_RATE} --valid_frequency={VALID_FREQUENCY} --distribute={_DISTRIBUTE_STRATEGY} --version={VERSION} --candidate_files_prefix={CANDIDATE_PREFIX} --train_dir_prefix={TRAIN_DIR_PREFIX}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-09-22 23:19:55 - Args: Namespace(batch_size=256, candidate_file_dir='spotify-tfrs-dir', candidate_files_prefix='small-dataset/', distribute='single', dropout_rate=0.4, embedding_dim=32, experiment_name='local-testing-v4', experiment_run='run-20220920-231915', layer_sizes='[64,32]', learning_rate=0.01, max_padding=375, model_dir='spotify-tfrs-dir', num_epochs=1, project='hybrid-vertex', projection_dim=5, seed=1234, train_dir='spotify-tfrs-dir', train_dir_prefix='small-dataset/', valid_frequency=10, valid_size='#TODO', version='v4')\n",
      "20-09-22 23:19:55 - Starting jobs main() script\n",
      "20-09-22 23:19:55 - Starting training...\n",
      "20-09-22 23:19:55 - TF_CONFIG = Not found\n",
      "20-09-22 23:19:55 - Detecting devices....\n",
      "2022-09-20 23:19:55.934475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-20 23:19:55.939740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-09-20 23:19:55.939782: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-20 23:19:55.939803: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jt-tfrs-spotify-sept-v2): /proc/driver/nvidia/version does not exist\n",
      "20-09-22 23:19:55 - Detected Devices [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15301094581524682234\n",
      "xla_global_id: -1\n",
      "]\n",
      "20-09-22 23:19:55 - Setting device strategy...\n",
      "WARNING:tensorflow:From /home/jupyter/spotify-tfrs/src/trainer/task.py:69: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "20-09-22 23:19:55 - From /home/jupyter/spotify-tfrs/src/trainer/task.py:69: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "20-09-22 23:19:55 - Single device training\n",
      "20-09-22 23:19:55 - num_replicas_in_sync = 1\n",
      "20-09-22 23:19:58 - args.train_dir: spotify-tfrs-dir\n",
      "20-09-22 23:19:58 - args.train_dir_prefix: small-dataset/\n",
      "20-09-22 23:19:59 - args.candidate_file_dir: spotify-tfrs-dir\n",
      "20-09-22 23:19:59 - args.candidate_files_prefix: small-dataset/\n",
      "20-09-22 23:19:59 - preparing train and valid splits...\n",
      "20-09-22 23:19:59 - Logging metaparams & hyperparams for Vertex Experiments\n",
      "20-09-22 23:19:59 - EXPERIMENT_NAME: local-testing-v4\n",
      " RUN_NAME: run-20220920-231915\n",
      "20-09-22 23:19:59 - Creating run: run-20220920-231915; for experiment: local-testing-v4\n",
      "Associating projects/934903580331/locations/us-central1/metadataStores/default/contexts/local-testing-v4-run-20220920-231915 to Experiment: local-testing-v4\n",
      "20-09-22 23:20:00 - Associating projects/934903580331/locations/us-central1/metadataStores/default/contexts/local-testing-v4-run-20220920-231915 to Experiment: local-testing-v4\n",
      "20-09-22 23:20:00 - logging metaparams\n",
      "20-09-22 23:20:00 - logging hyperparams\n",
      "20-09-22 23:20:00 - Setting model adapts and compiling the model\n",
      "20-09-22 23:20:00 - LAYER_SIZES: [64, 32]\n",
      "20-09-22 23:20:00 - adapting layers: True\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/spotify-tfrs/src/trainer/task.py\", line 474, in <module>\n",
      "    main(parsed_args)\n",
      "  File \"/home/jupyter/spotify-tfrs/src/trainer/task.py\", line 257, in main\n",
      "    model.query_tower.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(args.batch_size)) # TODO: adapts on full dataset or train only?\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 428, in adapt\n",
      "    super().adapt(data, batch_size=batch_size, steps=steps)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\", line 249, in adapt\n",
      "    self._adapt_function(iterator)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 915, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 954, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2454, in __call__\n",
      "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1861, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 502, in call\n",
      "    ctx=ctx)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 55, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd src; python3 -m trainer.task \\\n",
    "    --project={PROJECT_ID} --max_padding={MAX_PADDING} --candidate_file_dir={CANDIDATE_FILE_DIR} \\\n",
    "    --candidate_files_prefix={CANDIDATE_PREFIX} --train_dir_prefix={TRAIN_DIR_PREFIX} \\\n",
    "    --train_dir={TRAIN_DIR} --model_dir={MODEL_DIR} --experiment_name={_EXPERIMENT_NAME} --experiment_run={_RUN_NAME} \\\n",
    "    --num_epochs={NUM_EPOCHS} --batch_size={BATCH_SIZE} --embedding_dim={EMBEDDING_DIM} --projection_dim={PROJECTION_DIM} \\\n",
    "    --dropout_rate={DROPOUT_RATE} --layer_sizes={LAYER_SIZES} --learning_rate={LEARNING_RATE} \\\n",
    "    --valid_frequency={VALID_FREQUENCY} --distribute={_DISTRIBUTE_STRATEGY} --version={VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Custom Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME: tfrs\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/spotify-2tower-tfrs-v11-training\n",
      "FILE_LOCATION: ./src\n",
      "MACHINE_TYPE: e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-tfrs'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-tfrs')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 17 file(s) totalling 487.0 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1663778115.642582-eb09b5bca7d34ecba5ce09d302539578.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/f78ec76d-ac99-49a9-98a4-7d0e5ddeeed5].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/f78ec76d-ac99-49a9-98a4-7d0e5ddeeed5?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"f78ec76d-ac99-49a9-98a4-7d0e5ddeeed5\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1663778115.642582-eb09b5bca7d34ecba5ce09d302539578.tgz#1663778116073083\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1663778115.642582-eb09b5bca7d34ecba5ce09d302539578.tgz#1663778116073083...\n",
      "/ [1 files][ 75.1 KiB/ 75.1 KiB]                                                \n",
      "Operation completed over 1 objects/75.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  65.02kB\n",
      "Step 1/4 : FROM tensorflow/tensorflow:2.9.2-gpu\n",
      "2.9.2-gpu: Pulling from tensorflow/tensorflow\n",
      "d7bfe07ed847: Pulling fs layer\n",
      "c2aafb3bd4b2: Pulling fs layer\n",
      "73e07aa2f843: Pulling fs layer\n",
      "c650e29ed394: Pulling fs layer\n",
      "f4aa253753c0: Pulling fs layer\n",
      "d70afa273e9e: Pulling fs layer\n",
      "4f4cf30007fa: Pulling fs layer\n",
      "55a22edc0650: Pulling fs layer\n",
      "0492b52cf57d: Pulling fs layer\n",
      "9a71ab53b648: Pulling fs layer\n",
      "e90327ea4a75: Pulling fs layer\n",
      "37b11bd5ed06: Pulling fs layer\n",
      "b06edc21db1f: Pulling fs layer\n",
      "5bd76931cf4c: Pulling fs layer\n",
      "4f4cf30007fa: Waiting\n",
      "55a22edc0650: Waiting\n",
      "0492b52cf57d: Waiting\n",
      "b06edc21db1f: Waiting\n",
      "5bd76931cf4c: Waiting\n",
      "9a71ab53b648: Waiting\n",
      "c650e29ed394: Waiting\n",
      "d70afa273e9e: Waiting\n",
      "f4aa253753c0: Waiting\n",
      "e90327ea4a75: Waiting\n",
      "37b11bd5ed06: Waiting\n",
      "c2aafb3bd4b2: Verifying Checksum\n",
      "c2aafb3bd4b2: Download complete\n",
      "c650e29ed394: Verifying Checksum\n",
      "c650e29ed394: Download complete\n",
      "73e07aa2f843: Download complete\n",
      "f4aa253753c0: Verifying Checksum\n",
      "f4aa253753c0: Download complete\n",
      "d7bfe07ed847: Verifying Checksum\n",
      "d7bfe07ed847: Download complete\n",
      "55a22edc0650: Verifying Checksum\n",
      "55a22edc0650: Download complete\n",
      "0492b52cf57d: Verifying Checksum\n",
      "0492b52cf57d: Download complete\n",
      "d7bfe07ed847: Pull complete\n",
      "c2aafb3bd4b2: Pull complete\n",
      "9a71ab53b648: Verifying Checksum\n",
      "9a71ab53b648: Download complete\n",
      "e90327ea4a75: Verifying Checksum\n",
      "e90327ea4a75: Download complete\n",
      "73e07aa2f843: Pull complete\n",
      "c650e29ed394: Pull complete\n",
      "f4aa253753c0: Pull complete\n",
      "4f4cf30007fa: Verifying Checksum\n",
      "4f4cf30007fa: Download complete\n",
      "b06edc21db1f: Verifying Checksum\n",
      "b06edc21db1f: Download complete\n",
      "5bd76931cf4c: Download complete\n",
      "37b11bd5ed06: Verifying Checksum\n",
      "37b11bd5ed06: Download complete\n",
      "d70afa273e9e: Verifying Checksum\n",
      "d70afa273e9e: Download complete\n",
      "d70afa273e9e: Pull complete\n",
      "4f4cf30007fa: Pull complete\n",
      "55a22edc0650: Pull complete\n",
      "0492b52cf57d: Pull complete\n",
      "9a71ab53b648: Pull complete\n",
      "e90327ea4a75: Pull complete\n",
      "37b11bd5ed06: Pull complete\n",
      "b06edc21db1f: Pull complete\n",
      "5bd76931cf4c: Pull complete\n",
      "Digest: sha256:a616548c67cbc02c5b6b31b188b076d03ed3ab84715d387b1922bde97c8fccc6\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:2.9.2-gpu\n",
      " ---> 2554904d1c9d\n",
      "Step 2/4 : WORKDIR /src\n",
      " ---> Running in b022178e1e66\n",
      "Removing intermediate container b022178e1e66\n",
      " ---> f642b4f2b5c5\n",
      "Step 3/4 : COPY trainer/* trainer/\n",
      " ---> 16cab766e51f\n",
      "Step 4/4 : RUN pip install -r trainer/requirements.txt\n",
      " ---> Running in 43e252ce3ea9\n",
      "Collecting google-cloud-aiplatform==1.17.0\n",
      "  Downloading google_cloud_aiplatform-1.17.0-py2.py3-none-any.whl (2.2 MB)\n",
      "Collecting tensorflow-recommenders==0.7.0\n",
      "  Downloading tensorflow_recommenders-0.7.0-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: tensorboard==2.9.1 in /usr/local/lib/python3.8/dist-packages (from -r trainer/requirements.txt (line 4)) (2.9.1)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.8/dist-packages (from -r trainer/requirements.txt (line 5)) (0.6.1)\n",
      "Collecting tensorboard-plugin-profile==2.5.0\n",
      "  Downloading tensorboard_plugin_profile-2.5.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.6.1-py2.py3-none-any.whl (231 kB)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform==1.17.0->-r trainer/requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: protobuf<5.0.0dev,>=3.19.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform==1.17.0->-r trainer/requirements.txt (line 2)) (3.19.4)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.10.1-py3-none-any.whl (115 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: tensorflow>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (2.9.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (2.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (0.34.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (1.48.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (65.3.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (1.23.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorboard-plugin-profile==2.5.0->-r trainer/requirements.txt (line 6)) (1.14.0)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform==1.17.0->-r trainer/requirements.txt (line 2)) (3.0.9)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio_status-1.48.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (0.26.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (14.0.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.0->-r trainer/requirements.txt (line 3)) (3.7.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (4.9)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (4.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (2.1.1)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (3.8.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1->-r trainer/requirements.txt (line 4)) (3.2.0)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3972 sha256=8b6a4c3e95a2249c2bf10046fedc3c21b4dd123dab11f78c66bf647643071790\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/fb/3b/365271726c73d8bc0b5bf39ef0f5db5a9c75b2babe4fd67794\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: googleapis-common-protos, grpcio-status, google-api-core, google-cloud-core, google-crc32c, google-resumable-media, google-cloud-storage, proto-plus, python-dateutil, google-cloud-bigquery, grpc-google-iam-v1, google-cloud-resource-manager, google-cloud-aiplatform, tensorflow-recommenders, gviz-api, tensorboard-plugin-profile, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 google-api-core-2.10.1 google-cloud-aiplatform-1.17.0 google-cloud-bigquery-2.34.4 google-cloud-core-2.3.2 google-cloud-resource-manager-1.6.1 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 grpc-google-iam-v1-0.12.4 grpcio-status-1.48.1 gviz-api-1.10.0 proto-plus-1.22.1 python-dateutil-2.8.2 tensorboard-plugin-profile-2.5.0 tensorflow-recommenders-0.7.0\n",
      "\u001b[91mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "google-api-core 2.10.1 requires protobuf<5.0.0dev,>=3.20.1, but you'll have protobuf 3.19.4 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 43e252ce3ea9\n",
      " ---> 1494c856d718\n",
      "Successfully built 1494c856d718\n",
      "Successfully tagged gcr.io/hybrid-vertex/spotify-2tower-tfrs-v11-training:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/spotify-2tower-tfrs-v11-training\n",
      "The push refers to repository [gcr.io/hybrid-vertex/spotify-2tower-tfrs-v11-training]\n",
      "d7cfcd42ccef: Preparing\n",
      "be547d8f2a9f: Preparing\n",
      "150200ace7f2: Preparing\n",
      "b17efa562eec: Preparing\n",
      "72c0dd10b7ce: Preparing\n",
      "fe2e1ac49b20: Preparing\n",
      "3a45e9e1758d: Preparing\n",
      "d5687063377f: Preparing\n",
      "6d32722b0d16: Preparing\n",
      "2f454b40b4c7: Preparing\n",
      "cc0968b2442b: Preparing\n",
      "19396d48d56a: Preparing\n",
      "0572bc79294c: Preparing\n",
      "e2bc56acd3e5: Preparing\n",
      "da31846dede4: Preparing\n",
      "f70028479262: Preparing\n",
      "af7ed92504ae: Preparing\n",
      "2f454b40b4c7: Waiting\n",
      "cc0968b2442b: Waiting\n",
      "19396d48d56a: Waiting\n",
      "0572bc79294c: Waiting\n",
      "e2bc56acd3e5: Waiting\n",
      "da31846dede4: Waiting\n",
      "f70028479262: Waiting\n",
      "af7ed92504ae: Waiting\n",
      "fe2e1ac49b20: Waiting\n",
      "3a45e9e1758d: Waiting\n",
      "d5687063377f: Waiting\n",
      "6d32722b0d16: Waiting\n",
      "be547d8f2a9f: Pushed\n",
      "150200ace7f2: Pushed\n",
      "72c0dd10b7ce: Pushed\n",
      "b17efa562eec: Pushed\n",
      "3a45e9e1758d: Layer already exists\n",
      "6d32722b0d16: Layer already exists\n",
      "2f454b40b4c7: Layer already exists\n",
      "d7cfcd42ccef: Pushed\n",
      "d5687063377f: Pushed\n",
      "0572bc79294c: Layer already exists\n",
      "e2bc56acd3e5: Layer already exists\n",
      "da31846dede4: Layer already exists\n",
      "f70028479262: Layer already exists\n",
      "af7ed92504ae: Layer already exists\n",
      "cc0968b2442b: Pushed\n",
      "fe2e1ac49b20: Pushed\n",
      "19396d48d56a: Pushed\n",
      "latest: digest: sha256:1b05bbf634bfca5550fcf38e1afbbe1096d021df60f4a02bd96bfee0fb422602 size: 3889\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                           STATUS\n",
      "f78ec76d-ac99-49a9-98a4-7d0e5ddeeed5  2022-09-21T16:35:16+00:00  5M10S     gs://hybrid-vertex_cloudbuild/source/1663778115.642582-eb09b5bca7d34ecba5ce09d302539578.tgz  gcr.io/hybrid-vertex/spotify-2tower-tfrs-v11-training (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit train job to Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Could not load library libcudnn_adv_train.so.8. Error: /opt/conda/bin/../lib/libcudnn_ops_train.so.8: undefined symbol: _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv, version libcudnn_ops_infer.so.8\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spotify-2tower-tfrs-v11'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME:train-spotify-2tower-tfrs-v11\n",
      "BASE_OUTPUT_DIR:gs://spotify-tfrs-dir/spotify-2tower-tfrs-v11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "JOB_NAME = f'train-{MODEL_NAME}' #-{TIMESTAMP}'\n",
    "\n",
    "BASE_OUTPUT_DIR = f'gs://{MODEL_DIR}/{MODEL_NAME}'\n",
    "\n",
    "print(f'JOB_NAME:{JOB_NAME}')\n",
    "print(f'BASE_OUTPUT_DIR:{BASE_OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/934903580331/locations/us-central1/customJobs/2863252248664735744\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/934903580331/locations/us-central1/customJobs/2863252248664735744')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2863252248664735744?project=934903580331\n",
      "View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+934903580331+locations+us-central1+tensorboards+1347121646351155200+experiments+2863252248664735744\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 access the interactive shell terminals for the custom job:\n",
      "workerpool0-0:\n",
      "808b883fbe25400b-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/934903580331/locations/us-central1/customJobs/2863252248664735744 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/934903580331/locations/us-central1/customJobs/2863252248664735744\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=BASE_OUTPUT_DIR,\n",
    "    # labels={'gpu':f'{ACCELERATOR_TYPE}'}\n",
    ")\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SavedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 21:28:45.720839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-09-20 21:28:45.720904: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-20 21:28:45.720932: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jt-tfrs-spotify-sept-v2): /proc/driver/nvidia/version does not exist\n",
      "2022-09-20 21:28:45.721353: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "query_tower_uri = 'gs://spotify-tfrs-dir/v2/run-20220920-210334/query_tower'\n",
    "candidate_tower_uri = 'gs://spotify-tfrs-dir/v2/run-20220920-210334/candidate_tower'\n",
    "loaded_query_model = tf.saved_model.load(query_tower_uri)\n",
    "loaded_candidate_model = tf.saved_model.load(candidate_tower_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serving_default']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_candidate_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_1': TensorSpec(shape=(None, 32), dtype=tf.float32, name='output_1')}\n"
     ]
    }
   ],
   "source": [
    "infer = loaded_candidate_model.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_1': TensorShape([None, 32])}"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict2 = loaded_candidate_model.signatures['serving_default']\n",
    "predict2.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, album_name_can, album_uri_can, artist_followers_can, artist_genres_can, artist_name_can, artist_pop_can, artist_uri_can, duration_ms_can, track_name_can, track_pop_can, track_uri_can) at 0x7F27EA9BBA10>})"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_candidate_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_iter = parsed_dataset_candidates.batch(1).map(lambda data: predict2(\n",
    "                artist_name = data[\"artist_name_can\"],\n",
    "                track_name = data['track_name_can'],\n",
    "                album_name = data['album_name_can'],\n",
    "                track_uri = data['track_uri_can'],\n",
    "                artist_uri = data['artist_uri_can'],\n",
    "                album_uri = data['album_uri_can'],\n",
    "                duration_ms = data['duration_ms_can'],\n",
    "                track_pop = data['track_pop_can'],\n",
    "                artist_pop = data['artist_pop_can'],\n",
    "                artist_followers = data['artist_followers_can'],\n",
    "                artist_genres = data['artist_genres_can']))\n",
    "\n",
    "    \n",
    "candidate_features = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "> adapts vs vocab_dict\n",
    "\n",
    "```\n",
    "test_playlist_model = Playlist_Model(layer_sizes, vocab_dict_load)\n",
    "test_playlist_model.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(1000))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_duration_ms_seed_pl = 13000151.68\n",
    "var_duration_ms_seed_pl = 133092900971233.58\n",
    "vocab_dict_load['avg_duration_ms_seed_pl']=avg_duration_ms_seed_pl\n",
    "vocab_dict_load['var_duration_ms_seed_pl']=var_duration_ms_seed_pl\n",
    "\n",
    "avg_n_songs_pl = 55.21\n",
    "var_n_songs_pl = 2317.54\n",
    "vocab_dict_load['avg_n_songs_pl']=avg_n_songs_pl\n",
    "vocab_dict_load['var_n_songs_pl']=var_n_songs_pl\n",
    "\n",
    "avg_n_artists_pl = 30.56\n",
    "var_n_artists_pl = 769.26\n",
    "vocab_dict_load['avg_n_artists_pl']=avg_n_artists_pl\n",
    "vocab_dict_load['var_n_artists_pl']=var_n_artists_pl\n",
    "\n",
    "avg_n_albums_pl = 40.25\n",
    "var_n_albums_pl = 1305.54\n",
    "vocab_dict_load['avg_n_albums_pl']=avg_n_albums_pl\n",
    "vocab_dict_load['var_n_albums_pl']=var_n_albums_pl\n",
    "\n",
    "avg_artist_pop = 16.08\n",
    "var_artist_pop = 300.64\n",
    "vocab_dict_load['avg_artist_pop']=avg_artist_pop\n",
    "vocab_dict_load['var_artist_pop']=var_artist_pop\n",
    "\n",
    "avg_duration_ms_songs_pl = 234823.14\n",
    "var_duration_ms_songs_pl = 5558806228.41\n",
    "vocab_dict_load['avg_duration_ms_songs_pl']=avg_duration_ms_songs_pl\n",
    "vocab_dict_load['var_duration_ms_songs_pl']=var_duration_ms_songs_pl\n",
    "\n",
    "avg_artist_followers = 43337.77\n",
    "var_artist_followers = 377777790193.57\n",
    "vocab_dict_load['avg_artist_followers']=avg_artist_followers\n",
    "vocab_dict_load['var_artist_followers']=var_artist_followers\n",
    "\n",
    "avg_track_pop = 10.85\n",
    "var_track_pop = 202.18\n",
    "vocab_dict_load['avg_track_pop']=avg_track_pop\n",
    "vocab_dict_load['var_track_pop']=var_track_pop\n",
    "# vocab_dict_load['unique_pids_string']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archived Dockerfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# # Dockerfile-gpu\n",
    "# FROM gcr.io/deeplearning-platform-release/tf-gpu.2-9\n",
    "\n",
    "# WORKDIR /src\n",
    "\n",
    "# # Copies the trainer code to the docker image.\n",
    "# COPY trainer/* trainer/ \n",
    "\n",
    "# RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
